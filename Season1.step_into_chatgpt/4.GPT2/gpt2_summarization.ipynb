{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb430c0e-fd70-46e2-91e2-b14cf782bd06",
   "metadata": {},
   "source": [
    "# 基于MindSpore的GPT2文本摘要\n",
    "\n",
    "## 环境配置\n",
    "\n",
    "1. 配置python3.9环境\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f085c7-b2b3-4b18-95f9-13b298b10d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "!/home/ma-user/anaconda3/bin/conda create -n python-3.9.0 python=3.9.0 -y --override-channels --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n",
    "!/home/ma-user/anaconda3/envs/python-3.9.0/bin/pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9436813-c813-425e-b1ed-af0534ef862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "data = {\n",
    "   \"display_name\": \"python-3.9.0\",\n",
    "   \"env\": {\n",
    "      \"PATH\": \"/home/ma-user/anaconda3/envs/python-3.9.0/bin:/home/ma-user/anaconda3/envs/python-3.7.10/bin:/modelarts/authoring/notebook-conda/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/ma-user/modelarts/ma-cli/bin:/home/ma-user/modelarts/ma-cli/bin\"\n",
    "   },\n",
    "   \"language\": \"python\",\n",
    "   \"argv\": [\n",
    "      \"/home/ma-user/anaconda3/envs/python-3.9.0/bin/python\",\n",
    "      \"-m\",\n",
    "      \"ipykernel\",\n",
    "      \"-f\",\n",
    "      \"{connection_file}\"\n",
    "   ]\n",
    "}\n",
    "\n",
    "if not os.path.exists(\"/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/\"):\n",
    "    os.mkdir(\"/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/\")\n",
    "\n",
    "with open('/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/kernel.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdfae31-df62-4746-bd3e-1c556413ffd2",
   "metadata": {},
   "source": [
    "***注：以上代码执行完成后，需点击左上角或右上角将kernel更换为python-3.9.0***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757cb29c-48f4-4b6b-9b71-6c008e629eb8",
   "metadata": {},
   "source": [
    "2. 安装mindspore2.2.12，安装指南详见：[MindSpore安装](https://www.mindspore.cn/install)\n",
    "3. 安装MindNLP及相关依赖，MindNLP官方仓详见：[MindNLP](https://github.com/mindspore-lab/mindnlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6491560b-1ec6-4ca1-88cc-c7f9c1297725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: mindnlp==0.4.1 in /home/filament/.local/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: mindspore>=2.2.14 in /home/filament/.local/lib/python3.10/site-packages (from mindnlp==0.4.1) (2.4.0)\n",
      "Requirement already satisfied: tqdm in /home/filament/.local/lib/python3.10/site-packages (from mindnlp==0.4.1) (4.67.1)\n",
      "Requirement already satisfied: requests in /home/filament/.local/lib/python3.10/site-packages (from mindnlp==0.4.1) (2.32.3)\n",
      "Requirement already satisfied: datasets in /home/filament/.local/lib/python3.10/site-packages (from mindnlp==0.4.1) (3.6.0)\n",
      "Requirement already satisfied: evaluate in /home/filament/.local/lib/python3.10/site-packages (from mindnlp==0.4.1) (0.4.3)\n",
      "Requirement already satisfied: tokenizers==0.19.1 in /home/filament/.local/lib/python3.10/site-packages (from mindnlp==0.4.1) (0.19.1)\n",
      "Requirement already satisfied: safetensors in /home/filament/.local/lib/python3.10/site-packages (from mindnlp==0.4.1) (0.5.3)\n",
      "Requirement already satisfied: sentencepiece in /home/filament/.local/lib/python3.10/site-packages (from mindnlp==0.4.1) (0.2.0)\n",
      "Requirement already satisfied: regex in /home/filament/.local/lib/python3.10/site-packages (from mindnlp==0.4.1) (2024.11.6)\n",
      "Requirement already satisfied: addict in /home/filament/.local/lib/python3.10/site-packages (from mindnlp==0.4.1) (2.4.0)\n",
      "Requirement already satisfied: ml-dtypes in /home/filament/.local/lib/python3.10/site-packages (from mindnlp==0.4.1) (0.5.1)\n",
      "Requirement already satisfied: pyctcdecode in /home/filament/.local/lib/python3.10/site-packages (from mindnlp==0.4.1) (0.5.0)\n",
      "Requirement already satisfied: pytest==7.2.0 in /home/filament/.local/lib/python3.10/site-packages (from mindnlp==0.4.1) (7.2.0)\n",
      "Requirement already satisfied: pillow>=10.0.0 in /home/filament/.local/lib/python3.10/site-packages (from mindnlp==0.4.1) (11.1.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/filament/.local/lib/python3.10/site-packages (from pytest==7.2.0->mindnlp==0.4.1) (25.3.0)\n",
      "Requirement already satisfied: iniconfig in /home/filament/.local/lib/python3.10/site-packages (from pytest==7.2.0->mindnlp==0.4.1) (2.1.0)\n",
      "Requirement already satisfied: packaging in /home/filament/.local/lib/python3.10/site-packages (from pytest==7.2.0->mindnlp==0.4.1) (24.2)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /home/filament/.local/lib/python3.10/site-packages (from pytest==7.2.0->mindnlp==0.4.1) (1.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /home/filament/.local/lib/python3.10/site-packages (from pytest==7.2.0->mindnlp==0.4.1) (1.2.2)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/filament/.local/lib/python3.10/site-packages (from pytest==7.2.0->mindnlp==0.4.1) (2.2.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/filament/.local/lib/python3.10/site-packages (from tokenizers==0.19.1->mindnlp==0.4.1) (0.32.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.20.0 in /home/filament/.local/lib/python3.10/site-packages (from mindspore>=2.2.14->mindnlp==0.4.1) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.13.0 in /home/filament/.local/lib/python3.10/site-packages (from mindspore>=2.2.14->mindnlp==0.4.1) (6.30.2)\n",
      "Requirement already satisfied: asttokens>=2.0.4 in /home/filament/.local/lib/python3.10/site-packages (from mindspore>=2.2.14->mindnlp==0.4.1) (3.0.0)\n",
      "Requirement already satisfied: scipy>=1.5.4 in /home/filament/.local/lib/python3.10/site-packages (from mindspore>=2.2.14->mindnlp==0.4.1) (1.15.2)\n",
      "Requirement already satisfied: psutil>=5.6.1 in /home/filament/.local/lib/python3.10/site-packages (from mindspore>=2.2.14->mindnlp==0.4.1) (7.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.3 in /home/filament/.local/lib/python3.10/site-packages (from mindspore>=2.2.14->mindnlp==0.4.1) (1.6.3)\n",
      "Requirement already satisfied: filelock in /home/filament/.local/lib/python3.10/site-packages (from datasets->mindnlp==0.4.1) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/filament/.local/lib/python3.10/site-packages (from datasets->mindnlp==0.4.1) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/filament/.local/lib/python3.10/site-packages (from datasets->mindnlp==0.4.1) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/filament/.local/lib/python3.10/site-packages (from datasets->mindnlp==0.4.1) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/filament/.local/lib/python3.10/site-packages (from datasets->mindnlp==0.4.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/filament/.local/lib/python3.10/site-packages (from datasets->mindnlp==0.4.1) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/filament/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->mindnlp==0.4.1) (2025.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets->mindnlp==0.4.1) (6.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/filament/.local/lib/python3.10/site-packages (from requests->mindnlp==0.4.1) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->mindnlp==0.4.1) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->mindnlp==0.4.1) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->mindnlp==0.4.1) (2020.6.20)\n",
      "Requirement already satisfied: pygtrie<3.0,>=2.1 in /home/filament/.local/lib/python3.10/site-packages (from pyctcdecode->mindnlp==0.4.1) (2.5.0)\n",
      "Requirement already satisfied: hypothesis<7,>=6.14 in /home/filament/.local/lib/python3.10/site-packages (from pyctcdecode->mindnlp==0.4.1) (6.131.25)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.3->mindspore>=2.2.14->mindnlp==0.4.1) (0.42.0)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/lib/python3/dist-packages (from astunparse>=1.6.3->mindspore>=2.2.14->mindnlp==0.4.1) (1.16.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/filament/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->mindnlp==0.4.1) (3.11.18)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/filament/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1->mindnlp==0.4.1) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/filament/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1->mindnlp==0.4.1) (1.1.2)\n",
      "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /home/filament/.local/lib/python3.10/site-packages (from hypothesis<7,>=6.14->pyctcdecode->mindnlp==0.4.1) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/filament/.local/lib/python3.10/site-packages (from pandas->datasets->mindnlp==0.4.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets->mindnlp==0.4.1) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/filament/.local/lib/python3.10/site-packages (from pandas->datasets->mindnlp==0.4.1) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/filament/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->mindnlp==0.4.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/filament/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->mindnlp==0.4.1) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/filament/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->mindnlp==0.4.1) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/filament/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->mindnlp==0.4.1) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/filament/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->mindnlp==0.4.1) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/filament/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->mindnlp==0.4.1) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/filament/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->mindnlp==0.4.1) (1.20.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: tokenizers==0.19.1 in /home/filament/.local/lib/python3.10/site-packages (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/filament/.local/lib/python3.10/site-packages (from tokenizers==0.19.1) (0.32.0)\n",
      "Requirement already satisfied: filelock in /home/filament/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/filament/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/filament/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/filament/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/filament/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/filament/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/filament/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/filament/.local/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1) (2020.6.20)\n",
      "env: no_proxy='a.test.com,127.0.0.1,2.2.2.2'\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Collecting mindspore==2.6.0rc1\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/d4/87/e1e9d36ac3397a9bf7aa45cc039adc63dd6c21dc7bb9b3ad9c343cc3e630/mindspore-2.6.0rc1-cp310-cp310-manylinux1_x86_64.whl (809.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.2/809.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.20.0 in /home/filament/.local/lib/python3.10/site-packages (from mindspore==2.6.0rc1) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.13.0 in /home/filament/.local/lib/python3.10/site-packages (from mindspore==2.6.0rc1) (6.30.2)\n",
      "Requirement already satisfied: asttokens>=2.0.4 in /home/filament/.local/lib/python3.10/site-packages (from mindspore==2.6.0rc1) (3.0.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/filament/.local/lib/python3.10/site-packages (from mindspore==2.6.0rc1) (11.1.0)\n",
      "Requirement already satisfied: scipy>=1.5.4 in /home/filament/.local/lib/python3.10/site-packages (from mindspore==2.6.0rc1) (1.15.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/filament/.local/lib/python3.10/site-packages (from mindspore==2.6.0rc1) (24.2)\n",
      "Requirement already satisfied: psutil>=5.6.1 in /home/filament/.local/lib/python3.10/site-packages (from mindspore==2.6.0rc1) (7.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.3 in /home/filament/.local/lib/python3.10/site-packages (from mindspore==2.6.0rc1) (1.6.3)\n",
      "Requirement already satisfied: safetensors>=0.4.0 in /home/filament/.local/lib/python3.10/site-packages (from mindspore==2.6.0rc1) (0.5.3)\n",
      "Requirement already satisfied: dill>=0.3.7 in /home/filament/.local/lib/python3.10/site-packages (from mindspore==2.6.0rc1) (0.3.8)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.3->mindspore==2.6.0rc1) (0.42.0)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/lib/python3/dist-packages (from astunparse>=1.6.3->mindspore==2.6.0rc1) (1.16.0)\n",
      "Installing collected packages: mindspore\n",
      "  Attempting uninstall: mindspore\n",
      "    Found existing installation: mindspore 2.4.0\n",
      "    Uninstalling mindspore-2.4.0:\n",
      "      Successfully uninstalled mindspore-2.4.0\n",
      "Successfully installed mindspore-2.6.0rc1\n",
      "\u001b[33mWARNING: Skipping mindformers as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install mindnlp==0.4.1 -i https://pypi.tuna.tsinghua.edu.cn/simple #改为在线并锁定版本\n",
    "\n",
    "#!pip install mindnlp-0.4.1-py3-none-any.whl  # 将安装mindnlp版本更换为mindnlp-0.4.0-py3-none-any.whl（daily版本）\n",
    "!pip install tokenizers==0.19.1 -i https://pypi.tuna.tsinghua.edu.cn/simple  # 修改tokenizers版本为0.19.1\n",
    "\n",
    "%env no_proxy='a.test.com,127.0.0.1,2.2.2.2'\n",
    "\n",
    "!pip install mindspore==2.6.0rc1 -i https://mirrors.aliyun.com/pypi/simple \n",
    "\n",
    "#%pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.4.0/MindSpore/unified/aarch64/mindspore-2.4.0-cp39-cp39-linux_aarch64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip uninstall mindformers -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faae5c6-8397-4574-9e9c-f86230bb071a",
   "metadata": {},
   "source": [
    "***注：执行如上命令完成安装后，请点击上方的restart kernel图标重启kernel，再进行实验***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb699e4a-a2dc-44f2-b3cb-6b86fa9b24f6",
   "metadata": {},
   "source": [
    "### 数据集加载与处理\n",
    "\n",
    "1. 数据集加载\n",
    "\n",
    "    本次实验使用的是nlpcc2017摘要数据，内容为新闻正文及其摘要，总计50000个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27bf5931-7b09-4984-841c-fbea311d3955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filament/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.495 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.utils import http_get\n",
    "\n",
    "# download dataset\n",
    "url = 'https://download.mindspore.cn/toolkits/mindnlp/dataset/text_generation/nlpcc2017/train_with_summ.txt'\n",
    "path = http_get(url, './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5868b6-7a52-4f97-b934-4d3632a978a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mindspore.dataset import TextFileDataset\n",
    "\n",
    "# load dataset\n",
    "dataset = TextFileDataset(str(path), shuffle=False) \n",
    "dataset.get_dataset_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2697c68-be45-44c1-bc26-7c04424ebf0c",
   "metadata": {},
   "source": [
    "**本案例默认在GPU P100上运行，因中文文本，tokenizer使用的是bert tokenizer而非gpt tokenizer等原因，全量数据训练1个epoch的时间约为80分钟。**\n",
    "\n",
    "**为节约时间，我们选取了数据集中很小的一个子集（500条数据）来演示gpt2的微调和推理全流程，但由于数据量不足，会导致模型效果较差。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf79231-864c-4e11-9409-995c95cdb30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and testing dataset\n",
    "mini_dataset, _ = dataset.split([0.001, 0.999], randomize=False)\n",
    "train_dataset, test_dataset = mini_dataset.split([0.9, 0.1], randomize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f0d574-53a0-4bac-9303-6eb769418c04",
   "metadata": {},
   "source": [
    "2. 数据预处理\n",
    "\n",
    "    原始数据格式：\n",
    "    ```text\n",
    "    article: [CLS] article_context [SEP]\n",
    "    summary: [CLS] summary_context [SEP]\n",
    "    ```\n",
    "    预处理后的数据格式：\n",
    "\n",
    "    ```text\n",
    "    [CLS] article_context [SEP] summary_context [SEP]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1ee1961-0658-4e70-95c2-81fefd83a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# preprocess dataset\n",
    "def process_dataset(dataset, tokenizer, batch_size=4, max_seq_len=1024, shuffle=False):\n",
    "    def read_map(text):\n",
    "        data = json.loads(text.tobytes())\n",
    "        return np.array(data['article']), np.array(data['summarization'])\n",
    "\n",
    "    def merge_and_pad(article, summary):\n",
    "        # tokenization\n",
    "        # pad to max_seq_length, only truncate the article\n",
    "        tokenized = tokenizer(text=article, text_pair=summary,\n",
    "                              padding='max_length', truncation='only_first', max_length=max_seq_len)\n",
    "        return tokenized['input_ids'], tokenized['input_ids']\n",
    "    \n",
    "    dataset = dataset.map(read_map, 'text', ['article', 'summary'])\n",
    "    # change column names to input_ids and labels for the following training\n",
    "    dataset = dataset.map(merge_and_pad, ['article', 'summary'], ['input_ids', 'labels'])\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ce3dab-9486-4365-be7c-34bd5a761080",
   "metadata": {},
   "source": [
    "因GPT2无中文的tokenizer，我们使用BertTokenizer替代。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3cd8e57-72bc-4d2e-b38d-38b24efadd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filament/.local/lib/python3.10/site-packages/mindnlp/transformers/tokenization_utils_base.py:1526: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted, and will be then set to `False` by default. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mindnlp.transformers import BertTokenizer\n",
    "\n",
    "# We use BertTokenizer for tokenizing chinese context.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e89c26b-4970-449a-a0c4-b6c61845e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = process_dataset(train_dataset, tokenizer, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b65cc13-0a52-4bae-ab5f-ebb813a4d3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tensor(shape=[1, 1024], dtype=Int64, value=\n",
       " [[ 101, 1724, 3862 ...    0,    0,    0]]),\n",
       " Tensor(shape=[1, 1024], dtype=Int64, value=\n",
       " [[ 101, 1724, 3862 ...    0,    0,    0]])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dataset.create_tuple_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1497ee-2ad1-4da8-b659-9c7f2d45fccc",
   "metadata": {},
   "source": [
    "### 模型构建\n",
    "\n",
    "1. 构建GPT2ForSummarization模型，注意***shift right***的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f295944-ea2e-41e1-8301-472e09223792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改部分代码\n",
    "# from mindspore import ops\n",
    "# from mindnlp.transformers import GPT2LMHeadModel\n",
    "\n",
    "# class GPT2ForSummarization(GPT2LMHeadModel):\n",
    "#     def construct(\n",
    "#         self,\n",
    "#         input_ids = None,\n",
    "#         attention_mask = None,\n",
    "#         labels = None,\n",
    "#     ):\n",
    "#         outputs = super().construct(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         shift_logits = outputs.logits[..., :-1, :]\n",
    "#         shift_labels = labels[..., 1:]\n",
    "#         # Flatten the tokens\n",
    "#         loss = ops.cross_entropy(shift_logits.view(-1, shift_logits.shape[-1]), shift_labels.view(-1), ignore_index=tokenizer.pad_token_id)\n",
    "#         return loss\n",
    "from mindnlp.core.nn import functional as F\n",
    "from mindnlp.transformers import GPT2LMHeadModel\n",
    "\n",
    "class GPT2ForSummarization(GPT2LMHeadModel):\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids = None,\n",
    "        attention_mask = None,\n",
    "        labels = None,\n",
    "    ):\n",
    "        outputs = super().forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        shift_logits = outputs.logits[..., :-1, :]\n",
    "        shift_labels = labels[..., 1:]\n",
    "        # Flatten the tokens\n",
    "        loss = F.cross_entropy(shift_logits.view(-1, shift_logits.shape[-1]), shift_labels.view(-1), ignore_index=tokenizer.pad_token_id)\n",
    "        return (loss,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6af843-64d7-49a3-875f-605d6b2e74b2",
   "metadata": {},
   "source": [
    "2. 动态学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73c7be3d-44dc-49d4-abd8-c41f316a28d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除了动态学习率\n",
    "\n",
    "# from mindspore import ops\n",
    "# from mindspore.nn.learning_rate_schedule import LearningRateSchedule\n",
    "\n",
    "# class LinearWithWarmUp(LearningRateSchedule):\n",
    "#     \"\"\"\n",
    "#     Warmup-decay learning rate.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, learning_rate, num_warmup_steps, num_training_steps):\n",
    "#         super().__init__()\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.num_warmup_steps = num_warmup_steps\n",
    "#         self.num_training_steps = num_training_steps\n",
    "\n",
    "#     def construct(self, global_step):\n",
    "#         if global_step < self.num_warmup_steps:\n",
    "#             return global_step / float(max(1, self.num_warmup_steps)) * self.learning_rate\n",
    "#         return ops.maximum(\n",
    "#             0.0, (self.num_training_steps - global_step) / (max(1, self.num_training_steps - self.num_warmup_steps))\n",
    "#         ) * self.learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e9db2-11df-4cc4-8bef-87d473d99e5a",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a655320-2d05-4c93-bc8b-f1b4f45f809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "warmup_steps = 100\n",
    "learning_rate = 1.5e-4\n",
    "max_grad_norm = 1.0\n",
    "num_training_steps = num_epochs * train_dataset.get_dataset_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81ac9003-2dcf-42f8-b42d-3a788f172d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`.`PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MS_ALLOC_CONF]Runtime config:  enable_vmm:True  vmm_align_size:2MB\n"
     ]
    }
   ],
   "source": [
    "from mindspore import nn\n",
    "from mindnlp.transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "config = GPT2Config(vocab_size=len(tokenizer))\n",
    "model = GPT2ForSummarization(config)\n",
    "# 修改部分代码\n",
    "# lr_scheduler = LinearWithWarmUp(learning_rate=learning_rate, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps)\n",
    "# optimizer = nn.AdamWeightDecay(model.trainable_params(), learning_rate=lr_scheduler)\n",
    "# optimizer = nn.AdamWeightDecay(model.trainable_params(), learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2803c71c-3591-48cf-a6a9-6b840af749bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of model parameters: 102068736\n"
     ]
    }
   ],
   "source": [
    "# 记录模型参数数量\n",
    "print('number of model parameters: {}'.format(model.num_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1492649c-dfdb-4cfd-85bb-aef478aff5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改部分代码\n",
    "# from mindnlp._legacy.engine import Trainer\n",
    "# from mindnlp._legacy.engine.callbacks import CheckpointCallback\n",
    "\n",
    "# ckpoint_cb = CheckpointCallback(save_path='checkpoint', ckpt_name='gpt2_summarization',\n",
    "#                                 epochs=1, keep_checkpoint_max=2)\n",
    "\n",
    "# trainer = Trainer(network=model, train_dataset=train_dataset,\n",
    "#                   epochs=num_epochs, optimizer=optimizer, callbacks=ckpoint_cb)\n",
    "# trainer.set_amp(level='O1')  # 开启混合精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88259c93-5366-4406-a417-396808ec767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.engine import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"gpt2_summarization\",\n",
    "    save_steps=train_dataset.get_dataset_size(),\n",
    "    save_total_limit=3,\n",
    "    logging_steps=1000,\n",
    "    max_steps=num_training_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_steps=warmup_steps\n",
    "    \n",
    ")\n",
    "\n",
    "from mindnlp.engine import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebf47838-460a-49e4-8850-f10fe7b5ff2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 30/45 [05:39<02:33, 10.21s/it][WARNING] MD(777,7edbc9ffa6c0,python):2025-06-06-11:01:16.055.909 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 86.612%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "100%|██████████| 45/45 [08:48<00:00, 11.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 528.3991, 'train_samples_per_second': 0.681, 'train_steps_per_second': 0.085, 'train_loss': 9.115397135416666, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=45, training_loss=9.115397135416666, metrics={'train_runtime': 528.3991, 'train_samples_per_second': 0.681, 'train_steps_per_second': 0.085, 'train_loss': 9.115397135416666, 'epoch': 1.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 修改部分代码\n",
    "# trainer.run(tgt_columns=\"labels\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "787795ec-0c07-4be6-97b7-4defbe899117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_dataset(dataset, tokenizer, batch_size=1, max_seq_len=1024, max_summary_len=100):\n",
    "    def read_map(text):\n",
    "        data = json.loads(text.tobytes())\n",
    "        return np.array(data['article']), np.array(data['summarization'])\n",
    "\n",
    "    def pad(article):\n",
    "        tokenized = tokenizer(text=article, truncation=True, max_length=max_seq_len-max_summary_len)\n",
    "        return tokenized['input_ids']\n",
    "\n",
    "    dataset = dataset.map(read_map, 'text', ['article', 'summary'])\n",
    "    dataset = dataset.map(pad, 'article', ['input_ids'])\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "996842c4-f793-4393-ae64-2d4b065bc9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_test_dataset = process_test_dataset(test_dataset, tokenizer, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10421e6d-ec81-435d-9944-f1e35cd3eae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 101, 4373, 3360, 3173, 7319, 5381,  118, 4373, 3360, 3241, 2845,\n",
      "        6380, 8020, 6858, 6380, 1447,  133,  100,  135, 7942,  837, 2412,\n",
      "        8021, 5298, 6756, 7313, 7395, 8024, 1355, 4385, 6378, 5298, 1767,\n",
      "        7353, 6818,  671, 3418, 4510, 5296, 3327,  677, 3300,  671,  702,\n",
      "        7881, 4973, 8024, 8128, 2259, 4638, 1144, 3378, 4989, 1315, 4260,\n",
      "         677, 1343, 2929, 7881, 8024,  679, 2682, 2218, 3634, 1462,  700,\n",
      "        7942, 3787,  511, 3189, 1184, 1355, 4495, 1762, 1300, 4635, 1344,\n",
      "        3152, 1765, 7252,  671, 7730, 3413, 6378, 5298, 1767, 4638, 2692,\n",
      "        1912,  752, 3125, 8024,  808,  782, 1537, 1656,  511,  791, 2399,\n",
      "        8128, 2259, 4638, 1144, 3378, 3221, 1300, 4635, 1344, 2123, 4059,\n",
      "        7252, 3173, 5783, 3333,  782, 8024,  679,  719, 1184, 1168, 1071,\n",
      "        1828, 1356, 1144, 3378, 3378,  818, 3136, 5298, 4638, 2123, 4059,\n",
      "        7252, 3378, 7730, 3413, 1346, 1217, 3749, 6756, 7730, 7724, 1447,\n",
      "        1824, 6378,  511, 8132, 3189,  704, 1286, 8024, 1144, 3378, 7390,\n",
      "        1828, 1356, 1144, 3378, 3378, 1350, 1369, 1912, 1126, 1399, 2110,\n",
      "        1447, 2458, 6756, 1168, 3152, 1765, 7252, 8024,  955, 4500, 3152,\n",
      "        1765, 7252, 7160, 4635, 5106, 1322, 7353, 6818, 4638, 6378, 5298,\n",
      "        1767, 5298, 6756,  511, 8122, 3198, 6387, 8024, 6378, 5298, 7313,\n",
      "        7395, 8024, 1144, 3378, 1355, 4385, 6378, 5298, 1767, 7353, 6818,\n",
      "         671, 3418, 4510, 5296, 3327,  677, 3300,  671,  702, 7881, 4973,\n",
      "        8024,  671, 1372,  100, 1061, 1520,  100, 3633, 1388, 4708, 6001,\n",
      "        2094, 7607, 1726, 7881, 4973,  511, 2398, 3198, 1144, 3378,  981,\n",
      "        2209,  833, 2936, 7881, 1139, 1297, 8024, 4761, 6887, 6821, 4905,\n",
      "        7881,  817,  966, 8135, 1914, 1039,  511,  800, 6656, 6716, 6804,\n",
      "        2110, 1447, 2802,  749,  702, 2875, 1461, 8024,  912, 7607, 1944,\n",
      "        6814, 1343, 8024, 3617, 4260,  677, 4510, 5296, 3327, 2929, 7881,\n",
      "         511, 1144, 3378, 3378, 1355, 4385, 1400, 6841, 6814, 1343, 3617,\n",
      "        7349, 3632, 8024,  852,  711, 3198, 2347, 3241, 8024, 5023, 1144,\n",
      "        3378, 3378, 6628, 1168, 3198, 8024, 1144, 3378, 2347, 4260, 1168,\n",
      "        4510, 5296, 3327, 7553, 8024,  847, 2797, 2929, 7881, 3198,  679,\n",
      "        2708, 6239, 4821, 1168, 1928, 7553, 4638, 7770, 1327, 4510, 5296,\n",
      "        8024, 2496, 1315, 6716,  767,  511, 1071, 2797, 2958, 6158, 1912,\n",
      "        7463, 4638, 7167, 3363, 1173, 4959, 8024, 2221,  860, 2647, 2899,\n",
      "        1762, 4510, 5296, 3327,  677,  511, 8153, 3189,  678, 1286, 8024,\n",
      "        1762, 3152, 1765,  510, 2123, 4059, 7252, 3124, 2424, 1350, 4685,\n",
      "        1068, 6956, 7305, 1291, 6444,  678, 8024, 4685, 1068, 6569,  818,\n",
      "        3175,  680, 3647, 5442, 2157, 2247, 6809, 2768, 6608,  985, 1291,\n",
      "        6379, 8024, 3647, 5442, 2157, 2247, 1398, 2692, 2199, 3647, 5442,\n",
      "        2221,  860, 1357,  678, 3021, 6624, 8024, 4685, 1068, 6569,  818,\n",
      "        3175, 1066, 6608,  985, 3647, 5442, 2157, 2247, 8115,  119,  129,\n",
      "         674, 1039, 8024, 1071,  704, 2123, 4059, 7252, 3378, 7730, 3413,\n",
      "        6608,  802,  128,  119,  124,  674, 1039, 8024, 3647, 5442, 1828,\n",
      "        1356, 1144, 3378, 3378, 8020, 7730, 3413, 3136, 5298, 8021, 6608,\n",
      "         802,  126,  119,  124,  674, 1039, 8024, 3152, 1765,  897, 4510,\n",
      "        2792, 3315, 3341, 3766, 3300, 4684, 2970, 6569,  818, 8024,  852,\n",
      "        1139,  754,  782, 6887,  712,  721, 6608,  802,  124,  119,  123,\n",
      "         674, 1039,  511, 8020, 1333, 3403, 7579, 8038,  711, 2929,  671,\n",
      "        1372, 7881,  133,  100,  135, 6608,  677,  671, 3340, 1462, 1300,\n",
      "        4635,  671, 4511, 2094, 4260, 4510, 5296, 3327, 2929, 7881, 8024,\n",
      "         679, 2708, 6239, 4510, 6716,  767, 8021,  102]], dtype=int64), array(['玉林21岁小伙驾校培训期间爬上电线杆抓鸟，触到高压电线触电身亡；相关责任方赔偿家属15.8万元(图)'], dtype='<U50')]\n"
     ]
    }
   ],
   "source": [
    "print(next(batched_test_dataset.create_tuple_iterator(output_numpy=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a5288a4-f584-479b-9653-bda060e00279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改部分代码\n",
    "# model = GPT2LMHeadModel.from_pretrained('./checkpoint/gpt2_summarization_epoch_0.ckpt', config=config)\n",
    "model = GPT2LMHeadModel.from_pretrained('./gpt2_summarization/checkpoint-45', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1631f82e-857d-413a-a298-ff6f66d4a286",
   "metadata": {},
   "source": [
    "由于训练数据量少，epochs数少且tokenizer并未使用gpt tokenizer等因素，模型推理效果会较差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "090be0ea-3467-4eca-abb6-129d451f58ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:01.013.779 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 84.8249%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:01.644.571 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 86.232%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:02.259.817 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 87.3694%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:03.036.864 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.1888%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:04.017.105 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.1875%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:04.819.896 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.2121%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:05.567.599 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.2139%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:04.966.105 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.2137%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:05.663.820 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.2198%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:06.406.974 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.2198%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:07.100.020 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.2216%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:07.735.723 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.2198%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:08.399.173 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.2198%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:09.075.127 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.2198%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:09.712.431 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.2292%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:10.277.470 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.2292%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:10.982.927 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.2292%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:11.405.420 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.2455%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:11.953.699 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.2445%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:12.513.638 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.2494%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:13.027.630 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.2513%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:13.563.688 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.283%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:14.092.551 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.2849%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:14.533.112 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.2849%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n",
      "[WARNING] MD(777,7edbcbffe6c0,python):2025-06-06-11:05:14.977.520 [mindspore/ccsrc/minddata/dataset/engine/datasetops/batch_op.cc:133] operator()] Memory consumption is more than 88.2849%, which may cause OOM. Please reduce num_parallel_workers size / optimize 'per_batch_map' function / other python data preprocess function to reduce memory usage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 玉 林 新 闻 网 - 玉 林 晚 报 讯 （ 通 讯 员 < [UNK] > 黄 传 庆 ） 练 车 间 隙 ， 发 现 训 练 场 附 近 一 根 电 线 杆 上 有 一 个 鸟 窝 ， 21 岁 的 刁 某 立 即 爬 上 去 捉 鸟 ， 不 想 就 此 命 丧 黄 泉 。 日 前 发 生 在 博 白 县 文 地 镇 一 驾 校 训 练 场 的 意 外 事 故 ， 令 人 唏 嘘 。 今 年 21 岁 的 刁 某 是 博 白 县 宁 潭 镇 新 荣 村 人 ， 不 久 前 到 其 堂 叔 刁 某 某 任 教 练 的 宁 潭 镇 某 驾 校 参 加 汽 车 驾 驶 员 培 训 。 25 日 中 午 ， 刁 某 随 堂 叔 刁 某 某 及 另 外 几 名 学 员 开 车 到 文 地 镇 ， 借 用 文 地 镇 钛 白 粉 厂 附 近 的 训 练 场 练 车 。 14 时 许 ， 训 练 间 隙 ， 刁 某 发 现 训 练 场 附 近 一 根 电 线 杆 上 有 一 个 鸟 窝 ， 一 只 [UNK] 八 哥 [UNK] 正 叼 着 虫 子 飞 回 鸟 窝 。 平 时 刁 某 偶 尔 会 捕 鸟 出 卖 ， 知 道 这 种 鸟 价 值 100 多 元 。 他 跟 身 边 学 员 打 了 个 招 呼 ， 便 飞 奔 过 去 ， 欲 爬 上 电 线 杆 捉 鸟 。 刁 某 某 发 现 后 追 过 去 欲 阻 止 ， 但 为 时 已 晚 ， 等 刁 某 某 赶 到 时 ， 刁 某 已 爬 到 电 线 杆 顶 ， 伸 手 捉 鸟 时 不 慎 触 碰 到 头 顶 的 高 压 电 线 ， 当 即 身 亡 。 其 手 掌 被 外 露 的 钢 枝 刺 穿 ， 尸 体 悬 挂 在 电 线 杆 上 。 26 日 下 午 ， 在 文 地 、 宁 潭 镇 政 府 及 相 关 部 门 协 调 下 ， 相 关 责 任 方 与 死 者 家 属 达 成 赔 偿 协 议 ， 死 者 家 属 同 意 将 死 者 尸 体 取 下 搬 走 ， 相 关 责 任 方 共 赔 偿 死 者 家 属 15. 8 万 元 ， 其 中 宁 潭 镇 某 驾 校 赔 付 7. 3 万 元 ， 死 者 堂 叔 刁 某 某 （ 驾 校 教 练 ） 赔 付 5. 3 万 元 ， 文 地 供 电 所 本 来 没 有 直 接 责 任 ， 但 出 于 人 道 主 义 赔 付 3. 2 万 元 。 （ 原 标 题 ： 为 捉 一 只 鸟 < [UNK] > 赔 上 一 条 命 博 白 一 男 子 爬 电 线 杆 捉 鸟 ， 不 慎 触 电 身 亡 ） [SEP] ， ， 。 ， 的 ， [UNK] 的 的 。 [UNK] 。 。 的 [UNK] [UNK] ， 年 ， 、 ， 成 ， 小 ， 生 ， 是 的 一 的 不 ， 了 ， 说 ， 出 ， 市 ， 下 的 在 ， 行 ， 人 的 年\n"
     ]
    }
   ],
   "source": [
    "model.set_train(False)\n",
    "model.config.eos_token_id = model.config.sep_token_id\n",
    "i = 0\n",
    "for (input_ids, raw_summary) in batched_test_dataset.create_tuple_iterator():\n",
    "    output_ids = model.generate(input_ids, max_new_tokens=50, num_beams=5, no_repeat_ngram_size=2)\n",
    "    output_text = tokenizer.decode(output_ids[0].tolist())\n",
    "    print(output_text)\n",
    "    i += 1\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8654a5d5-d94b-4906-92bf-52d2d85685a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9613743c-3655-45e2-a720-c311a5854c94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "AIGalleryInfo": {
   "item_id": "13ee0843-9209-4b5e-b99f-9ac05d1e0ed6"
  },
  "flavorInfo": {
   "architecture": "X86_64",
   "category": "GPU"
  },
  "imageInfo": {
   "id": "e1a07296-22a8-4f05-8bc8-e936c8e54202",
   "name": "mindspore1.7.0-cuda10.1-py3.7-ubuntu18.04"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
