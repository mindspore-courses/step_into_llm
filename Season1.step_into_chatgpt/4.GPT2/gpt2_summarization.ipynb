{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mindspore\n",
    "import argparse\n",
    "import numpy as np\n",
    "import logging\n",
    "import mindspore.dataset as ds\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from mindspore.nn import CrossEntropyLoss\n",
    "from mindspore import nn, ops\n",
    "from mindspore.train.serialization import save_checkpoint\n",
    "from mindspore.dataset import TextFileDataset\n",
    "\n",
    "from mindnlp.transforms import BertTokenizer\n",
    "from mindnlp.modules import Accumulator\n",
    "from mindnlp.models import GPT2Config, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "batch_size = 8\n",
    "\n",
    "lr = 1e-4\n",
    "warmup_steps = 2000\n",
    "accumulate_step = 2\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "log_step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.utils import cache_file\n",
    "\n",
    "url = 'https://download.mindspore.cn/toolkits/mindnlp/dataset/text_generation/nlpcc2017/train_with_summ.txt'\n",
    "path, _ = cache_file('train_with_summ.txt', './', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TextFileDataset(str(path), shuffle=False)\n",
    "dataset.get_dataset_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset, test_dataset = dataset.split([0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# article: [CLS] xxxxx [SEP]\n",
    "# summary: [CLS] xxxxx [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_dataset(dataset, tokenizer, batch_size=8, max_seq_len=1024, shuffle=False):\n",
    "    def read_map(text):\n",
    "        data = json.loads(text.tobytes())\n",
    "        return np.array(data['article']), np.array(data['summarization'])\n",
    "\n",
    "    def merge_and_pad(article, summary):\n",
    "        article_len = len(article)\n",
    "        summary_len = len(summary)\n",
    "\n",
    "        sep_id = np.array([tokenizer.sep_token_id])\n",
    "        pad_id = np.array([tokenizer.pad_token_id])\n",
    "        if article_len + summary_len > max_seq_len:\n",
    "            new_article_len = max_seq_len - summary_len\n",
    "            merged = np.concatenate([article[:new_article_len], sep_id, summary[1:]])\n",
    "        elif article_len + summary_len - 1 < max_seq_len:\n",
    "            pad_len = max_seq_len - article_len - summary_len + 1\n",
    "            pad_text = np.array([tokenizer.pad_token_id] * pad_len)\n",
    "            merged = np.concatenate([article, summary[1:], pad_text])\n",
    "        else:\n",
    "            merged = np.concatenate([article, summary[1:]])\n",
    "            \n",
    "        return merged.astype(np.int32)\n",
    "\n",
    "    dataset = dataset.map(read_map, 'text', ['article', 'summary'], ['article', 'summary'])\n",
    "    dataset = dataset.map(tokenizer, 'article')\n",
    "    dataset = dataset.map(tokenizer, 'summary')\n",
    "    dataset = dataset.map(merge_and_pad, ['article', 'summary'], ['input_ids'], ['input_ids'])\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = process_dataset(train_dataset, tokenizer)\n",
    "eval_dataset = process_dataset(eval_dataset, tokenizer)\n",
    "test_dataset = process_dataset(test_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tensor(shape=[8, 1024], dtype=Int32, value=\n",
       " [[ 101, 8119,  118 ...    0,    0,    0],\n",
       "  [ 101, 3343, 1814 ... 7164,  511,  102],\n",
       "  [ 101, 6598, 3160 ...    0,    0,    0],\n",
       "  ...\n",
       "  [ 101,  928, 2622 ...    0,    0,    0],\n",
       "  [ 101, 3173, 3857 ...    0,    0,    0],\n",
       "  [ 101, 1346, 5440 ...    0,    0,    0]])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dataset.create_tuple_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindnlp._legacy.amp import auto_mixed_precision\n",
    "\n",
    "config = GPT2Config(vocab_size=len(tokenizer))\n",
    "model = GPT2LMHeadModel(config, ignore_index=tokenizer.pad_token_id)\n",
    "model = auto_mixed_precision(model, 'O1')\n",
    "\n",
    "optimizer = nn.AdamWeightDecay(model.trainable_params(), lr)\n",
    "accumulator = Accumulator(optimizer, accumulate_step, max_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindspore import ops, ms_function\n",
    "from mindspore.amp import init_status, all_finite, DynamicLossScaler\n",
    "\n",
    "loss_scaler = DynamicLossScaler(scale_value=2**10, scale_factor=2, scale_window=1000)\n",
    "\n",
    "def forward_fn(input_ids, labels):\n",
    "    outputs = model(input_ids, labels=labels)\n",
    "    loss = outputs[0]\n",
    "    return loss_scaler.scale(loss / accumulate_step)\n",
    "\n",
    "grad_fn = ops.value_and_grad(forward_fn, None, model.trainable_params())\n",
    "\n",
    "@ms_function\n",
    "def train_step(data, label):\n",
    "    status = init_status()\n",
    "    data = ops.depend(data, status)\n",
    "    loss, grads = grad_fn(data, label)\n",
    "    loss = loss_scaler.unscale(loss)\n",
    "\n",
    "    is_finite = all_finite(grads, status)\n",
    "    if is_finite:\n",
    "        grads = loss_scaler.unscale(grads)\n",
    "        loss = ops.depend(loss, accumulator(grads))\n",
    "    loss = ops.depend(loss, loss_scaler.adjust(is_finite))\n",
    "    return loss, is_finite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 5000/5000 [1:08:18<00:00,  1.22it/s, finite=True, loss=2.9114637, scale_value=32768.0]\n",
      "Epoch 1: 100%|██████████| 5000/5000 [1:06:15<00:00,  1.26it/s, finite=True, loss=2.100337, scale_value=1048576.0]\n",
      "Epoch 2:  60%|█████▉    | 2997/5000 [39:09<25:40,  1.30it/s, finite=True, loss=1.8634704, scale_value=4194304.0]  "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "total = train_dataset.get_dataset_size()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tqdm(total=total) as progress:\n",
    "        progress.set_description(f'Epoch {epoch}')\n",
    "        loss_total = 0\n",
    "        cur_step_nums = 0\n",
    "        for batch_idx, (input_ids,) in enumerate(train_dataset.create_tuple_iterator()):\n",
    "            cur_step_nums += 1\n",
    "            loss, is_finite = train_step(input_ids, input_ids)\n",
    "            loss_total += loss\n",
    "\n",
    "            progress.set_postfix(loss=loss_total/cur_step_nums, finite=is_finite, scale_value=loss_scaler.scale_value.asnumpy())\n",
    "            progress.update(1)\n",
    "        save_checkpoint(model, f'gpt_summarization_epoch_{epoch}.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore2_0",
   "language": "python",
   "name": "mindspore2_0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
