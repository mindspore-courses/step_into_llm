{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c0d934d-352d-41c5-b047-faeaf4607296",
   "metadata": {},
   "source": [
    "# 基于MindSpore的GPT2文本摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d2046e-0419-4c0b-99a3-765defa9d553",
   "metadata": {},
   "source": [
    "该实验可进行在线体验，在线体验链接（https://pangu.huaweicloud.com/gallery/asset-detail.html?id=13ee0843-9209-4b5e-b99f-9ac05d1e0ed6\n",
    "）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb430c0e-fd70-46e2-91e2-b14cf782bd06",
   "metadata": {},
   "source": [
    "## 环境配置\n",
    ">\n",
    ">此为在线运行平台配置python3.9的指南，如在其他环境平台运行案例，请根据实际情况修改如下代码\n",
    ">\n",
    "\n",
    "1. 配置python3.9环境\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9f085c7-b2b3-4b18-95f9-13b298b10d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "!/home/ma-user/anaconda3/bin/conda create -n python-3.9.0 python=3.9.0 -y --override-channels --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n",
    "!/home/ma-user/anaconda3/envs/python-3.9.0/bin/pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9436813-c813-425e-b1ed-af0534ef862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "data = {\n",
    "   \"display_name\": \"python-3.9.0\",\n",
    "   \"env\": {\n",
    "      \"PATH\": \"/home/ma-user/anaconda3/envs/python-3.9.0/bin:/home/ma-user/anaconda3/envs/python-3.7.10/bin:/modelarts/authoring/notebook-conda/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/ma-user/modelarts/ma-cli/bin:/home/ma-user/modelarts/ma-cli/bin\"\n",
    "   },\n",
    "   \"language\": \"python\",\n",
    "   \"argv\": [\n",
    "      \"/home/ma-user/anaconda3/envs/python-3.9.0/bin/python\",\n",
    "      \"-m\",\n",
    "      \"ipykernel\",\n",
    "      \"-f\",\n",
    "      \"{connection_file}\"\n",
    "   ]\n",
    "}\n",
    "\n",
    "if not os.path.exists(\"/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/\"):\n",
    "    os.mkdir(\"/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/\")\n",
    "\n",
    "with open('/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/kernel.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdfae31-df62-4746-bd3e-1c556413ffd2",
   "metadata": {},
   "source": [
    "***注：以上代码执行完成后，需点击左上角或右上角将kernel更换为python-3.9.0***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d89aafe-df20-48de-80a7-83408bed00be",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://mindspore-demo.obs.cn-north-4.myhuaweicloud.com/imgs/ai-gallery/change-kernel.PNG\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757cb29c-48f4-4b6b-9b71-6c008e629eb8",
   "metadata": {},
   "source": [
    "2. 安装mindspore2.2.12，安装指南详见：[MindSpore安装](https://www.mindspore.cn/install)\n",
    "3. 安装MindNLP及相关依赖，MindNLP官方仓详见：[MindNLP](https://github.com/mindspore-lab/mindnlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6491560b-1ec6-4ca1-88cc-c7f9c1297725",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "!pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.2.14/MindSpore/unified/x86_64/mindspore-2.2.11-cp39-cp39-linux_x86_64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install tokenizers==0.15.2 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!wget https://mindspore-demo.obs.cn-north-4.myhuaweicloud.com/mindnlp_install/mindnlp-0.3.1-py3-none-any.whl\n",
    "!pip install mindnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faae5c6-8397-4574-9e9c-f86230bb071a",
   "metadata": {},
   "source": [
    "***注：执行如上命令完成安装后，请点击上方的restart kernel图标重启kernel，再进行实验***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb699e4a-a2dc-44f2-b3cb-6b86fa9b24f6",
   "metadata": {},
   "source": [
    "### 数据集加载与处理\n",
    "\n",
    "1. 数据集加载\n",
    "\n",
    "    本次实验使用的是nlpcc2017摘要数据，内容为新闻正文及其摘要，总计50000个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27bf5931-7b09-4984-841c-fbea311d3955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "Building prefix dict from the default dictionary ...\n",
      "\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "\n",
      "Loading model cost 0.761 seconds.\n",
      "\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.utils import http_get\n",
    "\n",
    "# download dataset\n",
    "url = 'https://download.mindspore.cn/toolkits/mindnlp/dataset/text_generation/nlpcc2017/train_with_summ.txt'\n",
    "path = http_get(url, './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5868b6-7a52-4f97-b934-4d3632a978a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mindspore.dataset import TextFileDataset\n",
    "\n",
    "# load dataset\n",
    "dataset = TextFileDataset(str(path), shuffle=False)\n",
    "dataset.get_dataset_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2697c68-be45-44c1-bc26-7c04424ebf0c",
   "metadata": {},
   "source": [
    "**本案例默认在GPU P100上运行，因中文文本，tokenizer使用的是bert tokenizer而非gpt tokenizer等原因，全量数据训练1个epoch的时间约为80分钟。**\n",
    "\n",
    "**为节约时间，我们选取了数据集中很小的一个子集（500条数据）来演示gpt2的微调和推理全流程，但由于数据量不足，会导致模型效果较差。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf79231-864c-4e11-9409-995c95cdb30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and testing dataset\n",
    "mini_dataset, _ = dataset.split([0.01, 0.99], randomize=False)\n",
    "train_dataset, test_dataset = mini_dataset.split([0.9, 0.1], randomize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f0d574-53a0-4bac-9303-6eb769418c04",
   "metadata": {},
   "source": [
    "2. 数据预处理\n",
    "\n",
    "    原始数据格式：\n",
    "    ```text\n",
    "    article: [CLS] article_context [SEP]\n",
    "    summary: [CLS] summary_context [SEP]\n",
    "    ```\n",
    "    预处理后的数据格式：\n",
    "\n",
    "    ```text\n",
    "    [CLS] article_context [SEP] summary_context [SEP]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1ee1961-0658-4e70-95c2-81fefd83a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# preprocess dataset\n",
    "def process_dataset(dataset, tokenizer, batch_size=4, max_seq_len=1024, shuffle=False):\n",
    "    def read_map(text):\n",
    "        data = json.loads(text.tobytes())\n",
    "        return np.array(data['article']), np.array(data['summarization'])\n",
    "\n",
    "    def merge_and_pad(article, summary):\n",
    "        # tokenization\n",
    "        # pad to max_seq_length, only truncate the article\n",
    "        tokenized = tokenizer(text=article, text_pair=summary,\n",
    "                              padding='max_length', truncation='only_first', max_length=max_seq_len)\n",
    "        return tokenized['input_ids'], tokenized['input_ids']\n",
    "    \n",
    "    dataset = dataset.map(read_map, 'text', ['article', 'summary'])\n",
    "    # change column names to input_ids and labels for the following training\n",
    "    dataset = dataset.map(merge_and_pad, ['article', 'summary'], ['input_ids', 'labels'])\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ce3dab-9486-4365-be7c-34bd5a761080",
   "metadata": {},
   "source": [
    "因GPT2无中文的tokenizer，我们使用BertTokenizer替代。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3cd8e57-72bc-4d2e-b38d-38b24efadd49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mindnlp.transformers import BertTokenizer\n",
    "\n",
    "# We use BertTokenizer for tokenizing chinese context.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e89c26b-4970-449a-a0c4-b6c61845e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = process_dataset(train_dataset, tokenizer, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b65cc13-0a52-4bae-ab5f-ebb813a4d3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tensor(shape=[1, 1024], dtype=Int64, value=\n",
       " [[ 101, 1724, 3862 ...    0,    0,    0]]),\n",
       " Tensor(shape=[1, 1024], dtype=Int64, value=\n",
       " [[ 101, 1724, 3862 ...    0,    0,    0]])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dataset.create_tuple_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1497ee-2ad1-4da8-b659-9c7f2d45fccc",
   "metadata": {},
   "source": [
    "### 模型构建\n",
    "\n",
    "1. 构建GPT2ForSummarization模型，注意***shift right***的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f295944-ea2e-41e1-8301-472e09223792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import ops\n",
    "from mindnlp.transformers import GPT2LMHeadModel\n",
    "\n",
    "class GPT2ForSummarization(GPT2LMHeadModel):\n",
    "    def construct(\n",
    "        self,\n",
    "        input_ids = None,\n",
    "        attention_mask = None,\n",
    "        labels = None,\n",
    "    ):\n",
    "        outputs = super().construct(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        shift_logits = outputs.logits[..., :-1, :]\n",
    "        shift_labels = labels[..., 1:]\n",
    "        # Flatten the tokens\n",
    "        loss = ops.cross_entropy(shift_logits.view(-1, shift_logits.shape[-1]), shift_labels.view(-1), ignore_index=tokenizer.pad_token_id)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6af843-64d7-49a3-875f-605d6b2e74b2",
   "metadata": {},
   "source": [
    "2. 动态学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73c7be3d-44dc-49d4-abd8-c41f316a28d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import ops\n",
    "from mindspore.nn.learning_rate_schedule import LearningRateSchedule\n",
    "\n",
    "class LinearWithWarmUp(LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    Warmup-decay learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate, num_warmup_steps, num_training_steps):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "\n",
    "    def construct(self, global_step):\n",
    "        if global_step < self.num_warmup_steps:\n",
    "            return global_step / float(max(1, self.num_warmup_steps)) * self.learning_rate\n",
    "        return ops.maximum(\n",
    "            0.0, (self.num_training_steps - global_step) / (max(1, self.num_training_steps - self.num_warmup_steps))\n",
    "        ) * self.learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e9db2-11df-4cc4-8bef-87d473d99e5a",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a655320-2d05-4c93-bc8b-f1b4f45f809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "warmup_steps = 100\n",
    "learning_rate = 1.5e-4\n",
    "\n",
    "num_training_steps = num_epochs * train_dataset.get_dataset_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81ac9003-2dcf-42f8-b42d-3a788f172d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import nn\n",
    "from mindnlp.transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "config = GPT2Config(vocab_size=len(tokenizer))\n",
    "model = GPT2ForSummarization(config)\n",
    "\n",
    "lr_scheduler = LinearWithWarmUp(learning_rate=learning_rate, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps)\n",
    "optimizer = nn.AdamWeightDecay(model.trainable_params(), learning_rate=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2803c71c-3591-48cf-a6a9-6b840af749bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of model parameters: 102068736\n"
     ]
    }
   ],
   "source": [
    "# 记录模型参数数量\n",
    "print('number of model parameters: {}'.format(model.num_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1492649c-dfdb-4cfd-85bb-aef478aff5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use 'StaticLossScaler' with `scale_value=2 ** 10` when `loss_scaler` is None.\n"
     ]
    }
   ],
   "source": [
    "from mindnlp._legacy.engine import Trainer\n",
    "from mindnlp._legacy.engine.callbacks import CheckpointCallback\n",
    "\n",
    "ckpoint_cb = CheckpointCallback(save_path='checkpoint', ckpt_name='gpt2_summarization',\n",
    "                                epochs=1, keep_checkpoint_max=2)\n",
    "\n",
    "trainer = Trainer(network=model, train_dataset=train_dataset,\n",
    "                  epochs=num_epochs, optimizer=optimizer, callbacks=ckpoint_cb)\n",
    "trainer.set_amp(level='O1')  # 开启混合精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf47838-460a-49e4-8850-f10fe7b5ff2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train will start from the checkpoint saved in 'checkpoint'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/450 [00:00<?, ?it/s][WARNING] KERNEL(18952,7f8cb7fff700,python):2024-05-26-02:43:45.191.987 [mindspore/ccsrc/plugin/device/gpu/kernel/gpu_kernel.cc:40] CheckDeviceSm] It is recommended to use devices with a computing capacity >= 7, but the current device's computing capacity is 6\n",
      "\n",
      "Epoch 0: 100%|██████████| 450/450 [04:18<00:00,  1.74it/s, loss=7.1492634]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: 'gpt2_summarization_epoch_0.ckpt' has been saved in epoch: 0.\n"
     ]
    }
   ],
   "source": [
    "trainer.run(tgt_columns=\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "787795ec-0c07-4be6-97b7-4defbe899117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_dataset(dataset, tokenizer, batch_size=1, max_seq_len=1024, max_summary_len=100):\n",
    "    def read_map(text):\n",
    "        data = json.loads(text.tobytes())\n",
    "        return np.array(data['article']), np.array(data['summarization'])\n",
    "\n",
    "    def pad(article):\n",
    "        tokenized = tokenizer(text=article, truncation=True, max_length=max_seq_len-max_summary_len)\n",
    "        return tokenized['input_ids']\n",
    "\n",
    "    dataset = dataset.map(read_map, 'text', ['article', 'summary'])\n",
    "    dataset = dataset.map(pad, 'article', ['input_ids'])\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "996842c4-f793-4393-ae64-2d4b065bc9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_test_dataset = process_test_dataset(test_dataset, tokenizer, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10421e6d-ec81-435d-9944-f1e35cd3eae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 101,  126, 3299, 8110, 3189, 3241, 8024, 3633,  966, 2844, 1894,\n",
      "\n",
      "        5688,  511, 3727,  704, 2356,  704, 2552, 1278, 7368, 6117, 3890,\n",
      "\n",
      "        7599, 3969, 1048, 4554, 4906, 2844, 1894, 2207, 6145, 8024, 3633,\n",
      "\n",
      "        1762, 3146, 4415, 2843, 3131, 2642, 5442, 4500, 4638, 4289, 1501,\n",
      "\n",
      "        8024, 1762, 3690, 3187, 7344, 1906, 4638, 2658, 1105,  678, 8024,\n",
      "\n",
      "        6158,  671, 4511, 2094,  697, 3613, 3666, 2802,  511,  113, 1290,\n",
      "\n",
      "        1555, 2845,  126, 3299, 8115, 3189,  100, 4276, 3295, 2845, 6887,\n",
      "\n",
      "         114,  126, 3299, 8130, 3189,  677, 1286, 8024, 1290, 1555, 2845,\n",
      "\n",
      "        6381, 5442,  794, 1062, 2128, 3727, 1378, 6356, 3175,  749, 6237,\n",
      "\n",
      "        1168, 8024, 2802,  782, 4511, 2094, 1426, 3378, 2347, 6158, 6121,\n",
      "\n",
      "        3124, 2872, 4522,  511, 2945, 6356, 3175,  792, 5305, 8024,  126,\n",
      "\n",
      "        3299, 8110, 3189, 2496, 3241, 8024, 1728, 1426, 3378, 5326, 3678,\n",
      "\n",
      "        1728, 2843, 3131, 3187, 3126, 3647,  767, 8024, 6628, 1168, 1278,\n",
      "\n",
      "        7368, 4638, 1426, 3378, 2658, 5328, 4080, 1220, 8024, 2199, 2844,\n",
      "\n",
      "        1894, 4991, 2844, 1894, 2207, 6145, 2802,  839, 8024, 2193, 5636,\n",
      "\n",
      "        2207, 6145, 1928, 6956, 1912,  839, 8024, 1059, 6716, 1914, 1905,\n",
      "\n",
      "        6763, 5299, 5302, 2919,  839,  511, 1278, 7368, 2845, 6356, 1400,\n",
      "\n",
      "        8024, 1426, 3378, 6845, 4895, 4385, 1767,  511,  752,  816, 1355,\n",
      "\n",
      "        4495, 1400, 8024, 3696, 6356, 5018,  671, 3198, 7313, 2245, 2458,\n",
      "\n",
      "        6444, 3389, 8024, 7219, 2137, 2066, 4542,  782, 1426, 3378,  511,\n",
      "\n",
      "        7390, 1400, 8024, 1426, 3378, 6158, 6356, 3175,  837, 1542, 8024,\n",
      "\n",
      "        2190, 6824, 3791,  752, 2141,  897, 6371,  679, 6383,  511, 4680,\n",
      "\n",
      "        1184, 8024, 1426, 3378, 1728, 3868, 2066, 2192, 6119, 3996,  752,\n",
      "\n",
      "        8024, 6158, 6356, 3175, 6121, 3124, 2872, 4522, 8115, 3189,  511,\n",
      "\n",
      "        1290, 1555, 2845, 6381, 5442,  794, 3727,  704, 2356,  704, 2552,\n",
      "\n",
      "        1278, 7368,  749, 6237, 1168, 8024, 2207, 6145, 2347, 1139, 7368,\n",
      "\n",
      "        8024, 1726, 1168, 5632, 2346, 4638, 2339,  868, 2266,  855,  511,\n",
      "\n",
      "        1290, 1555, 6381, 5442,  133,  100,  135, 4374,  778, 5356, 6782,\n",
      "\n",
      "        8038, 1290, 1555, 2845,  897, 4943,  102]], dtype=int64), array(['汉中男子因继母抢救无效死亡迁怒护士，将其殴打至头部外伤、多处软组织挫伤；因涉嫌寻衅滋事被拘留(图)'], dtype='<U49')]\n"
     ]
    }
   ],
   "source": [
    "print(next(batched_test_dataset.create_tuple_iterator(output_numpy=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a5288a4-f584-479b-9653-bda060e00279",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('./checkpoint/gpt2_summarization_epoch_0.ckpt', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1631f82e-857d-413a-a298-ff6f66d4a286",
   "metadata": {},
   "source": [
    "由于训练数据量少，epochs数少且tokenizer并未使用gpt tokenizer等因素，模型推理效果会较差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "090be0ea-3467-4eca-abb6-129d451f58ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages/mindnlp/transformers/generation/utils.py:1402: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://hf-mirror.com/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "\n",
      "  warnings.warn(\n",
      "\n",
      "[WARNING] KERNEL(24053,7f01dd358700,python):2024-05-26-02:50:12.325.792 [mindspore/ccsrc/plugin/device/gpu/kernel/gpu_kernel.cc:40] CheckDeviceSm] It is recommended to use devices with a computing capacity >= 7, but the current device's computing capacity is 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 5 月 12 日 晚 ， 正 值 护 士 节 。 汉 中 市 中 心 医 院 血 液 风 湿 免 疫 科 护 士 小 袁 ， 正 在 整 理 抢 救 患 者 用 的 物 品 ， 在 毫 无 防 备 的 情 况 下 ， 被 一 男 子 两 次 殴 打 。 ( 华 商 报 5 月 15 日 [UNK] 版 曾 报 道 ) 5 月 22 日 上 午 ， 华 商 报 记 者 从 公 安 汉 台 警 方 了 解 到 ， 打 人 男 子 吴 某 已 被 行 政 拘 留 。 据 警 方 介 绍 ， 5 月 12 日 当 晚 ， 因 吴 某 继 母 因 抢 救 无 效 死 亡 ， 赶 到 医 院 的 吴 某 情 绪 激 动 ， 将 护 士 站 护 士 小 袁 打 伤 ， 导 致 小 袁 头 部 外 伤 ， 全 身 多 处 软 组 织 挫 伤 。 医 院 报 警 后 ， 吴 某 逃 离 现 场 。 事 件 发 生 后 ， 民 警 第 一 时 间 展 开 调 查 ， 锁 定 嫌 疑 人 吴 某 。 随 后 ， 吴 某 被 警 方 传 唤 ， 对 违 法 事 实 供 认 不 讳 。 目 前 ， 吴 某 因 涉 嫌 寻 衅 滋 事 ， 被 警 方 行 政 拘 留 15 日 。 华 商 报 记 者 从 汉 中 市 中 心 医 院 了 解 到 ， 小 袁 已 出 院 ， 回 到 自 己 的 工 作 岗 位 。 华 商 记 者 < [UNK] > 王 亮 编 辑 ： 华 商 报 供 稿 [SEP] ， 日 ， ， 。 。 ， 人 ， [UNK] ， 道 ， 中 ， 的 ， 年 ， 出 ， 时 ，. ， - ， 天 ， 月 ， 场 ， 温 ， 后 。 日 日 人 。 [UNK] 。 道 。 中 。 的 。 人\n"
     ]
    }
   ],
   "source": [
    "model.set_train(False)\n",
    "model.config.eos_token_id = model.config.sep_token_id\n",
    "i = 0\n",
    "for (input_ids, raw_summary) in batched_test_dataset.create_tuple_iterator():\n",
    "    output_ids = model.generate(input_ids, max_new_tokens=50, num_beams=5, no_repeat_ngram_size=2)\n",
    "    output_text = tokenizer.decode(output_ids[0].tolist())\n",
    "    print(output_text)\n",
    "    i += 1\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8654a5d5-d94b-4906-92bf-52d2d85685a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "AIGalleryInfo": {
   "item_id": "13ee0843-9209-4b5e-b99f-9ac05d1e0ed6"
  },
  "flavorInfo": {
   "architecture": "X86_64",
   "category": "GPU"
  },
  "imageInfo": {
   "id": "e1a07296-22a8-4f05-8bc8-e936c8e54202",
   "name": "mindspore1.7.0-cuda10.1-py3.7-ubuntu18.04"
  },
  "kernelspec": {
   "display_name": "ms2.2.14",
   "language": "python",
   "name": "ms2.2.14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
