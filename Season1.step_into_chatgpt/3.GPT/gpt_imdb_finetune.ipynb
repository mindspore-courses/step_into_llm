{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"./assets/table_of_contents_4.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在模型finetune中，需要根据不同的下游任务来处理输入，主要的下游任务可分为以下四类：\n",
    "\n",
    "- 分类（Classification）：给定一个输入文本，将其分为若干类别中的一类，如情感分类、新闻分类等;\n",
    "- 蕴含（Entailment）：给定两个输入文本，判断它们之间是否存在蕴含关系（即一个文本是否可以从另一个文本中推断出来）;\n",
    "- 相似度（Similarity）：给定两个输入文本，计算它们之间的相似度得分;\n",
    "- 多项选择题（Multiple choice）：给定一个问题和若干个答案选项，选择最佳的答案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"./assets/finetune_tasks.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用IMDb数据集，通过finetune GPT进行情感分类任务。\n",
    "\n",
    "IMDb数据集是一个常用的情感分类数据集，其中包含50,000条影评文本，其中25,000条用作训练数据，另外25,000条用作测试数据。每个样本都有一个二元标签，表示影评的情感是正面还是负面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] ME(19606:140553655732032,MainProcess):2023-05-06-18:07:00.195.685 [mindspore/run_check/_check_version.py:226] Cuda ['10.1', '11.1', '11.6'] version(libcu*.so need by mindspore-gpu) is not found. Please confirm that the path of cuda is set to the env LD_LIBRARY_PATH, or check whether the CUDA version in wheel package and the CUDA runtime in current device matches. Please refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[ERROR] ME(19606:140553655732032,MainProcess):2023-05-06-18:07:00.218.403 [mindspore/run_check/_check_version.py:226] Cuda ['10.1', '11.1', '11.6'] version(libcudnn*.so need by mindspore-gpu) is not found. Please confirm that the path of cuda is set to the env LD_LIBRARY_PATH, or check whether the CUDA version in wheel package and the CUDA runtime in current device matches. Please refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "/data/miniconda3/envs/mindspore/lib/python3.7/site-packages/mindnlp/utils/download.py:26: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "[WARNING] ME(19606:140553655732032,MainProcess):2023-05-06-18:07:01.457.447 [mindspore/ops/operations/math_ops.py:4741] The 'NPUAllocFloatStatus' operator will be deprecated in the future. Please don't use it.\n",
      "[WARNING] ME(19606:140553655732032,MainProcess):2023-05-06-18:07:01.458.583 [mindspore/ops/operations/math_ops.py:4875] The 'NPUClearFloatStatus' operator will be deprecated in the future. Please don't use it.\n",
      "[WARNING] ME(19606:140553655732032,MainProcess):2023-05-06-18:07:01.459.431 [mindspore/ops/operations/math_ops.py:4811] The 'NPUGetFloatStatus' operator will be deprecated in the future. Please don't use it.\n",
      "[WARNING] ME(19606:140553655732032,MainProcess):2023-05-06-18:07:01.460.366 [mindspore/common/api.py:844] 'mindspore.ms_class' will be deprecated and removed in a future version. Please use 'mindspore.jit_class' instead.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import mindspore\n",
    "from mindspore.dataset import text, GeneratorDataset, transforms\n",
    "from mindspore import nn\n",
    "\n",
    "from mindnlp import load_dataset\n",
    "from mindnlp.transforms import PadTransform, GPTTokenizer\n",
    "\n",
    "from mindnlp.engine import Trainer, Evaluator\n",
    "from mindnlp.engine.callbacks import CheckpointCallback, BestModelCallback\n",
    "from mindnlp.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "imdb_train, imdb_test = load_dataset('imdb', shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过`load_dataset`加载IMDb数据集后，我们需要对数据进行如下处理：\n",
    "\n",
    "- 将文本内容进行分词，并映射为对应的数字索引；\n",
    "- 统一序列长度：超过进行截断，不足通过`<pad>`占位符进行补全；\n",
    "- 按照分类任务的输入要求，在句首和句末分别添加*Start*与*Extract*占位符（此处用`<bos>`与`<eos>`表示）；\n",
    "- 批处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_dataset(dataset, tokenizer, max_seq_len=256, batch_size=32, shuffle=False):\n",
    "    \"\"\"数据集预处理\"\"\"\n",
    "    def pad_sample(text):\n",
    "        if len(text) + 2 >= max_seq_len:\n",
    "            return np.concatenate(\n",
    "                [np.array([tokenizer.bos_token_id]), text[: max_seq_len-2], np.array([tokenizer.eos_token_id])]\n",
    "            )\n",
    "        else:\n",
    "            pad_len = max_seq_len - len(text) - 2\n",
    "            return np.concatenate( \n",
    "                [np.array([tokenizer.bos_token_id]), text,\n",
    "                 np.array([tokenizer.eos_token_id]),\n",
    "                 np.array([tokenizer.pad_token_id] * pad_len)]\n",
    "            )\n",
    "\n",
    "    column_names = [\"text\", \"label\"]\n",
    "    rename_columns = [\"input_ids\", \"label\"]\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.map(operations=[tokenizer, pad_sample], input_columns=\"text\")\n",
    "    dataset = dataset.rename(input_columns=column_names, output_columns=rename_columns)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载GPT tokenizer，并添加上述使用到的`<bos>`, `<eos>`, `<pad>`占位符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tokenizer = GPTTokenizer.from_pretrained('openai-gpt')\n",
    "\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<bos>\",\n",
    "    \"eos_token\": \"<eos>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "}\n",
    "num_added_toks = gpt_tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于IMDb数据集本身不包含验证集，我们手动将其分割为训练和验证两部分，比例取0.7, 0.3。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train, imdb_val = imdb_train.split([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = process_dataset(imdb_train, gpt_tokenizer, shuffle=True)\n",
    "dataset_val = process_dataset(imdb_val, gpt_tokenizer)\n",
    "dataset_test = process_dataset(imdb_test, gpt_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练\n",
    "\n",
    "同BERT课件中的情感分类任务实现，这里我们依旧使用了混合精度。另外需要注意的一点是，由于在前序数据处理中，我们添加了3个特殊占位符，所以在token embedding中需要调整词典的大小(vocab_size + 3)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(19606:140553655732032,MainProcess):2023-05-06-18:07:05.507.135 [mindspore/train/serialization.py:1088] For 'load_param_into_net', 1 parameters in the 'net' are not loaded, because they are not in the 'parameter_dict', please check whether the network structure is consistent when training and loading checkpoint.\n",
      "[WARNING] ME(19606:140553655732032,MainProcess):2023-05-06-18:07:05.509.370 [mindspore/train/serialization.py:1090] score.weight is not loaded.\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.models import GPTForSequenceClassification\n",
    "from mindnlp._legacy.amp import auto_mixed_precision\n",
    "\n",
    "model = GPTForSequenceClassification.from_pretrained('openai-gpt', num_labels=2)\n",
    "model.pad_token_id = gpt_tokenizer.pad_token_id\n",
    "model.resize_token_embeddings(model.config.vocab_size + 3)\n",
    "model = auto_mixed_precision(model, 'O1')\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = nn.Adam(model.trainable_params(), learning_rate=2e-5)\n",
    "\n",
    "metric = Accuracy()\n",
    "\n",
    "ckpoint_cb = CheckpointCallback(save_path='checkpoint', ckpt_name='sentiment_model', epochs=1, keep_checkpoint_max=2)\n",
    "best_model_cb = BestModelCallback(save_path='checkpoint', auto_load=True)\n",
    "\n",
    "trainer = Trainer(network=model, train_dataset=dataset_train,\n",
    "                  eval_dataset=dataset_val, metrics=metric,\n",
    "                  epochs=3, loss_fn=loss, optimizer=optimizer, callbacks=[ckpoint_cb, best_model_cb],\n",
    "                  jit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train will start from the checkpoint saved in 'checkpoint'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfb9611b9464b0b9a0115efad17d51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: 'sentiment_model_epoch_0.ckpt' has been saved in epoch: 0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f49b726a2b44ab862c26aefd46290a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Score: {'Accuracy': 0.9098666666666667}\n",
      "---------------Best Model: 'best_so_far.ckpt' has been saved in epoch: 0.---------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6562340bc0a1430babeb728a44b38ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: 'sentiment_model_epoch_1.ckpt' has been saved in epoch: 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f644d45327412195f241b67fff71ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Score: {'Accuracy': 0.9397333333333333}\n",
      "---------------Best Model: 'best_so_far.ckpt' has been saved in epoch: 1.---------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8e14852e164433b9d20e04a78c3caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of stored checkpoints has been reached.\n",
      "Checkpoint: 'sentiment_model_epoch_2.ckpt' has been saved in epoch: 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2523c44aab41fdb463862aec2a3635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Score: {'Accuracy': 0.9598666666666666}\n",
      "---------------Best Model: 'best_so_far.ckpt' has been saved in epoch: 2.---------------\n",
      "Loading best model from 'checkpoint' with '['Accuracy']': [0.9598666666666666]...\n",
      "---------------The model is already load the best model from 'best_so_far.ckpt'.---------------\n"
     ]
    }
   ],
   "source": [
    "trainer.run(tgt_columns=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245ad103131d4180b457ec1060ba219c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Score: {'Accuracy': 0.91828}\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(network=model, eval_dataset=dataset_test, metrics=metric)\n",
    "evaluator.run(tgt_columns=\"label\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
