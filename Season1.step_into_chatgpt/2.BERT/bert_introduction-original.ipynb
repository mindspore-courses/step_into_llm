{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35bd8ac0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div align=center><img src=\"./assets/cover.png\" alt=\"bert-cover\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576ed71b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div align=center><img src=\"./assets/contents_1_2.png\" alt=\"table-of-contents\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f34239",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=center><img src=\"./assets/qr_code.jpeg\" alt=\"table-of-contents\" width=\"1300\"></div>\n",
    "\n",
    "https://github.com/mindspore-courses/step_into_chatgpt\n",
    "\n",
    "https://github.com/mindspore-lab/mindnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fabf36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3202年了，为什么还要学BERT\n",
    "\n",
    "- ### Decoder only模型当道： GPT3、Bloom、LLAMA、GLM\n",
    "\n",
    "    - #### Transformer Encoder结构在生成式任务上的缺陷\n",
    "\n",
    "    - #### BERT模型规模小\n",
    "\n",
    "    - #### Pretrain-Fintune范式的落寞\n",
    "\n",
    "- ### 2022年以前，学术界还是在倒腾BERT\n",
    "\n",
    "    - #### Finetune更容易针对单领域任务训练\n",
    "    \n",
    "    - #### BERT是首个大规模并行预训练的模型，也是当前的performance baseline\n",
    "    \n",
    "    - #### 由BERT入手学大模型训练、微调、Prompt最简单"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a7118",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# NLP中的预训练模型\n",
    "\n",
    "语言模型的演变经历了以下几个阶段：\n",
    "\n",
    "<div align=center><img src=\"./assets/language_models.svg\" alt=\"language-models\" width=\"1000\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558644f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. `word2vec`/`Glove`将离散的文本数据转换为固定长度的**静态词向量**，后根据下游任务训练不同的**语言模型**；\n",
    "\n",
    "2. `ELMo`预训练模型将文本数据*结合上下文信息*，转换为**动态词向量**，后根据下游任务训练不同的**语言模型**；\n",
    "\n",
    "3. `BERT`同样将文本数据转换为**动态词向量**，能够更好地捕捉*句子级别的信息与语境信息*，后续只需finetune最后的**输出层**即可适配下游任务；\n",
    "\n",
    "4. `GPT`等预训练语言模型主要用于*文本生成类任务*，需要通过**prompt方法**来应用于下游任务，指导模型生成特定的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242c5a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# BERT\n",
    "\n",
    "<div align=center><img src=\"./assets/BERT.jpg\" width=\"500\"></div>\n",
    "\n",
    "BERT模型本质上是结合了`ELMo`模型与`GPT`模型的优势。\n",
    "\n",
    "- 相比于ELMo，BERT仅需改动最后的输出层，而非模型架构，便可以在下游任务中达到很好的效果；\n",
    "- 相比于GPT，BERT在处理词元表示时考虑到了双向上下文的信息；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a5669",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# BERT 结构\n",
    "\n",
    "BERT（Bidirectional Encoder Representation from Transformers）是一个**仅有Encoder的Transformer**。\n",
    "\n",
    "我们调用`BertModel`来搭建BERT模型，并通过`BertConfig`配置模型相关参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b685c5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pretrain.src.bert import BertModel\n",
    "from pretrain.src.config import BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27de31d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BERT 输入\n",
    "\n",
    "- `Transformer`：Encoder接受源句子（source sentence），Decoder接受目标句子（target sentence）；\n",
    "- `BERT`: \n",
    "    - 针对句子对相关任务，将两个句子合并为一个句子对输入到Encoder中，\\<cls\\> + 第一个句子 + \\<sep\\> + 第二个句子 + \\<sep\\>;\n",
    "    - 针对单个文本相关任务，\\<cls\\> + 句子 + \\<sep\\>。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f40040",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=center><img src=\"./assets/transformer_vs_bert.png\" alt=\"transformer-vs-bert\" width=\"1000\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf4efc6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BERT Embedding\n",
    "\n",
    "BERT Embedding与Transformer中的Embedding操作类似，包含词嵌入（word embedding）与位置嵌入（positional embedding）。\n",
    "\n",
    "但与Transformer不同的是，BERT使用了可学习的位置信息，并额外增加了表示区分不同句子的段嵌入（segment embedding）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1d92c5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=center><img src=\"./assets/BERT-embedding.ppm\" alt=\"bert-embedding\" width=\"1000\"></div>\n",
    "\n",
    "> <font size=2>图片来源：Devlin, J.; Chang, M. W.; Lee, K.; Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04340e2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mindspore\n",
    "from mindspore import nn\n",
    "import mindspore.common.dtype as mstype\n",
    "from mindspore.common.initializer import initializer, TruncatedNormal\n",
    "\n",
    "class BertEmbeddings(nn.Cell):\n",
    "    \"\"\"\n",
    "    Embeddings for BERT, include word, position and token_type\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, embedding_table=TruncatedNormal(config.initializer_range))\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size, embedding_table=TruncatedNormal(config.initializer_range))\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size, embedding_table=TruncatedNormal(config.initializer_range))\n",
    "        self.layer_norm = nn.LayerNorm((config.hidden_size,), epsilon=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(1 - config.hidden_dropout_prob)\n",
    "\n",
    "    def construct(self, input_ids, token_type_ids=None, position_ids=None):\n",
    "        seq_len = input_ids.shape[1]\n",
    "        if position_ids is None:\n",
    "            position_ids = mnp.arange(seq_len)\n",
    "            position_ids = position_ids.expand_dims(0).expand_as(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = ops.zeros_like(input_ids)\n",
    "        \n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24afcb4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BERT 模型构建\n",
    "\n",
    "BERT模型的构建与上一节课程的Transformer Encoder构建类似。\n",
    "\n",
    "分别构建multi-head attention层，feed-forward network，并在中间用add&norm连接，最后通过线性层与softmax层进行输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0df426",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BERT 预训练\n",
    "\n",
    "BERT通过Masked LM（masked language model）与NSP（next sentence prediction）获取词语和句子级别的特征。\n",
    "\n",
    "<div align=center><img src=\"./assets/bert_pretrain_finetune.jpg\" alt=\"bert-pretrain\" width=\"1000\"></div>\n",
    "\n",
    "> <font size=2>图片来源：Devlin, J.; Chang, M. W.; Lee, K.; Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef77d903",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Masked Language Model (Masked LM)\n",
    "\n",
    "BERT模型通过Masked LM捕捉**词语层面**的信息。\n",
    "\n",
    "我们随机将每个句子中15%的词语进行遮盖，替换成掩码\\<mask\\>。在训练过程中，模型会对句子进行“完形填空”，预测这些被遮盖的词语是什么，通过减小被mask词语的损失值来对模型进行优化。\n",
    "\n",
    "<div align=center><img src=\"./assets/masked_lm.png\" alt=\"masked-lm\" width=\"1000\"></div>\n",
    "\n",
    "> <font size=2>图片来源: [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa96afc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "由于\\<mask\\>仅在预训练中出现，为了让预训练和微调中的数据处理尽可能接近，我们在随机mask的时候进行如下操作：\n",
    "- 80%的概率替换为\\<mask\\>\n",
    "- 10%的概率替换为文本中的随机词\n",
    "- 10%的概率不进行替换，保持原有的词元\n",
    "\n",
    "<div align=center><img src=\"./assets/masked_lm_2.png\" alt=\"masked-lm-2\" width=\"1000\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4469a1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "我们通过`BERTPredictionHeadTranform`实现单层感知机，对被遮盖的词元进行预测。在前向网络中，我们需要输入BERT模型的编码结果`hidden_states`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d4e932",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "activation_map = {\n",
    "    'relu': nn.ReLU(),\n",
    "    'gelu': nn.GELU(False),\n",
    "    'gelu_approximate': nn.GELU(),\n",
    "    'swish':nn.SiLU()\n",
    "}\n",
    "\n",
    "class BertPredictionHeadTransform(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Dense(config.hidden_size, config.hidden_size, weight_init=TruncatedNormal(config.initializer_range))\n",
    "        self.transform_act_fn = activation_map.get(config.hidden_act, nn.GELU(False))\n",
    "        self.layer_norm = nn.LayerNorm((config.hidden_size,), epsilon=config.layer_norm_eps)\n",
    "    \n",
    "    def construct(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c825d61f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "根据被遮盖的词元位置`masked_lm_positions`，获得这些词元的预测输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0cc3f8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import mindspore.ops as ops\n",
    "import mindspore.numpy as mnp\n",
    "from mindspore import Parameter, Tensor\n",
    "\n",
    "class BertLMPredictionHead(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(BertLMPredictionHead, self).__init__()\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "        self.decoder = nn.Dense(config.hidden_size, config.vocab_size, has_bias=False, weight_init=TruncatedNormal(config.initializer_range))\n",
    "\n",
    "        self.bias = Parameter(initializer('zeros', config.vocab_size), 'bias')\n",
    "\n",
    "    def construct(self, hidden_states, masked_lm_positions):\n",
    "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
    "        if masked_lm_positions is not None:\n",
    "            flat_offsets = mnp.arange(batch_size) * seq_len\n",
    "            flat_position = (masked_lm_positions + flat_offsets.reshape(-1, 1)).reshape(-1)\n",
    "            flat_sequence_tensor = hidden_states.reshape(-1, hidden_size)\n",
    "            hidden_states = ops.gather(flat_sequence_tensor, flat_position, 0)\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states) + self.bias\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c0e09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Next Sentence Prediction (NSP)\n",
    "\n",
    "BERT通过NSP捕捉**句子级别**的信息，使其可以理解句子与句子之间的联系，从而能够应用于问答或者推理任务。\n",
    "\n",
    "NSP本质上是一个**二分类任务**，通过输入一个句子对，判断两句话是否为连续句子。输入的两个句子A和B中，B有50%的概率是A的下一句。\n",
    "\n",
    "<div align=center><img src=\"./assets/nsp.png\" alt=\"nsp\" width=\"500\"></div>\n",
    "\n",
    "> <font size=2>图片来源: [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert)</font>\n",
    "\n",
    "另外，输入的内容最好是document-level的语料，而非sentence-level的语料，这样训练出的模型可以具备抓取长序列特征的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209b829",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "在这里，我们使用一个单隐藏层的多层感知机`BERTPooler`进行二分类预测。因为特殊占位符\\<cls\\>在预训练中对应了句子级别的特征信息，所以多层感知机分类器只需要输出\\<cls\\>对应的隐藏层输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1784651",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class BertPooler(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(BertPooler, self).__init__()\n",
    "        self.dense = nn.Dense(config.hidden_size, config.hidden_size, activation='tanh', weight_init=TruncatedNormal(config.initializer_range))\n",
    "    \n",
    "    def construct(self, hidden_states):\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b75af11",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "最后，多层感知机分类器的输出通过一个线性层`self.seq_relationship`，输出对nsp的预测。\n",
    "\n",
    "在`BERTPreTrainingHeads`中，我们对以上提到的两种方式进行整合。最终输出Maked LM（`prediction scores`）和NSP（`seq_realtionship_score`）的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc1f2a8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class BertPreTrainingHeads(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(BertPreTrainingHeads, self).__init__()\n",
    "        self.predictions = BertLMPredictionHead(config)\n",
    "        self.seq_relationship = nn.Dense(config.hidden_size, 2, weight_init=TruncatedNormal(config.initializer_range))\n",
    "    \n",
    "    def construct(self, sequence_output, pooled_output, masked_lm_positions):\n",
    "        prediction_scores = self.predictions(sequence_output, masked_lm_positions)\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return prediction_scores, seq_relationship_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bd5d8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### BERT预训练代码整合\n",
    "\n",
    "我们将上述的类进行实例化，并借此回顾一下BERT预训练的整体流程。\n",
    "\n",
    "1. `BertModel`构建BERT模型；\n",
    "2. `BertPretrainingHeads`整合了Masked LM与NSP两个训练任务， 输出预测结果；\n",
    "    - `BertLMPredictionHead`：输入BERT编码与\\<mask\\>的位置，输出对应位置词元的预测；\n",
    "    - `BERTPooler`：输入BERT编码，输出对\\<cls\\>的隐藏状态，并在`BertPretrainingHeads`中通过线性层输出预测结果；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc5a310",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class BertForPretraining(nn.Cell):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertPreTrainingHeads(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.cls.predictions.decoder.weight = self.bert.embeddings.word_embeddings.embedding_table\n",
    "\n",
    "    def construct(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, masked_lm_positions=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output, masked_lm_positions)\n",
    "\n",
    "        outputs = (prediction_scores, seq_relationship_score,) + outputs[2:]\n",
    "\n",
    "        return outputs"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
