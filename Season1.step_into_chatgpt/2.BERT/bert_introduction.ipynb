{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35bd8ac0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div align=center><img src=\"./assets/cover.png\" alt=\"bert-cover\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576ed71b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div align=center><img src=\"./assets/contents_1_2.png\" alt=\"table-of-contents\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f34239",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=center><img src=\"./assets/qr_code.jpeg\" alt=\"table-of-contents\" width=\"1300\"></div>\n",
    "\n",
    "https://github.com/mindspore-courses/step_into_chatgpt\n",
    "\n",
    "https://github.com/mindspore-lab/mindnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fabf36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3202年了，为什么还要学BERT\n",
    "\n",
    "- ### Decoder only模型当道： GPT3、Bloom、LLAMA、GLM\n",
    "\n",
    "    - #### Transformer Encoder结构在生成式任务上的缺陷\n",
    "\n",
    "    - #### BERT模型规模小\n",
    "\n",
    "    - #### Pretrain-Fintune范式的落寞\n",
    "\n",
    "- ### 2022年以前，学术界还是在倒腾BERT\n",
    "\n",
    "    - #### Finetune更容易针对单领域任务训练\n",
    "    \n",
    "    - #### BERT是首个大规模并行预训练的模型，也是当前的performance baseline\n",
    "    \n",
    "    - #### 由BERT入手学大模型训练、微调、Prompt最简单"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a7118",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# NLP中的预训练模型\n",
    "\n",
    "语言模型的演变经历了以下几个阶段：\n",
    "\n",
    "<div align=center><img src=\"./assets/language_models.svg\" alt=\"language-models\" width=\"1000\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558644f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. `word2vec`/`Glove`将离散的文本数据转换为固定长度的**静态词向量**，后根据下游任务训练不同的**语言模型**；\n",
    "\n",
    "2. `ELMo`预训练模型将文本数据*结合上下文信息*，转换为**动态词向量**，后根据下游任务训练不同的**语言模型**；\n",
    "\n",
    "3. `BERT`同样将文本数据转换为**动态词向量**，能够更好地捕捉*句子级别的信息与语境信息*，后续只需finetune最后的**输出层**即可适配下游任务；\n",
    "\n",
    "4. `GPT`等预训练语言模型主要用于*文本生成类任务*，需要通过**prompt方法**来应用于下游任务，指导模型生成特定的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242c5a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# BERT\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-transfer-learning.png\"></div>\n",
    "\n",
    "BERT模型本质上是结合了`ELMo`模型与`GPT`模型的优势。\n",
    "\n",
    "- 相比于ELMo，BERT仅需改动最后的输出层，而非模型架构，便可以在下游任务中达到很好的效果；\n",
    "- 相比于GPT，BERT在处理词元表示时考虑到了双向上下文的信息；\n",
    "\n",
    "BERT通过两种无监督任务（Masked Language Modelling 和 Next Sentence Prediction）进行预训练，其次，在下游任务中对预训练Transformer编码器的所有参数进行微调，额外的输出层将从头开始训练。\n",
    "\n",
    "> Reference: [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a5669",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# BERT 结构\n",
    "\n",
    "BERT（Bidirectional Encoder Representation from Transformers）是由Transformer的Encoder层堆叠而成，BERT的模型大小有如下两种：\n",
    "\n",
    "- BERT BASE：与Transformer参数量齐平，用于比较模型效果（110M parameters）\n",
    "- BERT LARGE：在BERT BASE基础上扩大参数量，达到了当时各任务最好的结果（340M parameters）\n",
    "\n",
    "| model | blocks | hidden size | attention heads |\n",
    "| :-----:| :----: | :----: | :----: |\n",
    "| Transformer | 6 | 512 | 8 |\n",
    "| BERT BASE | 12 | 768 | 12 |\n",
    "| BERT LARGE | 24 | 1024 | 16 |\n",
    "\n",
    "> Reference: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7e625e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"./assets/bert-base-bert-large-encoders.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d1ce8",
   "metadata": {},
   "source": [
    "接受输入序列后，BERT会输出每个位置对应的向量（长度等于hidden size），在后续下游任务中，我们会选取与任务相关的位置的向量，输入到最终输出层中得到结果。\n",
    "\n",
    "如在诈骗邮件分类任务中，我们会将表示句子级别信息的`[CLS]` token所对应的vector，放入classfier中，得到对spam/not spam分类的预测。\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-classifier.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27de31d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BERT 输入\n",
    "\n",
    "- 针对句子对相关任务，将两个句子合并为一个句子对输入到Encoder中，`[CLS]` + 第一个句子 + `[SEP]` + 第二个句子 + `[SEP]`;\n",
    "- 针对单个文本相关任务，`[CLS]` + 句子 + \n",
    "`[SEP]`。\n",
    "\n",
    "在诈骗邮件分类中，输入为单个句子，在拆分为tokens后，在序列首尾分别添加`[CLS]`与`[SEP]`即可。\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-input.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c39086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Collecting mindnlp\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/72/37/ef313c23fd587c3d1f46b0741c98235aecdfd93b4d6d446376f3db6a552c/mindnlp-0.3.1-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m127.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mindspore in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindnlp) (2.2.14)\n",
      "Collecting tqdm (from mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/48/5d/acf5905c36149bbaec41ccf7f2b68814647347b72075ac0b1fe3022fdc73/tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Collecting requests (from mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting datasets (from mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/a5/52/45dab187f03d48c765b94db0464f5c10431756e47ae4cc6a8029a7d57a36/datasets-3.0.0-py3-none-any.whl (474 kB)\n",
      "Collecting evaluate (from mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/a2/e7/cbca9e2d2590eb9b5aa8f7ebabe1beb1498f9462d2ecede5c9fd9735faaf/evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Collecting tokenizers (from mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/0c/52/24180c93a1a3d0f386d66a7f90e0e597c238667fd6d1689b3688409b2dd6/tokenizers-0.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors (from mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/5d/80/81ba44fc82afbf5ca553913ac49460e325dc5cf00c317b34c14d43ebd76b/safetensors-0.4.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n",
      "Collecting sentencepiece (from mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/5f/01/c95e42eb86282b2c79305d3e0b0ca5a743f85a61262bb7130999c70b9374/sentencepiece-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex (from mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/01/e6/a7256c99c312b68f01cfd4f8eae6e770906fffb3832ecb66f35ca5b86b96/regex-2024.9.11-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.0/782.0 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting addict (from mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/6a/00/b08f23b7d7e1e14ce01419a467b583edbb93c6cdb8654e54a9cc579cd61f/addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
      "Collecting ml-dtypes (from mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/a8/6f/49effaafbc24c7665bcea42cacb22e7198bbab5b473d908c5900c6bb6a59/ml_dtypes-0.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyctcdecode (from mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/a5/8a/93e2118411ae5e861d4f4ce65578c62e85d0f1d9cb389bd63bd57130604e/pyctcdecode-0.5.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting jieba (from mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytest==7.2.0 (from mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/67/68/a5eb36c3a8540594b6035e6cdae40c1ef1b6a2bfacbecc3d1a544583c078/pytest-7.2.0-py3-none-any.whl (316 kB)\n",
      "Collecting attrs>=19.2.0 (from pytest==7.2.0->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/6a/21/5b6702a7f963e95456c0de2d495f67bf5fd62840ac655dc451586d23d39a/attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Collecting iniconfig (from pytest==7.2.0->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/ef/a6/62565a6e1cf69e10f5727360368e451d4b7f58beeac6173dc9db836a5b46/iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Requirement already satisfied: packaging in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from pytest==7.2.0->mindnlp) (24.1)\n",
      "Collecting pluggy<2.0,>=0.12 (from pytest==7.2.0->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/88/5f/e351af9a41f866ac3f1fac4ca0613908d9a41741cfcf2228f4ad853b697d/pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from pytest==7.2.0->mindnlp) (1.2.2)\n",
      "Collecting tomli>=1.0.0 (from pytest==7.2.0->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/97/75/10a9ebee3fd790d20926a90a2547f0bf78f371b2f13aa822c759680ca7b9/tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting filelock (from datasets->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/b9/f8/feced7779d755758a52d1f6635d990b8d98dc0a29fa568bbe0625f18fdf3/filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from datasets->mindnlp) (2.0.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/af/61/bcd9b58e38ead6ad42b9ed00da33a3f862bc1d445e3d3164799c25550ac2/pyarrow-17.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m175.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.9,>=0.3.0 (from datasets->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Collecting pandas (from datasets->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/3d/dd/bed19c2974296661493d7acc4407b1d2db4e2a482197df100f8f965b6225/pandas-2.2.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/b4/b4/332647451ed7d2c021294b7c1e9c144dbb5586b1fb214ad4f5a404642835/xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "Collecting multiprocess (from datasets->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/da/d9/f7f9379981e39b8c2511c9e0326d212accacb82f12fbfdc1aa2ce2a7b2b6/multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/5e/44/73bea497ac69bafde2ee4269292fa3b41f1198f4bb7bbaaabde30ad29d4a/fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Collecting aiohttp (from datasets->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/23/69/200bf165b56c17854d54975f894de10dababc4d0226c07600c9abc679e7e/aiohttp-3.10.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.22.0 (from datasets->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/5f/f1/15dc793cb109a801346f910a6b350530f2a763a6e83b221725a0bcc1e297/huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "Collecting pyyaml>=5.1 (from datasets->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/3d/32/e7bd8535d22ea2874cef6a81021ba019474ace0d13a4819c2a4bce79bd6a/PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.4/737.4 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2 (from requests->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/98/69/5d8751b4b670d623aa7a47bef061d69c279e9f922f6705147983aa76c3ce/charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl (70 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/ce/d9/5f4c13cecde62396b0d3fe530a50ccea91e7dfc1ccf0e09c228841bb5ba8/urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/12/90/3c9ff0512038035f59d279fddeb79f5f1eccd8859f06d6163c58798b9487/certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Requirement already satisfied: protobuf>=3.13.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindspore->mindnlp) (5.28.2)\n",
      "Requirement already satisfied: asttokens>=2.0.4 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindspore->mindnlp) (2.4.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindspore->mindnlp) (10.4.0)\n",
      "Requirement already satisfied: scipy>=1.5.4 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindspore->mindnlp) (1.13.1)\n",
      "Requirement already satisfied: psutil>=5.6.1 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindspore->mindnlp) (6.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.3 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindspore->mindnlp) (1.6.3)\n",
      "Collecting numpy>=1.17 (from datasets->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/54/30/c2a907b9443cf42b90c17ad10c1e8fa801975f01cb9764f3f8eb8aea638b/numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m156.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pygtrie<3.0,>=2.1 (from pyctcdecode->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/ec/cd/bd196b2cf014afb1009de8b0f05ecd54011d881944e62763f3c1b1e8ef37/pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
      "Collecting hypothesis<7,>=6.14 (from pyctcdecode->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/5f/25/f1fb5b3ec58ed3c6014385672d4298e2f0c7291bfcd9ffd06627a641470d/hypothesis-6.112.1-py3-none-any.whl (467 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from asttokens>=2.0.4->mindspore->mindnlp) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from astunparse>=1.6.3->mindspore->mindnlp) (0.44.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/18/b6/58ea188899950d759a837f9a58b2aee1d1a380ea4d6211ce9b1823748851/aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/70/b0/6f1ebdabfb604e39a0f84428986b89ab55f246b64cddaa495f2c953e1f6b/frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/5e/41/0d0fb18c1ad574f807196f5f3d99164edf9de3e169a58c6dc2d6ed5742b9/multidict-6.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/2a/fc/6abf468b15db181f7b8b573eae9ebe99e7e492ed20473973fd37308c835d/yarl-1.12.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from huggingface-hub>=0.22.0->datasets->mindnlp) (4.12.2)\n",
      "Collecting sortedcontainers<3.0.0,>=2.1.0 (from hypothesis<7,>=6.14->pyctcdecode->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/32/46/9cb0e58b2deb7f82b84065f37f3bffeb12413f947f9388e4cac22c4621ce/sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from pandas->datasets->mindnlp) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/11/c3/005fcca25ce078d2cc29fd559379817424e94885510568bc1bc53d7d5846/pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets->mindnlp)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/a6/ab/7e5f53c3b9d14972843a647d8d7a853969a58aecc7559cb3267302c94774/tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Building wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=91bebdf4787e7a9adc4a6f7e97538f5438885563cc33dbace8a3245f90ca2071\n",
      "  Stored in directory: /home/ma-user/.cache/pip/wheels/2d/22/9e/9af7e8c2773513ac75905acfb75073922bcc1aa176f730a0c9\n",
      "Successfully built jieba\n",
      "Installing collected packages: sortedcontainers, sentencepiece, pytz, pygtrie, jieba, addict, xxhash, urllib3, tzdata, tqdm, tomli, safetensors, regex, pyyaml, pluggy, numpy, multidict, iniconfig, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, async-timeout, aiohappyeyeballs, yarl, requests, pytest, pyarrow, pandas, multiprocess, ml-dtypes, hypothesis, aiosignal, pyctcdecode, huggingface-hub, aiohttp, tokenizers, datasets, evaluate, mindnlp\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "Successfully installed addict-2.4.0 aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 async-timeout-4.0.3 attrs-24.2.0 certifi-2024.8.30 charset-normalizer-3.3.2 datasets-3.0.0 dill-0.3.8 evaluate-0.4.3 filelock-3.16.1 frozenlist-1.4.1 fsspec-2024.6.1 huggingface-hub-0.25.1 hypothesis-6.112.1 idna-3.10 iniconfig-2.0.0 jieba-0.42.1 mindnlp-0.3.1 ml-dtypes-0.5.0 multidict-6.1.0 multiprocess-0.70.16 numpy-1.26.4 pandas-2.2.3 pluggy-1.5.0 pyarrow-17.0.0 pyctcdecode-0.5.0 pygtrie-2.5.0 pytest-7.2.0 pytz-2024.2 pyyaml-6.0.2 regex-2024.9.11 requests-2.32.3 safetensors-0.4.5 sentencepiece-0.2.0 sortedcontainers-2.4.0 tokenizers-0.20.0 tomli-2.0.1 tqdm-4.66.5 tzdata-2024.2 urllib3-2.2.3 xxhash-3.5.0 yarl-1.12.1\n",
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Collecting pytesseract\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/7a/33/8312d7ce74670c9d39a532b2c246a853861120486be9443eebf048043637/pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from pytesseract) (24.1)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from pytesseract) (10.4.0)\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.13\n"
     ]
    }
   ],
   "source": [
    "# install mindnlp\n",
    "!pip install mindnlp\n",
    "!pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad0c6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.781 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2393, 3159, 2089, 6968, 2080, 4651, 4121, 12839, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'help', 'prince', 'may', '##uk', '##o', 'transfer', 'huge', 'inheritance', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sequence = 'help prince mayuko transfer huge inheritance'\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs)\n",
    "tokens = []\n",
    "for index in model_inputs['input_ids']:\n",
    "    tokens.append(tokenizer.convert_ids_to_tokens(index))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf4efc6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BERT Embedding\n",
    "\n",
    "输入到BERT模型的信息由三部分内容组成：\n",
    "\n",
    "- 表示内容的token ids\n",
    "- 表示位置的position ids\n",
    "- 用于区分不同句子的token type ids\n",
    "\n",
    "三种信息分别进入Embedding层，得到token embeddings、position embeddings与segment embeddings；与Transformer不同，以上三种均为**可学习**的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1d92c5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=center><img src=\"./assets/bert-embedding-modified.png\" alt=\"bert-embedding\" width=\"1000\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d04340e2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mindspore\n",
    "from mindspore import nn\n",
    "import mindspore.common.dtype as mstype\n",
    "from mindspore.common.initializer import initializer, TruncatedNormal\n",
    "\n",
    "class BertEmbeddings(nn.Cell):\n",
    "    \"\"\"\n",
    "    Embeddings for BERT, include word, position and token_type\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, embedding_table=TruncatedNormal(config.initializer_range))\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size, embedding_table=TruncatedNormal(config.initializer_range))\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size, embedding_table=TruncatedNormal(config.initializer_range))\n",
    "        self.layer_norm = nn.LayerNorm((config.hidden_size,), epsilon=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(1 - config.hidden_dropout_prob)\n",
    "\n",
    "    def construct(self, input_ids, token_type_ids=None, position_ids=None):\n",
    "        seq_len = input_ids.shape[1]\n",
    "        #初始化position_ids，使其为值(0, seq_len-1)的张量，与input_ids形状相同\n",
    "        if position_ids is None:\n",
    "            position_ids = mnp.arange(seq_len)\n",
    "            position_ids = position_ids.expand_dims(0).expand_as(input_ids)\n",
    "        #初始化token_type_ids，使其为元素值全为0的张量，与input_ids形状相同\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = ops.zeros_like(input_ids)\n",
    "        \n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        #BERT最终的embeddings为token_ids, position_ids, token_type_ids 三者相加\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24afcb4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BERT 模型构建\n",
    "\n",
    "BERT模型的构建与上一节课程的Transformer Encoder构建类似。\n",
    "\n",
    "分别构建multi-head attention层，feed-forward network，并在中间用add&norm连接，最后通过线性层与softmax层进行输出。\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-model-architecture.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931ad4de",
   "metadata": {},
   "source": [
    "### BERT self-attention 层\n",
    "\n",
    "<div align=center><img src=\"./assets/transformer_multi-headed_self-attention-recap.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5068b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Cell):\n",
    "    \"\"\"\n",
    "    Self attention layer for BERT.\n",
    "    \"\"\"\n",
    "    def __init__(self,  config):\n",
    "        super().__init__()\n",
    "        #检查隐藏层大小是否能被注意力头数量整除，每个注意力头需要相同大小的输入\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"The hidden size {config.hidden_size} is not a multiple of the number of attention \"\n",
    "                f\"heads {config.num_attention_heads}\"\n",
    "            )\n",
    "        self.output_attentions = config.output_attentions\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        \n",
    "        #定义Q,K,V\n",
    "        self.query = nn.Dense(config.hidden_size, self.all_head_size, \\\n",
    "            weight_init=TruncatedNormal(config.initializer_range))\n",
    "        self.key = nn.Dense(config.hidden_size, self.all_head_size, \\\n",
    "            weight_init=TruncatedNormal(config.initializer_range))\n",
    "        self.value = nn.Dense(config.hidden_size, self.all_head_size, \\\n",
    "            weight_init=TruncatedNormal(config.initializer_range))\n",
    "\n",
    "        self.dropout = Dropout(config.attention_probs_dropout_prob)\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        self.matmul = Matmul()\n",
    "    \n",
    "    #转换注意力计算的张量形状，便于并行化计算\n",
    "    def transpose_for_scores(self, input_x):\n",
    "        \"\"\"\n",
    "        transpose for scores\n",
    "        [batch_size, seq_len, num_heads, head_size] to [batch_size, num_heads, seq_len, head_size]\n",
    "        \"\"\"\n",
    "        new_x_shape = input_x.shape[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        input_x = input_x.view(*new_x_shape)\n",
    "        return input_x.transpose(0, 2, 1, 3)\n",
    "\n",
    "    def construct(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        #获取Q,K,V并转换张量形状\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        #计算注意力分数\n",
    "        attention_scores = self.matmul(query_layer, key_layer.swapaxes(-1, -2))\n",
    "        attention_scores = attention_scores / ops.sqrt(Tensor(self.attention_head_size, mstype.float32))\n",
    "        #如果提供了掩码，需要将对应位置的信息屏蔽\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "        \n",
    "        #计算加权后的上下文权重并转换为原来的形状\n",
    "        context_layer = self.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.transpose(0, 2, 1, 3)\n",
    "        new_context_layer_shape = context_layer.shape[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983cb64",
   "metadata": {},
   "source": [
    "### BERT self-attention 输出层 \n",
    "\n",
    "- BERTSelfOutput：residual connection + layer normalization\n",
    "- BERTAttention: self-attention + add&norm\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-attention-code.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7ec92f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfOutput(nn.Cell):\n",
    "    r\"\"\"\n",
    "    Bert Self Output\n",
    "    self-attention output + residual connection + layer norm\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Dense(config.hidden_size, config.hidden_size, \\\n",
    "            weight_init=TruncatedNormal(config.initializer_range))\n",
    "        self.layer_norm = nn.LayerNorm((config.hidden_size,), epsilon=1e-12)\n",
    "        self.dropout = Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def construct(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        #残差连接与层归一化\n",
    "        hidden_states = self.layer_norm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ef35f5",
   "metadata": {},
   "source": [
    "### BERT feed-forward 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "073f2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertIntermediate(nn.Cell):\n",
    "    r\"\"\"\n",
    "    Bert Intermediate\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Dense(config.hidden_size, config.intermediate_size, \\\n",
    "            weight_init=TruncatedNormal(config.initializer_range))\n",
    "        self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "    def construct(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        #激活函数，增加非线性部分\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd863caa",
   "metadata": {},
   "source": [
    "### BERT 最后的Add&Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0732f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Cell):\n",
    "    r\"\"\"\n",
    "    Bert Output\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Dense(config.intermediate_size, config.hidden_size, \\\n",
    "            weight_init=TruncatedNormal(config.initializer_range))\n",
    "        self.layer_norm = nn.LayerNorm((config.hidden_size,), epsilon=1e-12)\n",
    "        self.dropout = Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def construct(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        #残差连接与层归一化\n",
    "        hidden_states = self. layer_norm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66995cbe",
   "metadata": {},
   "source": [
    "### BERT Encoder\n",
    "\n",
    "- BertLayer：Encoder Layer，集合了self-attention, feed-forward并通过add&norm连接\n",
    "- BertEnocoder：通过Encoder Layer堆叠起来的Encoder结构\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-model-code.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7752a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Cell):\n",
    "    r\"\"\"\n",
    "    Bert Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        #BertAttention由BertSelfAttention, BertSelfOutput构成\n",
    "        self.attention = BertAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "    \n",
    "    #将attention层，feedforword层，和最终的Add&Norm顺序进行\n",
    "    def construct(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        attention_outputs = self.attention(hidden_states, attention_mask, head_mask)\n",
    "        attention_output = attention_outputs[0]\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        outputs = (layer_output,) + attention_outputs[1:]\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertEncoder(nn.Cell):\n",
    "    r\"\"\"\n",
    "    Bert Encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.layer = nn.CellList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def construct(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        all_hidden_states = ()\n",
    "        all_attentions = ()\n",
    "        #hidden_state为前一层的输出，累加多层权重结果\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if self.output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if self.output_attentions:\n",
    "                all_attentions += (layer_outputs[1],)\n",
    "\n",
    "        if self.output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "        if self.output_hidden_states:\n",
    "            outputs += (all_hidden_states,)\n",
    "        if self.output_attentions:\n",
    "            outputs += (all_attentions,)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a1b39",
   "metadata": {},
   "source": [
    "## BERT 输出\n",
    "\n",
    "BERT会针对每一个位置输出大小为hidden size的向量，在下游任务中，会根据任务内容的不同，选取不同的向量放入输出层。\n",
    "- 我们一般称`[CLS]`经过线性层+激活函数tanh的输出为**pooler output**，用于句子级别的分类/回归任务;\n",
    "- 我们一般称BERT输出的每个位置对应的vector为**sequence output**,用于词语级别的分类任务；\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-output.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3466d706",
   "metadata": {},
   "source": [
    "### BERT Pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b79508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Cell):\n",
    "    r\"\"\"\n",
    "    Bert Pooler\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Dense(config.hidden_size, config.hidden_size, \\\n",
    "            activation='tanh', weight_init=TruncatedNormal(config.initializer_range))\n",
    "\n",
    "    def construct(self, hidden_states):\n",
    "        #BERT pooler只关注句首的[CLS]\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0df426",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BERT 预训练\n",
    "\n",
    "BERT通过Masked LM（masked language model）与NSP（next sentence prediction）获取词语和句子级别的特征。\n",
    "\n",
    "<div align=center><img src=\"./assets/bert_pretrain_finetune.jpg\" alt=\"bert-pretrain\" width=\"1000\"></div>\n",
    "\n",
    "> <font size=2>图片来源：Devlin, J.; Chang, M. W.; Lee, K.; Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef77d903",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Masked Language Model (Masked LM)\n",
    "\n",
    "BERT模型通过Masked LM捕捉**词语层面**的信息。\n",
    "\n",
    "我们随机将每个句子中15%的词语进行遮盖，替换成掩码\\<mask\\>。在训练过程中，模型会对句子进行“完形填空”，预测这些被遮盖的词语是什么，通过减小被mask词语的损失值来对模型进行优化。\n",
    "\n",
    "<div align=center><img src=\"./assets/masked_lm.png\" alt=\"masked-lm\" width=\"1000\"></div>\n",
    "\n",
    "> <font size=2>图片来源: [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa96afc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "由于\\<mask\\>仅在预训练中出现，为了让预训练和微调中的数据处理尽可能接近，我们在随机mask的时候进行如下操作：\n",
    "- 80%的概率替换为\\<mask\\>\n",
    "- 10%的概率替换为文本中的随机词\n",
    "- 10%的概率不进行替换，保持原有的词元\n",
    "\n",
    "<div align=center><img src=\"./assets/masked_lm_2.png\" alt=\"masked-lm-2\" width=\"1000\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4469a1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "我们通过`BERTPredictionHeadTranform`实现单层感知机，对被遮盖的词元进行预测。在前向网络中，我们需要输入BERT模型的编码结果`hidden_states`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53d4e932",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "activation_map = {\n",
    "    'relu': nn.ReLU(),\n",
    "    'gelu': nn.GELU(False),\n",
    "    'gelu_approximate': nn.GELU(),\n",
    "    'swish':nn.SiLU()\n",
    "}\n",
    "\n",
    "class BertPredictionHeadTransform(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Dense(config.hidden_size, config.hidden_size, weight_init=TruncatedNormal(config.initializer_range))\n",
    "        self.transform_act_fn = activation_map.get(config.hidden_act, nn.GELU(False))\n",
    "        self.layer_norm = nn.LayerNorm((config.hidden_size,), epsilon=config.layer_norm_eps)\n",
    "    \n",
    "    def construct(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c825d61f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "根据被遮盖的词元位置`masked_lm_positions`，获得这些词元的预测输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de0cc3f8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import mindspore.ops as ops\n",
    "import mindspore.numpy as mnp\n",
    "from mindspore import Parameter, Tensor\n",
    "\n",
    "class BertLMPredictionHead(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(BertLMPredictionHead, self).__init__()\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "        self.decoder = nn.Dense(config.hidden_size, config.vocab_size, has_bias=False, weight_init=TruncatedNormal(config.initializer_range))\n",
    "\n",
    "        self.bias = Parameter(initializer('zeros', config.vocab_size), 'bias')\n",
    "\n",
    "    def construct(self, hidden_states, masked_lm_positions):\n",
    "\n",
    "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
    "        #判断是否提供了需要预测的token位置，如果提供了可以仅预测masked的部分，提高预测效率\n",
    "        if masked_lm_positions is not None:\n",
    "            flat_offsets = mnp.arange(batch_size) * seq_len\n",
    "            flat_position = (masked_lm_positions + flat_offsets.reshape(-1, 1)).reshape(-1)\n",
    "            flat_sequence_tensor = hidden_states.reshape(-1, hidden_size)\n",
    "            hidden_states = ops.gather(flat_sequence_tensor, flat_position, 0)\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states) + self.bias\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c0e09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Next Sentence Prediction (NSP)\n",
    "\n",
    "BERT通过NSP捕捉**句子级别**的信息，使其可以理解句子与句子之间的联系，从而能够应用于问答或者推理任务。\n",
    "\n",
    "NSP本质上是一个**二分类任务**，通过输入一个句子对，判断两句话是否为连续句子。输入的两个句子A和B中，B有50%的概率是A的下一句。\n",
    "\n",
    "<div align=center><img src=\"./assets/nsp.png\" alt=\"nsp\" width=\"500\"></div>\n",
    "\n",
    "> <font size=2>图片来源: [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert)</font>\n",
    "\n",
    "另外，输入的内容最好是document-level的语料，而非sentence-level的语料，这样训练出的模型可以具备抓取长序列特征的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209b829",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "在这里，我们使用一个单隐藏层的多层感知机`BERTPooler`进行二分类预测。因为特殊占位符\\<cls\\>在预训练中对应了句子级别的特征信息，所以多层感知机分类器只需要输出\\<cls\\>对应的隐藏层输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1784651",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class BertPooler(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(BertPooler, self).__init__()\n",
    "        self.dense = nn.Dense(config.hidden_size, config.hidden_size, activation='tanh', weight_init=TruncatedNormal(config.initializer_range))\n",
    "    \n",
    "    def construct(self, hidden_states):\n",
    "        #BERT pooler只关注句首的[CLS]\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b75af11",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "最后，多层感知机分类器的输出通过一个线性层`self.seq_relationship`，输出对nsp的预测。\n",
    "\n",
    "在`BERTPreTrainingHeads`中，我们对以上提到的两种方式进行整合。最终输出Maked LM（`prediction scores`）和NSP（`seq_realtionship_score`）的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edc1f2a8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class BertPreTrainingHeads(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(BertPreTrainingHeads, self).__init__()\n",
    "        self.predictions = BertLMPredictionHead(config)\n",
    "        self.seq_relationship = nn.Dense(config.hidden_size, 2, weight_init=TruncatedNormal(config.initializer_range))\n",
    "    \n",
    "    def construct(self, sequence_output, pooled_output, masked_lm_positions):\n",
    "        prediction_scores = self.predictions(sequence_output, masked_lm_positions)\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return prediction_scores, seq_relationship_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bd5d8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### BERT预训练代码整合\n",
    "\n",
    "我们将上述的类进行实例化，并借此回顾一下BERT预训练的整体流程。\n",
    "\n",
    "1. `BertModel`构建BERT模型；\n",
    "2. `BertPretrainingHeads`整合了Masked LM与NSP两个训练任务， 输出预测结果；\n",
    "    - `BertLMPredictionHead`：输入BERT编码与\\<mask\\>的位置，输出对应位置词元的预测；\n",
    "    - `BERTPooler`：输入BERT编码，输出对\\<cls\\>的隐藏状态，并在`BertPretrainingHeads`中通过线性层输出预测结果；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dc5a310",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class BertForPretraining(nn.Cell):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertPreTrainingHeads(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.cls.predictions.decoder.weight = self.bert.embeddings.word_embeddings.embedding_table\n",
    "\n",
    "    def construct(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, masked_lm_positions=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask\n",
    "        )\n",
    "        #得到序列输出和池化输出，序列输出对应Masked LM任务，池化输出对应NSP任务\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "        #序列输出和池化输出分别经过对应的全连接层\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output, masked_lm_positions)\n",
    "\n",
    "        outputs = (prediction_scores, seq_relationship_score,) + outputs[2:]\n",
    "\n",
    "        return outputs"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
