{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境配置"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 此为在线运行平台配置python3.9的指南，如在其他环境平台运行案例，请根据实际情况修改如下代码"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步：设置python版本为3.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "!/home/ma-user/anaconda3/bin/conda create -n python-3.9.0 python=3.9.0 -y --override-channels --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n",
    "!/home/ma-user/anaconda3/envs/python-3.9.0/bin/pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "data = {\n",
    "   \"display_name\": \"python-3.9.0\",\n",
    "   \"env\": {\n",
    "      \"PATH\": \"/home/ma-user/anaconda3/envs/python-3.9.0/bin:/home/ma-user/anaconda3/envs/python-3.7.10/bin:/modelarts/authoring/notebook-conda/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/ma-user/modelarts/ma-cli/bin:/home/ma-user/modelarts/ma-cli/bin\"\n",
    "   },\n",
    "   \"language\": \"python\",\n",
    "   \"argv\": [\n",
    "      \"/home/ma-user/anaconda3/envs/python-3.9.0/bin/python\",\n",
    "      \"-m\",\n",
    "      \"ipykernel\",\n",
    "      \"-f\",\n",
    "      \"{connection_file}\"\n",
    "   ]\n",
    "}\n",
    "\n",
    "if not os.path.exists(\"/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/\"):\n",
    "    os.mkdir(\"/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/\")\n",
    "\n",
    "with open('/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/kernel.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注：以上代码运行完成后，需要重新设置kernel为python-3.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://mindspore-demo.obs.cn-north-4.myhuaweicloud.com/imgs/ai-gallery/change-kernel.PNG\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二步：安装MindSpore框架和MindNLP套件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mindspore官网提供了不同的mindspore版本，可以根据自己的操作系统和Python版本，安装不同版本的mindspore\n",
    "\n",
    "\n",
    "https://www.mindspore.cn/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting mindspore==2.5.0\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/23/22/dff0f1bef6c0846a97271ae5d39ca187914f39562f9e3f6787041dea1a97/mindspore-2.5.0-cp39-cp39-manylinux1_x86_64.whl (958.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.4/958.4 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting numpy<2.0.0,>=1.20.0 (from mindspore==2.5.0)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/54/30/c2a907b9443cf42b90c17ad10c1e8fa801975f01cb9764f3f8eb8aea638b/numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf>=3.13.0 (from mindspore==2.5.0)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/28/50/1925de813499546bc8ab3ae857e3ec84efe7d2f19b34529d0c7c3d02d11d/protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Requirement already satisfied: asttokens>=2.0.4 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindspore==2.5.0) (3.0.0)\n",
      "Collecting pillow>=6.2.0 (from mindspore==2.5.0)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/f6/46/0bd0ca03d9d1164a7fa33d285ef6d1c438e963d0c8770e4c5b3737ef5abe/pillow-11.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.5.4 (from mindspore==2.5.0)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/35/f5/d0ad1a96f80962ba65e2ce1de6a1e59edecd1f0a7b55990ed208848012e0/scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindspore==2.5.0) (24.2)\n",
      "Requirement already satisfied: psutil>=5.6.1 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindspore==2.5.0) (5.9.1)\n",
      "Collecting astunparse>=1.6.3 (from mindspore==2.5.0)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting safetensors>=0.4.0 (from mindspore==2.5.0)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/a6/f8/dae3421624fcc87a89d42e1898a798bc7ff72c61f38973a65d60df8f124c/safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Collecting dill>=0.3.7 (from mindspore==2.5.0)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/46/d1/e73b6ad76f0b1fb7f23c35c6d95dbc506a9c8804f43dda8cb5b0fa6331fd/dill-0.3.9-py3-none-any.whl (119 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from astunparse>=1.6.3->mindspore==2.5.0) (0.45.1)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from astunparse>=1.6.3->mindspore==2.5.0) (1.17.0)\n",
      "Installing collected packages: safetensors, protobuf, pillow, numpy, dill, astunparse, scipy, mindspore\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "auto-tune 0.1.0 requires te, which is not installed.\n",
      "schedule-search 0.0.1 requires absl-py, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed astunparse-1.6.3 dill-0.3.9 mindspore-2.5.0 numpy-1.26.4 pillow-11.1.0 protobuf-6.30.2 safetensors-0.5.3 scipy-1.13.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.5.0/MindSpore/unified/x86_64/mindspore-2.5.0-cp39-cp39-linux_x86_64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，这里如果通过pip安装mindnlp的话，需要参考链接对mindnlp/core/nn/modules/module.py进行Files changed所示的修改，确保loss正确下降，链接教程：https://github.com/mindspore-lab/mindnlp/pull/2007/files\n",
    "可以通过git clone下载最新的mindnlp，然后将下载的最新的mindnlp版本替换到你环境中安装mindnlp的位置，一般是/home/user/miniconda3/envs/yourenv/lib/python3.9/site-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Collecting mindnlp==0.4.0\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/0f/a8/5a072852d28a51417b5e330b32e6ae5f26b491ef01a15ba968e77f785e69/mindnlp-0.4.0-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: mindspore>=2.2.14 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindnlp==0.4.0) (2.5.0)\n",
      "Requirement already satisfied: tqdm in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindnlp==0.4.0) (4.67.1)\n",
      "Requirement already satisfied: requests in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindnlp==0.4.0) (2.32.3)\n",
      "Requirement already satisfied: datasets in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindnlp==0.4.0) (3.5.0)\n",
      "Requirement already satisfied: evaluate in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindnlp==0.4.0) (0.4.3)\n",
      "Requirement already satisfied: tokenizers==0.19.1 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindnlp==0.4.0) (0.19.1)\n",
      "Requirement already satisfied: safetensors in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindnlp==0.4.0) (0.5.3)\n",
      "Requirement already satisfied: sentencepiece in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindnlp==0.4.0) (0.2.0)\n",
      "Requirement already satisfied: regex in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindnlp==0.4.0) (2024.11.6)\n",
      "Requirement already satisfied: addict in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindnlp==0.4.0) (2.4.0)\n",
      "Requirement already satisfied: ml-dtypes in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindnlp==0.4.0) (0.5.1)\n",
      "Requirement already satisfied: pyctcdecode in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindnlp==0.4.0) (0.5.0)\n",
      "Collecting jieba (from mindnlp==0.4.0)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pytest==7.2.0 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindnlp==0.4.0) (7.2.0)\n",
      "Requirement already satisfied: pillow>=10.0.0 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindnlp==0.4.0) (11.1.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from pytest==7.2.0->mindnlp==0.4.0) (24.3.0)\n",
      "Requirement already satisfied: iniconfig in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from pytest==7.2.0->mindnlp==0.4.0) (2.1.0)\n",
      "Requirement already satisfied: packaging in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from pytest==7.2.0->mindnlp==0.4.0) (24.2)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from pytest==7.2.0->mindnlp==0.4.0) (1.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from pytest==7.2.0->mindnlp==0.4.0) (1.2.2)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from pytest==7.2.0->mindnlp==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from tokenizers==0.19.1->mindnlp==0.4.0) (0.30.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.20.0 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindspore>=2.2.14->mindnlp==0.4.0) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.13.0 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindspore>=2.2.14->mindnlp==0.4.0) (6.30.2)\n",
      "Requirement already satisfied: asttokens>=2.0.4 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindspore>=2.2.14->mindnlp==0.4.0) (3.0.0)\n",
      "Requirement already satisfied: scipy>=1.5.4 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindspore>=2.2.14->mindnlp==0.4.0) (1.13.1)\n",
      "Requirement already satisfied: psutil>=5.6.1 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindspore>=2.2.14->mindnlp==0.4.0) (5.9.1)\n",
      "Requirement already satisfied: astunparse>=1.6.3 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindspore>=2.2.14->mindnlp==0.4.0) (1.6.3)\n",
      "Requirement already satisfied: dill>=0.3.7 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from mindspore>=2.2.14->mindnlp==0.4.0) (0.3.8)\n",
      "Requirement already satisfied: filelock in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from datasets->mindnlp==0.4.0) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from datasets->mindnlp==0.4.0) (19.0.1)\n",
      "Requirement already satisfied: pandas in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from datasets->mindnlp==0.4.0) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from datasets->mindnlp==0.4.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from datasets->mindnlp==0.4.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->mindnlp==0.4.0) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from datasets->mindnlp==0.4.0) (3.11.16)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from datasets->mindnlp==0.4.0) (6.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from requests->mindnlp==0.4.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from requests->mindnlp==0.4.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from requests->mindnlp==0.4.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from requests->mindnlp==0.4.0) (2025.1.31)\n",
      "Requirement already satisfied: pygtrie<3.0,>=2.1 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from pyctcdecode->mindnlp==0.4.0) (2.5.0)\n",
      "Requirement already satisfied: hypothesis<7,>=6.14 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from pyctcdecode->mindnlp==0.4.0) (6.130.13)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from astunparse>=1.6.3->mindspore>=2.2.14->mindnlp==0.4.0) (0.45.1)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from astunparse>=1.6.3->mindspore>=2.2.14->mindnlp==0.4.0) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.4.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.4.0) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.4.0) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.4.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.4.0) (6.4.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.4.0) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.4.0) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1->mindnlp==0.4.0) (4.13.1)\n",
      "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from hypothesis<7,>=6.14->pyctcdecode->mindnlp==0.4.0) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from pandas->datasets->mindnlp==0.4.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from pandas->datasets->mindnlp==0.4.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jiangna1/miniconda3/envs/llama39/lib/python3.9/site-packages (from pandas->datasets->mindnlp==0.4.0) (2025.2)\n",
      "Building wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314508 sha256=30064bba508d12a9c2c545bdec7e271f61d5a83e9fdd53298a82e74659e1fd26\n",
      "  Stored in directory: /home/jiangna1/.cache/pip/wheels/95/ef/7c/d8b3108835edfa15487417c5bddff166482b195d8090117ac5\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba, mindnlp\n",
      "Successfully installed jieba-0.42.1 mindnlp-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mindnlp==0.4.0 -i https://mirrors.aliyun.com/pypi/simple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA微调推理全流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLaMA（Large Language Model Meta AI）是由Meta（前Facebook）推出的一系列开源大规模语言模型，意味着大模型应用进入了“免费时代”，初创公司也能够以低廉的价格来创建类似ChatGPT这样的聊天机器人。 LLaMA 专注于自然语言处理任务，其结构基于Transformer架构，但是进行了改进，比如引入更高效的注意力机制和更紧凑的模型设计，例如使用SwiGLU激活函数 使用RoPE位置编码等。\n",
    "\n",
    "LLaMA的模型参数规模设计更为精细，以更少的参数实现了与更大模型相当的性能。例如，LLaMA-7B模型的性能可以与175B参数规模的GPT-3媲美。模型在推理过程中采用了多查询注意力（Multi-Query Attention, MQA）机制，改进了传统多头注意力的查询方式，将多个注意力头的查询统一为单个查询头，从而显著减少了推理时间和显存需求，提升了效率。适用于文本生成、问答、翻译等任务。例如，在文本生成任务中，LLaMA可以生成高质量的文章段落，其轻量化设计降低了硬件需求，使研究者和开发者更容易使用高性能的语言模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入必要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "import mindspore.dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(52617:140460371482432,MainProcess):2025-04-11-09:21:14.572.566 [mindspore/context.py:1335] For 'context.set_context', the parameter 'device_target' will be deprecated and removed in a future version. Please use the api mindspore.set_device() instead.\n"
     ]
    }
   ],
   "source": [
    "#将模式设置为动态图模式（PYNATIVE_MODE），并指定设备目标为Ascend芯片\n",
    "ms.set_context(mode=ms.PYNATIVE_MODE, device_target=\"Ascend\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#指定模型路径\n",
    "base_model_path = \"/home/jiangna1/.mindnlp/model/hfl/chinese-llama-2-1.3b\" #中文模型，这里我提前下载到了本地减少下载时间\n",
    "# base_model_path = \"NousResearch/Hermes-3-Llama-3.2-3B\" #英文模型\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集\n",
    "\n",
    "这里提供两份用于微调的数据集，分别用于中文模型和英文模型，其中中文模型为hfl/chinese-llama-2-1.3b，参数量为1.3B，英文模型为NousResearch/Hermes-3-Llama-3.2-3B，参数量为3.2B，用户可以根据自己的配置或期望训练时长自行选择。\n",
    "\n",
    "数据来源皆为hugging face公开用于微调的数据集，其中中文数据集来源为弱智吧，数据格式为：\n",
    "\n",
    "  {\"instruction\": \"只剩一个心脏了还能活吗？\",\n",
    "\n",
    "    \"output\": \"能，人本来就只有一个心脏。\"},\n",
    "\n",
    "  {\"instruction\": \"爸爸再婚，我是不是就有了个新娘？\",\n",
    "\n",
    "    \"output\": \"不是的，你有了一个继母。\\\"新娘\\\"是指新婚的女方，而你爸爸再婚，他的新婚妻子对你来说是继母。\"}\n",
    "\n",
    "\n",
    "英文数据来源为Alpaca,数据格式为：\n",
    "\n",
    "  {\"instruction\": \"Give three tips for staying healthy.\",\n",
    "\n",
    "    \"input\": \"\",\n",
    "\n",
    "    \"output\": \"1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.\"},\n",
    "\n",
    "  {\"instruction\": \"What are the three primary colors?\",\n",
    "\n",
    "    \"input\": \"\",\n",
    "\n",
    "    \"output\": \"The three primary colors are red, blue, and yellow.\"}\n",
    "\n",
    "\n",
    "以下教程同时包括包括中文和英文模型的微调教程为例，其中英文模型微调效果更好，但因为时间关系，本模型展示主要以小规模的中文为例，可以自行根据自己的需求修改数据来源和模型。\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据加载和数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新建 tokenize_function 函数用于数据预处理，具体内容可见下面代码注释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example, tokenizer):\n",
    "    instruction = example.get(\"instruction\", \"\")\n",
    "    input_text = example.get(\"input\", \"\")\n",
    "    output = example.get(\"output\", \"\")\n",
    "    # prompt\n",
    "    if input_text:\n",
    "        prompt = f\"User: {instruction} {input_text}\\nAssistant: {output}\"\n",
    "    else:\n",
    "        prompt = f\"User: {instruction}\\nAssistant: {output}\"\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    input_ids = np.array(tokenized[\"input_ids\"], dtype=np.int32)\n",
    "\n",
    "    # Handle label\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    labels = np.array(\n",
    "        [-100 if token_id == pad_token_id else token_id for token_id in input_ids], dtype=np.int32\n",
    "    )\n",
    "    return input_ids, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据来源如下，为了避免网络问题，建议先下载到本地\n",
    "\n",
    "https://huggingface.co/datasets/LooksJuicy/ruozhiba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_path = \"/home/jiangna1/mindnlp_llama_all/alpaca_data.json\" #英文数据集\n",
    "data_path = \"/home/jiangna1/mindnlp_llama_all/chinese_data.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看数据具体内容，该数据只包括instruction和output两列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'instruction': '只剩一个心脏了还能活吗？', 'output': '能，人本来就只有一个心脏。'}, {'instruction': '爸爸再婚，我是不是就有了个新娘？', 'output': '不是的，你有了一个继母。\"新娘\"是指新婚的女方，而你爸爸再婚，他的新婚妻子对你来说是继母。'}, {'instruction': '樟脑丸是我吃过最难吃的硬糖有奇怪的味道怎么还有人买', 'output': '樟脑丸并不是硬糖，而是一种常见的驱虫药，不能食用。虽然它的味道可能不太好，但是由于其有效的驱虫效果，所以仍然有很多人会购买。'}]\n"
     ]
    }
   ],
   "source": [
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从指定路径加载预训练的分词器，该分词器能将输入文本分割成模型可处理的词元。接着，将填充标记设置为结束标记，这样在处理不同长度的文本序列时，用结束标记来填充额外位置，避免引入额外特殊标记，减少模型学习负担。最后，设置填充方向为右侧，使文本在右侧添加填充标记达到统一长度，维持文本原始顺序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 29871, 2056]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mindnlp.transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.encode(' ;')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据分为训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_generator(dataset, tokenizer):\n",
    "    for item in dataset:\n",
    "        yield tokenize_function(item, tokenizer)\n",
    "        \n",
    "split_ratio = 0.9\n",
    "split_index = int(len(data) * split_ratio)\n",
    "train_data, val_data = data[:split_index], data[split_index:]\n",
    "\n",
    "train_dataset = ds.GeneratorDataset(\n",
    "    source=lambda: data_generator(train_data, tokenizer), \n",
    "    column_names=[\"input_ids\", \"labels\"]\n",
    ")\n",
    "\n",
    "eval_dataset = ds.GeneratorDataset(\n",
    "    source=lambda: data_generator(val_data, tokenizer), \n",
    "    column_names=[\"input_ids\", \"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看处理后的数据，tokenizer 将输入的文本（prompt）拆分为词片段（tokens），然后将每个词片段映射为对应的 token ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MS_ALLOC_CONF]Runtime config:  enable_vmm:True  vmm_align_size:2MB\n",
      "Sample 0: Input IDs: [    1  4911 29901 29871 47133 32002 37755 30743 33302 31704]\n",
      "Sample 0: Labels: [    1  4911 29901 29871 47133 32002 37755 30743 33302 31704]\n",
      "\n",
      "Sample 1: Input IDs: [    1  4911 29901 29871 33594 31733 33364 30214 30672 32308]\n",
      "Sample 1: Labels: [    1  4911 29901 29871 33594 31733 33364 30214 30672 32308]\n",
      "\n",
      "Sample 2: Input IDs: [    1  4911 29901 29871 47019 33027 31818 34030 39950 44345]\n",
      "Sample 2: Labels: [    1  4911 29901 29871 47019 33027 31818 34030 39950 44345]\n",
      "\n",
      "Sample 3: Input IDs: [    1  4911 29901 29871 34214 30698 30429 36310 32658 30743]\n",
      "Sample 3: Labels: [    1  4911 29901 29871 34214 30698 30429 36310 32658 30743]\n",
      "\n",
      "Sample 4: Input IDs: [    1  4911 29901 32581 34822 31639  2882  6530 30883 30210]\n",
      "Sample 4: Labels: [    1  4911 29901 32581 34822 31639  2882  6530 30883 30210]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(train_dataset.create_dict_iterator()):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"Sample {i}: Input IDs: {sample['input_ids'][:10]}\") \n",
    "    print(f\"Sample {i}: Labels: {sample['labels'][:10]}\\n\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lora指令微调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定微调结果输出路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 指定输出路径\n",
    "peft_output_dir = \"/home/jiangna1/mindnlp_llama_all/pert_model_Chinese\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载基座模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`.`PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiangna1/miniconda3/envs/ms39/lib/python3.9/site-packages/mindnlp/transformers/generation/configuration_utils.py:557: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/jiangna1/miniconda3/envs/ms39/lib/python3.9/site-packages/mindnlp/transformers/generation/configuration_utils.py:562: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/jiangna1/miniconda3/envs/ms39/lib/python3.9/site-packages/mindnlp/transformers/generation/configuration_utils.py:557: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/jiangna1/miniconda3/envs/ms39/lib/python3.9/site-packages/mindnlp/transformers/generation/configuration_utils.py:562: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.transformers import AutoModelForCausalLM,  GenerationConfig\n",
    "\n",
    "ms_base_model = AutoModelForCausalLM.from_pretrained(base_model_path, ms_dtype=ms.float16)\n",
    "ms_base_model.generation_config = GenerationConfig.from_pretrained(base_model_path)\n",
    "ms_base_model.generation_config.pad_token_id = ms_base_model.generation_config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#修改精度，会使训练变慢，但是训练loss下降效果会变好\n",
    "for name, param in ms_base_model.parameters_and_names():\n",
    "    param.set_dtype(ms.float32) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分代码的主要作用是创建一个 LoRA的配置对象 ms_config，大语言模型进行微调时，LoRA 是一种高效的参数微调方法，通过在预训练模型的基础上添加低秩矩阵来减少需要训练的参数数量，从而提高微调效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindnlp.peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
    "\n",
    "ms_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,#微调任务的类型\n",
    "    #指定需要应用 LoRA 调整的目标模块\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    inference_mode=False,  \n",
    "    r=8,                   \n",
    "    lora_alpha=32,         \n",
    "    lora_dropout=0.1       \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于给定的基础模型和 LoRA 配置创建一个可进行参数高效微调的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_model = get_peft_model(ms_base_model, ms_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练参数的设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.engine import TrainingArguments, Trainer\n",
    "\n",
    "num_train_epochs = 70\n",
    "fp16 = True\n",
    "overwrite_output_dir = True\n",
    "per_device_train_batch_size = 16\n",
    "per_device_eval_batch_size = 32\n",
    "gradient_accumulation_steps = 16\n",
    "gradient_checkpointing = True\n",
    "evaluation_strategy = \"steps\"\n",
    "learning_rate = 1e-5\n",
    "lr_scheduler_type = \"cosine\" \n",
    "weight_decay = 0.01\n",
    "warmup_ratio = 0.1\n",
    "max_grad_norm = 0.3\n",
    "group_by_length = False \n",
    "auto_find_batch_size = False\n",
    "save_steps = 50 \n",
    "logging_strategy = \"steps\"\n",
    "logging_steps = 150                \n",
    "load_best_model_at_end = True \n",
    "packing = False\n",
    "save_total_limit = 3\n",
    "neftune_noise_alpha = 5 \n",
    "eval_steps = 10\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=peft_output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    load_best_model_at_end=load_best_model_at_end,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    evaluation_strategy=evaluation_strategy,\n",
    "    eval_steps=eval_steps,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    auto_find_batch_size=auto_find_batch_size,\n",
    "    save_total_limit=save_total_limit,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    save_steps=save_steps,\n",
    "    logging_strategy=logging_strategy,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=ms_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=training_arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/350 [00:19<1:51:45, 19.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 10/350 [02:02<1:04:58, 11.47s/it]We detected that you are passing `past_key_values` as a tuple and this is deprecated. Please use an appropriate `Cache` class\n",
      "                                                  \n",
      "  3%|▎         | 10/350 [02:05<1:04:58, 11.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.853940486907959, 'eval_runtime': 2.8532, 'eval_samples_per_second': 1.752, 'eval_steps_per_second': 0.35, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  6%|▌         | 20/350 [04:00<1:02:29, 11.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.840895891189575, 'eval_runtime': 2.3863, 'eval_samples_per_second': 2.095, 'eval_steps_per_second': 0.419, 'epoch': 3.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  9%|▊         | 30/350 [05:54<1:00:27, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.8152430057525635, 'eval_runtime': 2.3786, 'eval_samples_per_second': 2.102, 'eval_steps_per_second': 0.42, 'epoch': 5.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 11%|█▏        | 40/350 [07:49<58:26, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.772057294845581, 'eval_runtime': 2.3891, 'eval_samples_per_second': 2.093, 'eval_steps_per_second': 0.419, 'epoch': 7.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 14%|█▍        | 50/350 [09:44<56:29, 11.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.7203030586242676, 'eval_runtime': 2.3927, 'eval_samples_per_second': 2.09, 'eval_steps_per_second': 0.418, 'epoch': 9.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 17%|█▋        | 60/350 [11:48<54:53, 11.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.6663565635681152, 'eval_runtime': 2.3972, 'eval_samples_per_second': 2.086, 'eval_steps_per_second': 0.417, 'epoch': 11.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 20%|██        | 70/350 [13:43<52:18, 11.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.6159634590148926, 'eval_runtime': 2.3981, 'eval_samples_per_second': 2.085, 'eval_steps_per_second': 0.417, 'epoch': 13.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 23%|██▎       | 80/350 [15:38<50:19, 11.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.565094470977783, 'eval_runtime': 2.3789, 'eval_samples_per_second': 2.102, 'eval_steps_per_second': 0.42, 'epoch': 15.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 26%|██▌       | 90/350 [17:33<49:11, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5170516967773438, 'eval_runtime': 2.3956, 'eval_samples_per_second': 2.087, 'eval_steps_per_second': 0.417, 'epoch': 16.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 29%|██▊       | 100/350 [19:28<47:17, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.4649040699005127, 'eval_runtime': 2.3707, 'eval_samples_per_second': 2.109, 'eval_steps_per_second': 0.422, 'epoch': 18.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 31%|███▏      | 110/350 [21:30<45:42, 11.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.407757520675659, 'eval_runtime': 2.3942, 'eval_samples_per_second': 2.088, 'eval_steps_per_second': 0.418, 'epoch': 20.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 34%|███▍      | 120/350 [23:25<43:28, 11.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.343446969985962, 'eval_runtime': 2.3932, 'eval_samples_per_second': 2.089, 'eval_steps_per_second': 0.418, 'epoch': 22.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 37%|███▋      | 130/350 [25:20<41:29, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.267294406890869, 'eval_runtime': 2.3965, 'eval_samples_per_second': 2.086, 'eval_steps_per_second': 0.417, 'epoch': 24.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 40%|████      | 140/350 [27:15<39:26, 11.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.219864845275879, 'eval_runtime': 2.3872, 'eval_samples_per_second': 2.094, 'eval_steps_per_second': 0.419, 'epoch': 26.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 150/350 [29:08<37:32, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5504, 'learning_rate': 7.056435515653059e-06, 'epoch': 28.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 43%|████▎     | 150/350 [29:10<37:32, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1902430057525635, 'eval_runtime': 2.5486, 'eval_samples_per_second': 1.962, 'eval_steps_per_second': 0.392, 'epoch': 28.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 46%|████▌     | 160/350 [31:12<35:42, 11.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.161935329437256, 'eval_runtime': 2.391, 'eval_samples_per_second': 2.091, 'eval_steps_per_second': 0.418, 'epoch': 30.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 49%|████▊     | 170/350 [33:07<33:32, 11.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.138967752456665, 'eval_runtime': 2.3927, 'eval_samples_per_second': 2.09, 'eval_steps_per_second': 0.418, 'epoch': 32.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 51%|█████▏    | 180/350 [35:03<32:11, 11.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.122406482696533, 'eval_runtime': 2.3994, 'eval_samples_per_second': 2.084, 'eval_steps_per_second': 0.417, 'epoch': 33.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 54%|█████▍    | 190/350 [36:58<30:17, 11.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.106480836868286, 'eval_runtime': 2.3919, 'eval_samples_per_second': 2.09, 'eval_steps_per_second': 0.418, 'epoch': 35.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 57%|█████▋    | 200/350 [38:53<28:21, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0940558910369873, 'eval_runtime': 2.397, 'eval_samples_per_second': 2.086, 'eval_steps_per_second': 0.417, 'epoch': 37.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 60%|██████    | 210/350 [40:56<26:40, 11.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.082423686981201, 'eval_runtime': 2.3797, 'eval_samples_per_second': 2.101, 'eval_steps_per_second': 0.42, 'epoch': 39.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 63%|██████▎   | 220/350 [42:51<24:30, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0746374130249023, 'eval_runtime': 2.405, 'eval_samples_per_second': 2.079, 'eval_steps_per_second': 0.416, 'epoch': 41.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 66%|██████▌   | 230/350 [44:46<22:31, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0679070949554443, 'eval_runtime': 2.3952, 'eval_samples_per_second': 2.088, 'eval_steps_per_second': 0.418, 'epoch': 43.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 69%|██████▊   | 240/350 [46:41<20:33, 11.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.060976266860962, 'eval_runtime': 2.3947, 'eval_samples_per_second': 2.088, 'eval_steps_per_second': 0.418, 'epoch': 45.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 71%|███████▏  | 250/350 [48:36<18:39, 11.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0542078018188477, 'eval_runtime': 2.3973, 'eval_samples_per_second': 2.086, 'eval_steps_per_second': 0.417, 'epoch': 47.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 74%|███████▍  | 260/350 [50:39<17:09, 11.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.049464702606201, 'eval_runtime': 2.3926, 'eval_samples_per_second': 2.09, 'eval_steps_per_second': 0.418, 'epoch': 48.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 77%|███████▋  | 270/350 [52:34<15:09, 11.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.047043561935425, 'eval_runtime': 2.3877, 'eval_samples_per_second': 2.094, 'eval_steps_per_second': 0.419, 'epoch': 50.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 80%|████████  | 280/350 [54:29<13:14, 11.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0444722175598145, 'eval_runtime': 2.3917, 'eval_samples_per_second': 2.091, 'eval_steps_per_second': 0.418, 'epoch': 52.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 83%|████████▎ | 290/350 [56:24<11:20, 11.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0422093868255615, 'eval_runtime': 2.3938, 'eval_samples_per_second': 2.089, 'eval_steps_per_second': 0.418, 'epoch': 54.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 300/350 [58:17<09:25, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0383, 'learning_rate': 6.088921331488568e-07, 'epoch': 56.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 86%|████████▌ | 300/350 [58:19<09:25, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0407938957214355, 'eval_runtime': 2.3837, 'eval_samples_per_second': 2.098, 'eval_steps_per_second': 0.42, 'epoch': 56.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 89%|████████▊ | 310/350 [1:00:22<07:34, 11.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0404062271118164, 'eval_runtime': 2.3948, 'eval_samples_per_second': 2.088, 'eval_steps_per_second': 0.418, 'epoch': 58.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 91%|█████████▏| 320/350 [1:02:17<05:38, 11.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0399832725524902, 'eval_runtime': 2.3929, 'eval_samples_per_second': 2.089, 'eval_steps_per_second': 0.418, 'epoch': 60.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 94%|█████████▍| 330/350 [1:04:12<03:43, 11.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.039731740951538, 'eval_runtime': 2.3855, 'eval_samples_per_second': 2.096, 'eval_steps_per_second': 0.419, 'epoch': 62.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 97%|█████████▋| 340/350 [1:06:07<01:51, 11.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0396060943603516, 'eval_runtime': 2.3904, 'eval_samples_per_second': 2.092, 'eval_steps_per_second': 0.418, 'epoch': 64.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 350/350 [1:08:03<00:00, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.039623260498047, 'eval_runtime': 2.3877, 'eval_samples_per_second': 2.094, 'eval_steps_per_second': 0.419, 'epoch': 65.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The intermediate checkpoints of PEFT may not be saved correctly, consider using a custom callback to save adapter_model.bin in corresponding saving folders. Check some examples here: https://github.com/huggingface/peft/issues/96\n",
      "100%|██████████| 350/350 [1:08:12<00:00, 11.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4092.112, 'train_samples_per_second': 23.264, 'train_steps_per_second': 0.086, 'train_loss': 3.2515819876534597, 'epoch': 65.88}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in trainer.model.parameters_and_names():\n",
    "    param.set_dtype(ms.float16)\n",
    "trainer.model.save_pretrained(peft_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后存储还是存储为 16 位浮点数，保存训练后的 LoRA 模型保存到指定的输出目录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，lora微调后保存的并不是完整的参数，在推理时，需要将保存的 LoRA 参数加载到原预训练模型中，合并后得到完整的模型，然后进行推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用PeftModel进行配置和参数合并，最后将模型设置为评估模式，进行推理任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model merge succeeded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(55296, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear (4096 -> 4096)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear (4096 -> 8)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear (8 -> 4096)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear(\n",
       "            (base_layer): Linear (4096 -> 4096)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear (4096 -> 8)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear (8 -> 4096)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear (4096 -> 4096)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear (4096 -> 8)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear (8 -> 4096)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear(\n",
       "            (base_layer): Linear (4096 -> 4096)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear (4096 -> 8)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear (8 -> 4096)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear (4096 -> 11008)\n",
       "          (up_proj): Linear (4096 -> 11008)\n",
       "          (down_proj): Linear (11008 -> 4096)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear (4096 -> 55296)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将 LoRA微调后的参数加载到预训练模型中\n",
    "from mindnlp.peft import PeftModel\n",
    "model = PeftModel.from_pretrained(ms_base_model, peft_output_dir)\n",
    "model = model.merge_and_unload()\n",
    "print('model merge succeeded')\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个函数，用于根据用户输入的问题生成相应的回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "\n",
    "def generate_response(question, model, tokenizer, max_length=256):\n",
    "    prompt = f\"以下是用户和助手之间的问答。\\n问：{question}\\n答：\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"ms\", padding=True, truncation=True, max_length=512)\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        # temperature=0.7,\n",
    "        # top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=3,\n",
    "        max_length=max_length,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    response = response.split(\"Answer：\")[-1].strip()\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiangna1/miniconda3/envs/ms39/lib/python3.9/site-packages/mindnlp/transformers/generation/configuration_utils.py:557: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/jiangna1/miniconda3/envs/ms39/lib/python3.9/site-packages/mindnlp/transformers/generation/configuration_utils.py:562: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 如何保持清醒?\n",
      "LLAMA: 以下是用户和助手之间的问答。\n",
      "问：如何保持清醒?\n",
      "答：在你睡觉的时候，你的大脑会一直处于兴奋状态中;当你醒来时，它就会继续工作了。所以如果你的睡眠时间很短的话，你就不会感到太疲劳或昏沉。你可以通过使用一些药物来帮助恢复精力、提高警觉度以及降低血压等方法使自己进入深度睡眠的状态。此外，你还可以通过服用维生素B6片剂或者吃富含蛋白质的食物等方式让自己重新振作起来。\n"
     ]
    }
   ],
   "source": [
    "question = \"如何保持清醒?\"\n",
    "response = generate_response(question, model, tokenizer)\n",
    "\n",
    "print(f\"User: {question}\")\n",
    "print(f\"LLAMA: {response}\")"
   ]
  }
 ],
 "metadata": {
  "AIGalleryInfo": {
   "item_id": "5443b528-0dd5-4909-ac4f-1c9cf839e2aa"
  },
  "flavorInfo": {
   "architecture": "X86_64",
   "category": "GPU"
  },
  "imageInfo": {
   "id": "e1a07296-22a8-4f05-8bc8-e936c8e54202",
   "name": "mindspore1.7.0-cuda10.1-py3.7-ubuntu18.04"
  },
  "kernelspec": {
   "display_name": "ms39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
