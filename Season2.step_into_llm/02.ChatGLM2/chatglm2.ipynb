{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d6ef06-9b7c-4986-b022-edba097094f9",
   "metadata": {},
   "source": [
    "# 推理示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f9884a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-19 12:28:47,192 - mindformers - INFO - Config in the yaml file ./checkpoint_download/glm2/glm2_6b.yaml are used for tokenizer building.\n",
      "2023-08-19 12:28:47,232 - mindformers - INFO - Load the tokenizer name ChatGLM2Tokenizer from the ./checkpoint_download/glm2/glm2_6b.yaml\n",
      "2023-08-19 12:28:47,255 - mindformers - INFO - config in the yaml file ./checkpoint_download/glm2/glm2_6b.yaml are used for tokenizer building.\n",
      "2023-08-19 12:28:47,277 - mindformers - WARNING - Can't find the tokenizer_config.json in the file_dict. The content of file_dict is : {}\n",
      "2023-08-19 12:28:47,287 - mindformers - INFO - build tokenizer class name is: ChatGLM2Tokenizer using args {'bos_token': '<sop>', 'eos_token': '<eop>', 'end_token': '</s>', 'mask_token': '[MASK]', 'gmask_token': '[gMASK]', 'pad_token': '<pad>', 'unk_token': '<unk>', 'vocab_file': './checkpoint_download/glm2/tokenizer.model'}.\n",
      "2023-08-19 12:28:47,367 - mindformers - INFO - ChatGLM2Tokenizer Tokenizer built successfully!\n",
      "2023-08-19 12:30:55,807 - mindformers - INFO - start to read the ckpt file: 12487185277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-19 12:31:41,733 - mindformers - INFO - weights in ./checkpoint_download/glm2/glm2_6b.ckpt are loaded\n",
      "2023-08-19 12:31:41,735 - mindformers - INFO - model built successfully!\n",
      "[Round 1]\n",
      "\n",
      "问：你好\n",
      "\n",
      "答： 你好👋！我是人工智能助手 ChatGLM2-6B，很高兴见到你，欢迎问我任何问题。\n"
     ]
    }
   ],
   "source": [
    "from mindformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"glm2_6b\")\n",
    "model = AutoModel.from_pretrained(\"glm2_6b\")\n",
    "\n",
    "query = \"你好\"\n",
    "\n",
    "prompted_inputs = tokenizer.build_prompt(query)\n",
    "input_tokens = tokenizer([prompted_inputs])\n",
    "\n",
    "outputs = model.generate(input_tokens[\"input_ids\"], max_length=100)\n",
    "response = tokenizer.decode(outputs)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698715b6-2417-401d-955e-9a810187c76c",
   "metadata": {},
   "source": [
    "# 推理部署示例 - 对话机器人\n",
    "建议在终端运行脚本chatglm2_demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ea0b074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-12 03:33:32,005 - mindformers[mindformers/models/base_model.py:110] - INFO - start to read the ckpt file: 12487185277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-12 03:35:09,629 - mindformers[mindformers/models/base_model.py:116] - INFO - weights in ./checkpoint_download/glm2/glm2_6b.ckpt are loaded\n",
      "2023-11-12 03:35:09,644 - mindformers[mindformers/auto_class.py:291] - INFO - model built successfully!\n",
      "2023-11-12 03:35:15,251 - mindformers[mindformers/auto_class.py:702] - INFO - Config in the yaml file ./checkpoint_download/glm2/glm2_6b.yaml are used for tokenizer building.\n",
      "2023-11-12 03:35:16,582 - mindformers[mindformers/auto_class.py:709] - INFO - Load the tokenizer name ChatGLM2Tokenizer from the ./checkpoint_download/glm2/glm2_6b.yaml\n",
      "2023-11-12 03:35:16,633 - mindformers[mindformers/models/base_tokenizer.py:1979] - INFO - config in the yaml file ./checkpoint_download/glm2/glm2_6b.yaml are used for tokenizer building.\n",
      "2023-11-12 03:35:16,682 - mindformers[mindformers/models/base_tokenizer.py:1989] - WARNING - Can't find the tokenizer_config.json in the file_dict. The content of file_dict is : {}\n",
      "2023-11-12 03:35:16,684 - mindformers[mindformers/models/base_tokenizer.py:1995] - INFO - build tokenizer class name is: ChatGLM2Tokenizer using args {'bos_token': '<sop>', 'eos_token': '<eop>', 'end_token': '</s>', 'mask_token': '[MASK]', 'gmask_token': '[gMASK]', 'pad_token': '<pad>', 'unk_token': '<unk>', 'vocab_file': './checkpoint_download/glm2/tokenizer.model'}.\n",
      "2023-11-12 03:35:16,766 - mindformers[mindformers/auto_class.py:788] - INFO - ChatGLM2Tokenizer Tokenizer built successfully!\n",
      "欢迎使用 ChatGLM2-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "用户： 你好\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-12 03:40:47,389 - mindformers[mindformers/generation/text_generator.py:442] - INFO - total time: 292.39666175842285 s; generated tokens: 28 tokens; generate speed: 0.09576032719256387 tokens/s\n",
      "\u001b[H\u001b[2J欢迎使用 ChatGLM2-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\n",
      "\n",
      "用户：你好\n",
      "\n",
      "ChatGLM2-6B：你好👋！我是人工智能助手 ChatGLM2-6B，很高兴见到你，欢迎问我任何问题。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "用户： 请介绍下华为\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-12 03:43:40,087 - mindformers[mindformers/generation/text_generator.py:442] - INFO - total time: 5.809668779373169 s; generated tokens: 108 tokens; generate speed: 18.589700050275948 tokens/s\n",
      "\u001b[H\u001b[2J欢迎使用 ChatGLM2-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\n",
      "\n",
      "用户：你好\n",
      "\n",
      "ChatGLM2-6B：你好👋！我是人工智能助手 ChatGLM2-6B，很高兴见到你，欢迎问我任何问题。\n",
      "\n",
      "用户：请介绍下华为\n",
      "\n",
      "ChatGLM2-6B：华为是一家总部位于中国的全球知名科技公司,成立于1987年,是全球领先的信息与通信技术(ICT)解决方案供应商之一。\n",
      "\n",
      "华为的业务范围涵盖了网络、终端、云计算、软件、芯片等多个领域,旗下的智能手机、电脑、平板电脑等消费电子产品在国内外市场上都享有较高的声誉。此外,华为还在5G、人工智能、云计算等领域进行了大量的投资和研发,致力于成为全球领先的数字化技术领导者。\n",
      "\n",
      "华为以其高品质的产品和服务赢得了全球客户的信任和好评,也曾因其领先\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "用户： 请介绍下chatglm2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-12 03:44:03,998 - mindformers[mindformers/generation/text_generator.py:442] - INFO - total time: 2.8868305683135986 s; generated tokens: 54 tokens; generate speed: 18.70563537490363 tokens/s\n",
      "\u001b[H\u001b[2J欢迎使用 ChatGLM2-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\n",
      "\n",
      "用户：你好\n",
      "\n",
      "ChatGLM2-6B：你好👋！我是人工智能助手 ChatGLM2-6B，很高兴见到你，欢迎问我任何问题。\n",
      "\n",
      "用户：请介绍下华为\n",
      "\n",
      "ChatGLM2-6B：华为是一家总部位于中国的全球知名科技公司,成立于1987年,是全球领先的信息与通信技术(ICT)解决方案供应商之一。\n",
      "\n",
      "华为的业务范围涵盖了网络、终端、云计算、软件、芯片等多个领域,旗下的智能手机、电脑、平板电脑等消费电子产品在国内外市场上都享有较高的声誉。此外,华为还在5G、人工智能、云计算等领域进行了大量的投资和研发,致力于成为全球领先的数字化技术领导者。\n",
      "\n",
      "华为以其高品质的产品和服务赢得了全球客户的信任和好评,也曾因其领先\n",
      "\n",
      "用户：请介绍下chatglm2\n",
      "\n",
      "ChatGLM2-6B：ChatGLM2-6B 是一个人工智能助手，基于清华大学 KEG 实验室与智谱 AI 于 2023 年联合训练的语言模型 GLM2-6B 开发而成，可以针对用户的问题和要求提供适当的答复和支持。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "用户： stop\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import signal\n",
    "import readline\n",
    "\n",
    "import time\n",
    "import mindspore as ms\n",
    "from mindformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "ms.set_context(mode=ms.GRAPH_MODE, device_target=\"Ascend\", device_id=0)\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"glm2_6b\")\n",
    "# 可以在此使用下行代码指定自定义权重进行推理，默认使用自动从obs上下载的预训练权重\n",
    "# config.checkpoint_name_or_path = \"/path/to/glm2_6b_finetune.ckpt\"\n",
    "config.use_past = True\n",
    "model = AutoModel.from_config(config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"glm2_6b\")\n",
    "\n",
    "os_name = platform.system()\n",
    "clear_command = 'cls' if os_name == 'Windows' else 'clear'\n",
    "stop_stream = False\n",
    "\n",
    "\n",
    "def build_prompt(history):\n",
    "    prompt = \"欢迎使用 ChatGLM2-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\"\n",
    "    for query, response in history:\n",
    "        prompt += f\"\\n\\n用户：{query}\"\n",
    "        prompt += f\"\\n\\nChatGLM2-6B：{response}\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def signal_handler(signal, frame):\n",
    "    global stop_stream\n",
    "    stop_stream = True\n",
    "\n",
    "\n",
    "def main():\n",
    "    history = []\n",
    "    global stop_stream\n",
    "    print(\"欢迎使用 ChatGLM2-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\")\n",
    "    while True:\n",
    "        query = input(\"\\n用户：\")\n",
    "        if query.strip() == \"stop\":\n",
    "            break\n",
    "        if query.strip() == \"clear\":\n",
    "            history = []\n",
    "            os.system(clear_command)\n",
    "            print(\"欢迎使用 ChatGLM2-6B 模型，输入内容即可进行对话，clear 清空对话历史，stop 终止程序\")\n",
    "            continue\n",
    "        count = 0\n",
    "        \n",
    "        # prompt: [Round 1]\\n\\n问：{query}\\n\\n答： \n",
    "        prompted_inputs = tokenizer.build_prompt(query)\n",
    "        inputs = tokenizer(prompted_inputs)['input_ids']\n",
    "        outputs = model.generate(inputs, max_length=128)\n",
    "        response = tokenizer.decode(outputs)[0].split(\"答： \")[-1]\n",
    "        history = history + [(query, response)]\n",
    "\n",
    "        if stop_stream:\n",
    "            stop_stream = False\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "            if count % 8 == 0:\n",
    "                os.system(clear_command)\n",
    "                print(build_prompt(history), flush=True)\n",
    "                signal.signal(signal.SIGINT, signal_handler)\n",
    "        os.system(clear_command)\n",
    "        print(build_prompt(history), flush=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
