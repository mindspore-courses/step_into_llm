{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f9884a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-19 12:28:47,192 - mindformers - INFO - Config in the yaml file ./checkpoint_download/glm2/glm2_6b.yaml are used for tokenizer building.\n",
      "2023-08-19 12:28:47,232 - mindformers - INFO - Load the tokenizer name ChatGLM2Tokenizer from the ./checkpoint_download/glm2/glm2_6b.yaml\n",
      "2023-08-19 12:28:47,255 - mindformers - INFO - config in the yaml file ./checkpoint_download/glm2/glm2_6b.yaml are used for tokenizer building.\n",
      "2023-08-19 12:28:47,277 - mindformers - WARNING - Can't find the tokenizer_config.json in the file_dict. The content of file_dict is : {}\n",
      "2023-08-19 12:28:47,287 - mindformers - INFO - build tokenizer class name is: ChatGLM2Tokenizer using args {'bos_token': '<sop>', 'eos_token': '<eop>', 'end_token': '</s>', 'mask_token': '[MASK]', 'gmask_token': '[gMASK]', 'pad_token': '<pad>', 'unk_token': '<unk>', 'vocab_file': './checkpoint_download/glm2/tokenizer.model'}.\n",
      "2023-08-19 12:28:47,367 - mindformers - INFO - ChatGLM2Tokenizer Tokenizer built successfully!\n",
      "2023-08-19 12:30:55,807 - mindformers - INFO - start to read the ckpt file: 12487185277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-19 12:31:41,733 - mindformers - INFO - weights in ./checkpoint_download/glm2/glm2_6b.ckpt are loaded\n",
      "2023-08-19 12:31:41,735 - mindformers - INFO - model built successfully!\n",
      "[Round 1]\n",
      "\n",
      "é—®ï¼šä½ å¥½\n",
      "\n",
      "ç­”ï¼š ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n"
     ]
    }
   ],
   "source": [
    "from mindformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"glm2_6b\")\n",
    "model = AutoModel.from_pretrained(\"glm2_6b\")\n",
    "\n",
    "query = \"ä½ å¥½\"\n",
    "\n",
    "prompted_inputs = tokenizer.build_prompt(query)\n",
    "input_tokens = tokenizer([prompted_inputs])\n",
    "\n",
    "outputs = model.generate(input_tokens[\"input_ids\"], max_length=100)\n",
    "response = tokenizer.decode(outputs)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ea0b074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-24 10:37:14,086 - mindformers - INFO - Config in the yaml file ./checkpoint_download/glm2/glm2_6b.yaml are used for tokenizer building.\n",
      "2023-08-24 10:37:14,107 - mindformers - INFO - Load the tokenizer name ChatGLM2Tokenizer from the ./checkpoint_download/glm2/glm2_6b.yaml\n",
      "2023-08-24 10:37:14,126 - mindformers - INFO - config in the yaml file ./checkpoint_download/glm2/glm2_6b.yaml are used for tokenizer building.\n",
      "2023-08-24 10:37:14,144 - mindformers - WARNING - Can't find the tokenizer_config.json in the file_dict. The content of file_dict is : {}\n",
      "2023-08-24 10:37:14,145 - mindformers - INFO - build tokenizer class name is: ChatGLM2Tokenizer using args {'bos_token': '<sop>', 'eos_token': '<eop>', 'end_token': '</s>', 'mask_token': '[MASK]', 'gmask_token': '[gMASK]', 'pad_token': '<pad>', 'unk_token': '<unk>', 'vocab_file': './checkpoint_download/glm2/tokenizer.model'}.\n",
      "2023-08-24 10:37:14,175 - mindformers - INFO - ChatGLM2Tokenizer Tokenizer built successfully!\n",
      "2023-08-24 10:39:28,886 - mindformers - INFO - start to read the ckpt file: 12487185277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-24 10:40:08,614 - mindformers - INFO - weights in ./checkpoint_download/glm2/glm2_6b.ckpt are loaded\n",
      "2023-08-24 10:40:08,621 - mindformers - INFO - model built successfully!\n",
      "æ¬¢è¿ä½¿ç”¨ ChatGLM2-6B æ¨¡å‹ï¼Œè¾“å…¥å†…å®¹å³å¯è¿›è¡Œå¯¹è¯ï¼Œclear æ¸…ç©ºå¯¹è¯å†å²ï¼Œstop ç»ˆæ­¢ç¨‹åº\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ç”¨æˆ·ï¼š ä½ å¥½\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2Jæ¬¢è¿ä½¿ç”¨ ChatGLM2-6B æ¨¡å‹ï¼Œè¾“å…¥å†…å®¹å³å¯è¿›è¡Œå¯¹è¯ï¼Œclear æ¸…ç©ºå¯¹è¯å†å²ï¼Œstop ç»ˆæ­¢ç¨‹åº\n",
      "\n",
      "ç”¨æˆ·ï¼šä½ å¥½\n",
      "\n",
      "ChatGLM2-6Bï¼šä½ å¥½ï¼Œæˆ‘æ˜¯ ChatGLM2-6Bï¼Œ ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘èƒŒåä½¿ç”¨çš„æ¨¡å‹æ˜¯ GLM2-6Bï¼Œ æ˜¯ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œ å…·æœ‰è¶…è¿‡ 2000 äº¿å‚æ•°ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡ã€‚\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ç”¨æˆ·ï¼š clear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2Jæ¬¢è¿ä½¿ç”¨ ChatGLM2-6B æ¨¡å‹ï¼Œè¾“å…¥å†…å®¹å³å¯è¿›è¡Œå¯¹è¯ï¼Œclear æ¸…ç©ºå¯¹è¯å†å²ï¼Œstop ç»ˆæ­¢ç¨‹åº\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ç”¨æˆ·ï¼š ä»‹ç»åä¸ºæ‰‹æœº\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2Jæ¬¢è¿ä½¿ç”¨ ChatGLM2-6B æ¨¡å‹ï¼Œè¾“å…¥å†…å®¹å³å¯è¿›è¡Œå¯¹è¯ï¼Œclear æ¸…ç©ºå¯¹è¯å†å²ï¼Œstop ç»ˆæ­¢ç¨‹åº\n",
      "\n",
      "ç”¨æˆ·ï¼šä»‹ç»åä¸ºæ‰‹æœº\n",
      "\n",
      "ChatGLM2-6Bï¼šä»‹ç»åä¸ºæ‰‹æœºäº§å“çº¿ï¼Œåä¸ºæ‰‹æœºäº§å“çº¿åŒ…æ‹¬å“ªäº›å‹å·ï¼Ÿ \n",
      "\n",
      "åä¸ºæ‰‹æœºäº§å“çº¿åŒ…æ‹¬ä»¥ä¸‹å‹å·:\n",
      "\n",
      "1. åä¸ºPç³»åˆ—:P10ã€P10 Plusã€P40ã€P40 Proã€P40 Pro Lite\n",
      "\n",
      "2. åä¸ºMateç³»åˆ—:Mate 10ã€Mate 10 Proã€Mate 20ã€Mate 20 Proã€Mate 30ã€Mate 3\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ç”¨æˆ·ï¼š stop\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import signal\n",
    "import readline\n",
    "\n",
    "import time\n",
    "import mindspore as ms\n",
    "import numpy as np\n",
    "from mindformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"glm2_6b\")\n",
    "model = AutoModel.from_pretrained(\"glm2_6b\")\n",
    "\n",
    "os_name = platform.system()\n",
    "clear_command = 'cls' if os_name == 'Windows' else 'clear'\n",
    "stop_stream = False\n",
    "\n",
    "\n",
    "def build_prompt(history):\n",
    "    prompt = \"æ¬¢è¿ä½¿ç”¨ ChatGLM2-6B æ¨¡å‹ï¼Œè¾“å…¥å†…å®¹å³å¯è¿›è¡Œå¯¹è¯ï¼Œclear æ¸…ç©ºå¯¹è¯å†å²ï¼Œstop ç»ˆæ­¢ç¨‹åº\"\n",
    "    for query, response in history:\n",
    "        prompt += f\"\\n\\nç”¨æˆ·ï¼š{query}\"\n",
    "        prompt += f\"\\n\\nChatGLM2-6Bï¼š{response}\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def signal_handler(signal, frame):\n",
    "    global stop_stream\n",
    "    stop_stream = True\n",
    "\n",
    "\n",
    "def main():\n",
    "    history = []\n",
    "    global stop_stream\n",
    "    print(\"æ¬¢è¿ä½¿ç”¨ ChatGLM2-6B æ¨¡å‹ï¼Œè¾“å…¥å†…å®¹å³å¯è¿›è¡Œå¯¹è¯ï¼Œclear æ¸…ç©ºå¯¹è¯å†å²ï¼Œstop ç»ˆæ­¢ç¨‹åº\")\n",
    "    while True:\n",
    "        query = input(\"\\nç”¨æˆ·ï¼š\")\n",
    "        if query.strip() == \"stop\":\n",
    "            break\n",
    "        if query.strip() == \"clear\":\n",
    "            history = []\n",
    "            os.system(clear_command)\n",
    "            print(\"æ¬¢è¿ä½¿ç”¨ ChatGLM2-6B æ¨¡å‹ï¼Œè¾“å…¥å†…å®¹å³å¯è¿›è¡Œå¯¹è¯ï¼Œclear æ¸…ç©ºå¯¹è¯å†å²ï¼Œstop ç»ˆæ­¢ç¨‹åº\")\n",
    "            continue\n",
    "        count = 0\n",
    "\n",
    "        inputs = tokenizer(query)\n",
    "        outputs = model.generate(np.expand_dims(np.array(inputs['input_ids']).astype(np.int32), 0),\n",
    "                                 max_length=100, do_sample=False, top_p=0.7, top_k=1)\n",
    "        response = tokenizer.decode(outputs)[0]\n",
    "        history = history + [(query, response)]\n",
    "\n",
    "        if stop_stream:\n",
    "            stop_stream = False\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "            if count % 8 == 0:\n",
    "                os.system(clear_command)\n",
    "                print(build_prompt(history), flush=True)\n",
    "                signal.signal(signal.SIGINT, signal_handler)\n",
    "        os.system(clear_command)\n",
    "        print(build_prompt(history), flush=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6523c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
