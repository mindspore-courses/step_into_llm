{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRPC数据集使用PEFT训练\n",
    "\n",
    "环境 python==3.9 mindnlp==0.4.1 mindspore==2.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.283 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import mindspore\n",
    "from mindspore import context, Tensor, ops\n",
    "from mindspore.dataset import NumpySlicesDataset, SequentialSampler\n",
    "from mindspore.common.parameter import Parameter\n",
    "from mindspore.nn import AdamWeightDecay\n",
    "from mindnlp.engine import Evaluator\n",
    "from mindnlp.metrics import Accuracy\n",
    "from mindnlp.common.grad import value_and_grad\n",
    "from mindnlp.dataset import load_dataset\n",
    "from mindnlp.transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from mindnlp.peft import LoraConfig, get_peft_model, TaskType\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 导入辅助函数\n",
    "from train_llama_lora.fix_mrpc_training import (\n",
    "    print_dataset_keys,\n",
    "    improved_forward_fn,\n",
    "    improved_train_step,\n",
    "    examine_batch,\n",
    "    prepare_mrpc_batch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 定义数据处理类和函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"单个输入示例，包含一个全局唯一标识符、文本A、可选的文本B和标签\"\"\"\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"将实例序列化为Python字典\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"将实例序列化为JSON字符串\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"表示模型输入特征，包含输入ID、注意力掩码、标记类型ID、标签和输入长度\"\"\"\n",
    "    def __init__(self, input_ids, attention_mask, token_type_ids, label, input_len):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.input_len = input_len\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"将实例序列化为Python字典\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"将实例序列化为JSON字符串\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset_to_examples(ds):\n",
    "    \"\"\"将数据集转换为示例列表\"\"\"\n",
    "    examples = []\n",
    "    iter0 = ds.create_tuple_iterator()\n",
    "    for i, (text_a, text_b, label, idx, label_text) in enumerate(iter0):\n",
    "        examples.append(\n",
    "            InputExample(guid=i, text_a=str(text_a.asnumpy()), text_b=str(text_b.asnumpy()), label=int(label))\n",
    "        )\n",
    "    \n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"截断文本对，使其总长度不超过指定的最大长度\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        # 优先选择文本更长的文本进行截断\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, tokenizer, max_seq_length=512):\n",
    "    \"\"\"将示例列表转换为特征列表\"\"\"\n",
    "    features = []\n",
    "\n",
    "    for ex_index, example in enumerate(examples):\n",
    "        tokenizer.return_token = True\n",
    "        tokens_a = tokenizer(example.text_a)\n",
    "        tokens_b  = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer(example.text_b)\n",
    "        if tokens_b is not None:\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "        tokens = []\n",
    "        token_type_ids = []\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "            token_type_ids.append(0)\n",
    "\n",
    "        if tokens_b is not None:\n",
    "            for token in tokens_b[1:]:\n",
    "                tokens.append(token)\n",
    "                token_type_ids.append(1)\n",
    "\n",
    "        tokenizer.return_token=False\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        input_len = len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            attention_mask.append(0)\n",
    "            token_type_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(attention_mask) == max_seq_length\n",
    "        assert len(token_type_ids) == max_seq_length\n",
    "        \n",
    "        label_id = example.label\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(input_ids=input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          token_type_ids=token_type_ids,\n",
    "                          label=label_id,\n",
    "                          input_len=input_len)\n",
    "        )\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_examples(tokenizer, max_seq_length, mrpc_datas):\n",
    "    \"\"\"加载数据集并转换为模型训练所需的特征\"\"\"\n",
    "    \n",
    "    train_examples = convert_dataset_to_examples(mrpc_datas)\n",
    "\n",
    "    features = convert_examples_to_features(train_examples, tokenizer, max_seq_length=max_seq_length)\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = [f.input_ids for f in features]\n",
    "    all_attention_mask = [f.attention_mask for f in features]\n",
    "    all_token_type_ids = [f.token_type_ids for f in features]\n",
    "    all_lens = [f.input_len for f in features]\n",
    "    all_labels = [f.label for f in features]\n",
    "    dataset = ((all_input_ids, all_attention_mask, all_token_type_ids, all_lens, all_labels))\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader_from_ds(ds, batch_size):\n",
    "    \"\"\"从数据集创建数据加载器\"\"\"\n",
    "    train_sampler = SequentialSampler()  # 应用 SequentialSampler 以顺序方式采样数据\n",
    "    col_names = ['input_ids', 'attention_mask', 'token_type_ids', 'lens', 'labels']\n",
    "    train_dataloader = NumpySlicesDataset(ds, sampler=train_sampler, column_names=col_names)  # 使用 NumpySlicesDataset 包装数据集\n",
    "    train_dataloader = train_dataloader.batch(batch_size)  # 根据指定批次大小 进行 批处理\n",
    "\n",
    "    return train_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 设置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(10862:123128869447488,MainProcess):2025-05-21-18:18:23.362.307 [mindspore/context.py:1401] For 'context.set_context', the parameter 'device_target' will be deprecated and removed in a future version. Please use the api mindspore.set_device() instead.\n"
     ]
    }
   ],
   "source": [
    "# 定义训练参数\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.save_dir = \"./saved_models\"  # 模型保存目录\n",
    "        self.lr = 1e-4                  # 学习率\n",
    "        self.num_epochs = 1            # 训练轮数\n",
    "        self.debug = False              # 是否启用调试模式\n",
    "        self.batch_size = 16            # 批次大小\n",
    "        self.max_seq_len = 256          # 最大序列长度\n",
    "        self.model_name = \"gpt2\"        # 基础模型名称\n",
    "        self.use_lora = True            # 是否使用LoRA进行微调\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# 设置运行模式\n",
    "context.set_context(mode=context.PYNATIVE_MODE, device_target=\"GPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载MRPC数据集...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载tokenizer...\n",
      "添加了 3 个特殊token\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yyy/桌面/mindnlp/mindnlp/transformers/tokenization_utils_base.py:1526: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted, and will be then set to `False` by default. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"加载MRPC数据集...\")\n",
    "# 加载MRPC数据集\n",
    "mrpc_dict = load_dataset(\"SetFit/mrpc\")\n",
    "mrpc_train = mrpc_dict['train']\n",
    "mrpc_valid = mrpc_dict['validation']\n",
    "mrpc_test = mrpc_dict['test']\n",
    "\n",
    "print(\"加载tokenizer...\")\n",
    "# 加载tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(args.model_name)\n",
    "# 添加特殊token\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<bos>\",\n",
    "    \"eos_token\": \"<eos>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print(f\"添加了 {num_added_toks} 个特殊token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理数据集...\n"
     ]
    }
   ],
   "source": [
    "print(\"处理数据集...\")\n",
    "# 处理数据集\n",
    "train_ds = load_examples(tokenizer, args.max_seq_len, mrpc_train)\n",
    "valid_ds = load_examples(tokenizer, args.max_seq_len, mrpc_valid)\n",
    "test_ds = load_examples(tokenizer, args.max_seq_len, mrpc_test)\n",
    "\n",
    "# 转换为dataloader\n",
    "train_dataloader = get_dataloader_from_ds(train_ds, args.batch_size)\n",
    "valid_dataloader = get_dataloader_from_ds(valid_ds, args.batch_size)\n",
    "test_dataloader = get_dataloader_from_ds(test_ds, args.batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 加载模型和配置PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载模型...\n",
      "[MS_ALLOC_CONF]Runtime config:  enable_vmm:True  vmm_align_size:2MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "应用LoRA配置...\n",
      "可训练参数信息:\n",
      "trainable params: 296,448 || all params: 124,740,096 || trainable%: 0.237652534755144\n"
     ]
    }
   ],
   "source": [
    "print(\"加载模型...\")\n",
    "# 加载模型\n",
    "model = GPT2ForSequenceClassification.from_pretrained(args.model_name, num_labels=2, force_download=False, use_safetensors=False)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.resize_token_embeddings(model.config.vocab_size + num_added_toks)\n",
    "\n",
    "# 配置PEFT (LoRA)\n",
    "if args.use_lora:\n",
    "    print(\"应用LoRA配置...\")\n",
    "    peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=True)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    print(\"可训练参数信息:\")\n",
    "    model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 25 个可训练参数\n",
      "转换后的参数数量: 25\n"
     ]
    }
   ],
   "source": [
    "# 获取可训练参数\n",
    "params = []\n",
    "if hasattr(model, \"get_trainable_parameters\"):\n",
    "    params = model.get_trainable_parameters()\n",
    "elif hasattr(model, \"trainable_params\"):\n",
    "    params = model.trainable_params()\n",
    "\n",
    "print(f\"找到 {len(params)} 个可训练参数\")\n",
    "\n",
    "# 转换参数\n",
    "converted_params = []\n",
    "for param in params:\n",
    "    if hasattr(param, \"data\") and hasattr(param, \"name\"):\n",
    "        converted_param = Parameter(param.data, name=param.name)\n",
    "    else:\n",
    "        converted_param = param\n",
    "    converted_params.append(converted_param)\n",
    "\n",
    "print(f\"转换后的参数数量: {len(converted_params)}\")\n",
    "\n",
    "# 使用转换后的参数\n",
    "optimizer = AdamWeightDecay(params=converted_params, learning_rate=args.lr)\n",
    "metric = Accuracy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建保存目录\n",
    "os.makedirs(args.save_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 定义训练和评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集列名: ['input_ids', 'attention_mask', 'token_type_ids', 'lens', 'labels']\n",
      "处理后训练数据集列名: ['input_ids', 'attention_mask', 'token_type_ids', 'labels']\n"
     ]
    }
   ],
   "source": [
    "# 预处理数据集，移除不需要的'lens'列\n",
    "def remove_lens_column(dataset):\n",
    "    \"\"\"移除数据集中的'lens'列\"\"\"\n",
    "    columns_to_keep = [col for col in dataset.get_col_names() if col != 'lens']\n",
    "    return dataset.project(columns=columns_to_keep)\n",
    "\n",
    "# 打印数据集列名\n",
    "print(\"训练数据集列名:\", train_dataloader.get_col_names())\n",
    "\n",
    "# 处理数据集\n",
    "train_dataset = remove_lens_column(train_dataloader)\n",
    "valid_dataset = remove_lens_column(valid_dataloader)\n",
    "test_dataset = remove_lens_column(test_dataloader)\n",
    "\n",
    "print(\"处理后训练数据集列名:\", train_dataset.get_col_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义前向函数(使用改进的版本)\n",
    "def forward_fn(model, data):\n",
    "    \"\"\"前向计算函数\"\"\"\n",
    "    # 只在第一个批次时启用详细输出\n",
    "    verbose = getattr(forward_fn, 'first_batch', False)\n",
    "    if verbose:\n",
    "        forward_fn.first_batch = False\n",
    "    \n",
    "    # 使用改进版本\n",
    "    return improved_forward_fn(model, data, verbose=verbose)\n",
    "\n",
    "# 设置第一个批次标志\n",
    "forward_fn.first_batch = True\n",
    "\n",
    "# 使用value_and_grad包装前向计算函数\n",
    "grad_fn = value_and_grad(forward_fn, None, optimizer.parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练步骤(使用改进的版本)\n",
    "def train_step(model, optimizer, data):\n",
    "    \"\"\"执行一个训练步骤\"\"\"\n",
    "    # 首先处理批次数据,确保包含所需字段\n",
    "    processed_data = prepare_mrpc_batch(data)\n",
    "    # 使用改进的训练步骤\n",
    "    return improved_train_step(model, optimizer, processed_data, grad_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义评估函数\n",
    "def evaluate(model, dataset, metric):\n",
    "    \"\"\"评估模型\"\"\"\n",
    "    metric.clear()\n",
    "    model.set_train(False)\n",
    "    \n",
    "    for data in dataset.create_dict_iterator():\n",
    "        # 处理批次数据,确保包含所需字段\n",
    "        processed_data = prepare_mrpc_batch(data)\n",
    "        \n",
    "        # 确保标签单独传递\n",
    "        if 'labels' in processed_data:\n",
    "            labels = processed_data.pop('labels')\n",
    "        elif 'label' in processed_data:\n",
    "            labels = processed_data.pop('label')\n",
    "        else:\n",
    "            print(\"警告: 评估数据中没有标签!\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # 前向计算\n",
    "            outputs = model(**processed_data)\n",
    "            \n",
    "            # 更新指标\n",
    "            if isinstance(outputs, tuple):\n",
    "                logits = outputs[0]\n",
    "            elif hasattr(outputs, \"logits\"):\n",
    "                # 处理SequenceClassifierOutputWithPast对象\n",
    "                logits = outputs.logits\n",
    "            else:\n",
    "                logits = outputs\n",
    "                \n",
    "            metric.update(logits, labels)\n",
    "        except Exception as e:\n",
    "            print(f\"评估时出错: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # 计算指标\n",
    "    result = metric.eval()\n",
    "    model.set_train(True)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检查数据格式...\n",
      "\n",
      "批次数据检查:\n",
      "批次包含 4 个键:\n",
      "  input_ids: 形状=(16, 256), 类型=mindspore.int64\n",
      "    前5个值: [50256 50256 50256 50256     0]\n",
      "  attention_mask: 形状=(16, 256), 类型=mindspore.int64\n",
      "    前5个值: [1 1 1 1 0]\n",
      "  token_type_ids: 形状=(16, 256), 类型=mindspore.int64\n",
      "    前5个值: [0 0 1 1 0]\n",
      "  labels: 形状=(16,), 类型=mindspore.int64\n",
      "    前5个值: [1 0 1 0 1]\n",
      "    唯一标签值: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# 在开始训练前检查一个批次数据\n",
    "print(\"检查数据格式...\")\n",
    "first_batch = next(train_dataset.create_dict_iterator())\n",
    "examine_batch(first_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练: 1个epoch, 共230步\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bd450da0d5491cb4e37870386dffdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据包含以下键: ['input_ids', 'attention_mask', 'token_type_ids', 'labels']\n",
      "使用'labels'作为标签\n",
      "Epoch 1/1, 平均损失: 2.0934\n",
      "模型已保存至 ./saved_models/epoch_1\n",
      "验证集准确率: 0.5760\n",
      "找到更好的模型! 已保存至 ./saved_models/best_model\n"
     ]
    }
   ],
   "source": [
    "# 训练循环\n",
    "model.set_train(True)\n",
    "best_acc = 0\n",
    "total_steps = args.num_epochs * train_dataset.get_dataset_size()\n",
    "\n",
    "print(f\"开始训练: {args.num_epochs}个epoch, 共{total_steps}步\")\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    # 训练一个epoch\n",
    "    model.set_train(True)\n",
    "    train_loss = 0\n",
    "    train_steps = 0\n",
    "    \n",
    "    # 重置第一个批次标志（每个epoch只在第一个批次详细输出）\n",
    "    forward_fn.first_batch = epoch == 0\n",
    "    \n",
    "    progress_bar = tqdm(train_dataset.create_dict_iterator(), total=train_dataset.get_dataset_size())\n",
    "    for batch in progress_bar:\n",
    "        loss = train_step(model, optimizer, batch)\n",
    "        \n",
    "        # 错误处理\n",
    "        if isinstance(loss, mindspore.Tensor):\n",
    "            try:\n",
    "                loss_value = loss.asnumpy()\n",
    "                train_loss += loss_value\n",
    "            except:\n",
    "                print(f\"警告: 无法转换损失值 {loss}\")\n",
    "                train_loss += 1000.0  # 使用一个大的默认值\n",
    "        else:\n",
    "            # 处理SequenceClassifierOutputWithPast类型的返回值\n",
    "            try:\n",
    "                if hasattr(loss, \"loss\"):\n",
    "                    loss_value = loss.loss.asnumpy()\n",
    "                    train_loss += loss_value\n",
    "                else:\n",
    "                    print(f\"警告: 返回对象没有loss属性 {type(loss)}\")\n",
    "                    train_loss += 1000.0  # 使用一个大的默认值\n",
    "            except Exception as e:\n",
    "                print(f\"警告: 处理损失值时出错 {e}\")\n",
    "                train_loss += 1000.0  # 使用一个大的默认值\n",
    "            \n",
    "        train_steps += 1\n",
    "        \n",
    "        # 更新进度条\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{args.num_epochs}\")\n",
    "        progress_bar.set_postfix(loss=train_loss/train_steps)\n",
    "        \n",
    "        # 在debug模式下,训练几个批次后停止\n",
    "        if args.debug and train_steps >= 5:\n",
    "            print(\"Debug模式: 提前停止训练\")\n",
    "            break\n",
    "    \n",
    "    # 计算该epoch的平均损失\n",
    "    avg_loss = train_loss / train_steps\n",
    "    print(f\"Epoch {epoch+1}/{args.num_epochs}, 平均损失: {avg_loss:.4f}\")\n",
    "    \n",
    "    # 保存模型检查点\n",
    "    try:\n",
    "        # 使用模型自带的save_pretrained方法替代mindspore.save_checkpoint\n",
    "        save_dir = os.path.join(args.save_dir, f\"epoch_{epoch+1}\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        model.save_pretrained(save_dir)\n",
    "        print(f\"模型已保存至 {save_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"保存模型时出错: {e}\")\n",
    "    \n",
    "    # 评估\n",
    "    acc = evaluate(model, valid_dataset, metric)\n",
    "    print(f\"验证集准确率: {acc:.4f}\")\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        try:\n",
    "            # 使用模型自带的save_pretrained方法替代mindspore.save_checkpoint\n",
    "            best_save_dir = os.path.join(args.save_dir, \"best_model\")\n",
    "            os.makedirs(best_save_dir, exist_ok=True)\n",
    "            model.save_pretrained(best_save_dir)\n",
    "            print(f\"找到更好的模型! 已保存至 {best_save_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"保存最佳模型时出错: {e}\")\n",
    "    \n",
    "    # 在debug模式下,只运行一个epoch\n",
    "    if args.debug:\n",
    "        print(\"Debug模式: 提前停止训练\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 在测试集上评估并保存最终模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在测试集上评估...\n",
      "测试集准确率: 0.5832\n"
     ]
    }
   ],
   "source": [
    "# 在测试集上评估\n",
    "print(\"在测试集上评估...\")\n",
    "test_acc = evaluate(model, test_dataset, metric)\n",
    "print(f\"测试集准确率: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最终模型和tokenizer已保存至 ./saved_models/final_model\n",
      "训练信息已记录到README.md\n"
     ]
    }
   ],
   "source": [
    "# 保存最终模型和tokenizer\n",
    "try:\n",
    "    final_save_dir = os.path.join(args.save_dir, \"final_model\")\n",
    "    os.makedirs(final_save_dir, exist_ok=True)\n",
    "    \n",
    "    # 保存模型\n",
    "    model.save_pretrained(final_save_dir)\n",
    "    \n",
    "    # 保存tokenizer\n",
    "    tokenizer.save_pretrained(final_save_dir)\n",
    "    \n",
    "    print(f\"最终模型和tokenizer已保存至 {final_save_dir}\")\n",
    "    \n",
    "    # 创建README文件，记录训练信息\n",
    "    readme_path = os.path.join(final_save_dir, \"README.md\")\n",
    "    with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"# MRPC数据集上的GPT2模型\\n\\n\")\n",
    "        f.write(f\"## 训练信息\\n\")\n",
    "        f.write(f\"- 模型: {args.model_name}\\n\")\n",
    "        f.write(f\"- 数据集: MRPC\\n\")\n",
    "        f.write(f\"- 训练轮数: {args.num_epochs}\\n\")\n",
    "        f.write(f\"- 批次大小: {args.batch_size}\\n\")\n",
    "        f.write(f\"- 学习率: {args.lr}\\n\")\n",
    "        f.write(f\"- 验证集准确率: {best_acc:.4f}\\n\")\n",
    "        f.write(f\"- 测试集准确率: {test_acc:.4f}\\n\")\n",
    "        \n",
    "    print(\"训练信息已记录到README.md\")\n",
    "except Exception as e:\n",
    "    print(f\"保存最终模型时出错: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
