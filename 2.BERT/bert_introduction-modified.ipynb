{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35bd8ac0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div align=center><img src=\"./assets/cover.png\" alt=\"bert-cover\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576ed71b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div align=center><img src=\"./assets/contents_1_2.png\" alt=\"table-of-contents\"></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67f34239",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=center><img src=\"./assets/qr_code.jpeg\" alt=\"table-of-contents\" width=\"1300\"></div>\n",
    "\n",
    "https://github.com/mindspore-courses/step_into_chatgpt\n",
    "\n",
    "https://github.com/mindspore-lab/mindnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fabf36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3202年了，为什么还要学BERT\n",
    "\n",
    "- ### Decoder only模型当道： GPT3、Bloom、LLAMA、GLM\n",
    "\n",
    "    - #### Transformer Encoder结构在生成式任务上的缺陷\n",
    "\n",
    "    - #### BERT模型规模小\n",
    "\n",
    "    - #### Pretrain-Fintune范式的落寞\n",
    "\n",
    "- ### 2022年以前，学术界还是在倒腾BERT\n",
    "\n",
    "    - #### Finetune更容易针对单领域任务训练\n",
    "    \n",
    "    - #### BERT是首个大规模并行预训练的模型，也是当前的performance baseline\n",
    "    \n",
    "    - #### 由BERT入手学大模型训练、微调、Prompt最简单"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a7118",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# NLP中的预训练模型\n",
    "\n",
    "语言模型的演变经历了以下几个阶段：\n",
    "\n",
    "<div align=center><img src=\"./assets/language_models.svg\" alt=\"language-models\" width=\"1000\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558644f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. `word2vec`/`Glove`将离散的文本数据转换为固定长度的**静态词向量**，后根据下游任务训练不同的**语言模型**；\n",
    "\n",
    "2. `ELMo`预训练模型将文本数据*结合上下文信息*，转换为**动态词向量**，后根据下游任务训练不同的**语言模型**；\n",
    "\n",
    "3. `BERT`同样将文本数据转换为**动态词向量**，能够更好地捕捉*句子级别的信息与语境信息*，后续只需finetune最后的**输出层**即可适配下游任务；\n",
    "\n",
    "4. `GPT`等预训练语言模型主要用于*文本生成类任务*，需要通过**prompt方法**来应用于下游任务，指导模型生成特定的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242c5a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# BERT\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-transfer-learning.png\"></div>\n",
    "\n",
    "BERT模型本质上是结合了`ELMo`模型与`GPT`模型的优势。\n",
    "\n",
    "- 相比于ELMo，BERT仅需改动最后的输出层，而非模型架构，便可以在下游任务中达到很好的效果；\n",
    "- 相比于GPT，BERT在处理词元表示时考虑到了双向上下文的信息；\n",
    "\n",
    "BERT通过两种无监督任务（Masked Language Modelling 和 Next Sentence Prediction）进行预训练，其次，在下游任务中对预训练Transformer编码器的所有参数进行微调，额外的输出层将从头开始训练。\n",
    "\n",
    "> Reference: [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a5669",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# BERT 结构\n",
    "\n",
    "BERT（Bidirectional Encoder Representation from Transformers）是由Transformer的Encoder层堆叠而成，BERT的模型大小有如下两种：\n",
    "\n",
    "- BERT BASE：与Transformer参数量齐平，用于比较模型效果（110M parameters）\n",
    "- BERT LARGE：在BERT BASE基础上扩大参数量，达到了当时各任务最好的结果（340M parameters）\n",
    "\n",
    "| model | blocks | hidden size | attention heads |\n",
    "| :-----:| :----: | :----: | :----: |\n",
    "| Transformer | 6 | 512 | 8 |\n",
    "| BERT BASE | 12 | 768 | 12 |\n",
    "| BERT LARGE | 24 | 1024 | 16 |\n",
    "\n",
    "> Reference: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7e625e",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"./assets/bert-base-bert-large-encoders.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d1ce8",
   "metadata": {},
   "source": [
    "接受输入序列后，BERT会输出每个位置对应的向量（长度等于hidden size），在后续下游任务中，我们会选取与任务相关的位置的向量，输入到最终输出层中得到结果。\n",
    "\n",
    "如在诈骗邮件分类任务中，我们会将表示句子级别信息的`[CLS]` token所对应的vector，放入classfier中，得到对spam/not spam分类的预测。\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-classifier.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27de31d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BERT 输入\n",
    "\n",
    "- 针对句子对相关任务，将两个句子合并为一个句子对输入到Encoder中，`[CLS]` + 第一个句子 + `[SEP]` + 第二个句子 + `[SEP]`;\n",
    "- 针对单个文本相关任务，`[CLS]` + 句子 + \n",
    "`[SEP]`。\n",
    "\n",
    "在诈骗邮件分类中，输入为单个句子，在拆分为tokens后，在序列首尾分别添加`[CLS]`与`[SEP]`即可。\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-input.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c39086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install mindnlp\n",
    "!pip install git+https://openi.pcl.ac.cn/lvyufeng/mindnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad0c6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101  2393  3159  2089  6968  2080  4651  4121 12839   102]\n",
      "['[CLS]', 'help', 'prince', 'may', '##uk', '##o', 'transfer', 'huge', 'inheritance', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.transforms.tokenizers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sequence = 'help prince mayuko transfer huge inheritance'\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs)\n",
    "tokens = []\n",
    "for index in model_inputs:\n",
    "    tokens.append(tokenizer.id_to_token(index))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf4efc6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BERT Embedding\n",
    "\n",
    "输入到BERT模型的信息由三部分内容组成：\n",
    "\n",
    "- 表示内容的token ids\n",
    "- 表示位置的position ids\n",
    "- 用于区分不同句子的token type ids\n",
    "\n",
    "三种信息分别进入Embedding层，得到token embeddings、position embeddings与segment embeddings；与Transformer不同，以上三种均为**可学习**的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1d92c5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=center><img src=\"./assets/bert-embedding-modified.png\" alt=\"bert-embedding\" width=\"1000\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04340e2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mindspore\n",
    "from mindspore import nn\n",
    "import mindspore.common.dtype as mstype\n",
    "from mindspore.common.initializer import initializer, TruncatedNormal\n",
    "\n",
    "class BertEmbeddings(nn.Cell):\n",
    "    \"\"\"\n",
    "    Embeddings for BERT, include word, position and token_type\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, embedding_table=TruncatedNormal(config.initializer_range))\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size, embedding_table=TruncatedNormal(config.initializer_range))\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size, embedding_table=TruncatedNormal(config.initializer_range))\n",
    "        self.layer_norm = nn.LayerNorm((config.hidden_size,), epsilon=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(1 - config.hidden_dropout_prob)\n",
    "\n",
    "    def construct(self, input_ids, token_type_ids=None, position_ids=None):\n",
    "        seq_len = input_ids.shape[1]\n",
    "        if position_ids is None:\n",
    "            position_ids = mnp.arange(seq_len)\n",
    "            position_ids = position_ids.expand_dims(0).expand_as(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = ops.zeros_like(input_ids)\n",
    "        \n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24afcb4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BERT 模型构建\n",
    "\n",
    "BERT模型的构建与上一节课程的Transformer Encoder构建类似。\n",
    "\n",
    "分别构建multi-head attention层，feed-forward network，并在中间用add&norm连接，最后通过线性层与softmax层进行输出。\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-model-architecture.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931ad4de",
   "metadata": {},
   "source": [
    "### BERT self-attention 层\n",
    "\n",
    "<div align=center><img src=\"./assets/transformer_multi-headed_self-attention-recap.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5068b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Cell):\n",
    "    \"\"\"\n",
    "    Self attention layer for BERT.\n",
    "    \"\"\"\n",
    "    def __init__(self,  config):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"The hidden size {config.hidden_size} is not a multiple of the number of attention \"\n",
    "                f\"heads {config.num_attention_heads}\"\n",
    "            )\n",
    "        self.output_attentions = config.output_attentions\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Dense(config.hidden_size, self.all_head_size, \\\n",
    "            weight_init=TruncatedNormal(config.initializer_range))\n",
    "        self.key = nn.Dense(config.hidden_size, self.all_head_size, \\\n",
    "            weight_init=TruncatedNormal(config.initializer_range))\n",
    "        self.value = nn.Dense(config.hidden_size, self.all_head_size, \\\n",
    "            weight_init=TruncatedNormal(config.initializer_range))\n",
    "\n",
    "        self.dropout = Dropout(config.attention_probs_dropout_prob)\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        self.matmul = Matmul()\n",
    "\n",
    "    def transpose_for_scores(self, input_x):\n",
    "        r\"\"\"\n",
    "        transpose for scores\n",
    "        [batch_size, seq_len, num_heads, head_size] to [batch_size, num_heads, seq_len, head_size]\n",
    "        \"\"\"\n",
    "        new_x_shape = input_x.shape[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        input_x = input_x.view(*new_x_shape)\n",
    "        return input_x.transpose(0, 2, 1, 3)\n",
    "\n",
    "    def construct(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" snd \"key\" to get the raw attention scores.\n",
    "        attention_scores = self.matmul(query_layer, key_layer.swapaxes(-1, -2))\n",
    "        attention_scores = attention_scores / ops.sqrt(Tensor(self.attention_head_size, mstype.float32))\n",
    "        # Apply the attention mask is (precommputed for all layers in BertModel forward() function)\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = self.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.transpose(0, 2, 1, 3)\n",
    "        new_context_layer_shape = context_layer.shape[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983cb64",
   "metadata": {},
   "source": [
    "### BERT self-attention 输出层 \n",
    "\n",
    "- BERTSelfOutput：residual connection + layer normalization\n",
    "- BERTAttention: self-attention + add&norm\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-attention-code.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec92f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfOutput(nn.Cell):\n",
    "    r\"\"\"\n",
    "    Bert Self Output\n",
    "    self-attention output + residual connection + layer norm\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Dense(config.hidden_size, config.hidden_size, \\\n",
    "            weight_init=TruncatedNormal(config.initializer_range))\n",
    "        self.layer_norm = nn.LayerNorm((config.hidden_size,), epsilon=1e-12)\n",
    "        self.dropout = Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def construct(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.layer_norm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ef35f5",
   "metadata": {},
   "source": [
    "### BERT feed-forward 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertIntermediate(nn.Cell):\n",
    "    r\"\"\"\n",
    "    Bert Intermediate\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Dense(config.hidden_size, config.intermediate_size, \\\n",
    "            weight_init=TruncatedNormal(config.initializer_range))\n",
    "        self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "    def construct(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd863caa",
   "metadata": {},
   "source": [
    "### BERT 最后的Add&Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0732f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Cell):\n",
    "    r\"\"\"\n",
    "    Bert Output\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Dense(config.intermediate_size, config.hidden_size, \\\n",
    "            weight_init=TruncatedNormal(config.initializer_range))\n",
    "        self.layer_norm = nn.LayerNorm((config.hidden_size,), epsilon=1e-12)\n",
    "        self.dropout = Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def construct(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self. layer_norm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66995cbe",
   "metadata": {},
   "source": [
    "### BERT Encoder\n",
    "\n",
    "- BertLayer：Encoder Layer，集合了self-attention, feed-forward并通过add&norm连接\n",
    "- BertEnocoder：通过Encoder Layer堆叠起来的Encoder结构\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-model-code.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7752a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Cell):\n",
    "    r\"\"\"\n",
    "    Bert Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = BertAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def construct(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        attention_outputs = self.attention(hidden_states, attention_mask, head_mask)\n",
    "        attention_output = attention_outputs[0]\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        outputs = (layer_output,) + attention_outputs[1:]\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertEncoder(nn.Cell):\n",
    "    r\"\"\"\n",
    "    Bert Encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.layer = nn.CellList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def construct(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        all_hidden_states = ()\n",
    "        all_attentions = ()\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if self.output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if self.output_attentions:\n",
    "                all_attentions += (layer_outputs[1],)\n",
    "\n",
    "        if self.output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "        if self.output_hidden_states:\n",
    "            outputs += (all_hidden_states,)\n",
    "        if self.output_attentions:\n",
    "            outputs += (all_attentions,)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a1b39",
   "metadata": {},
   "source": [
    "## BERT 输出\n",
    "\n",
    "BERT会针对每一个位置输出大小为hidden size的向量，在下游任务中，会根据任务内容的不同，选取不同的向量放入输出层。\n",
    "- 我们一般称`[CLS]`经过线性层+激活函数tanh的输出为**pooler output**，用于句子级别的分类/回归任务;\n",
    "- 我们一般称BERT输出的每个位置对应的vector为**sequence output**,用于词语级别的分类任务；\n",
    "\n",
    "<div align=center><img src=\"./assets/bert-output.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3466d706",
   "metadata": {},
   "source": [
    "### BERT Pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b79508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Cell):\n",
    "    r\"\"\"\n",
    "    Bert Pooler\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Dense(config.hidden_size, config.hidden_size, \\\n",
    "            activation='tanh', weight_init=TruncatedNormal(config.initializer_range))\n",
    "\n",
    "    def construct(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding.\n",
    "        # to the first token\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0df426",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## BERT 预训练\n",
    "\n",
    "BERT通过Masked LM（masked language model）与NSP（next sentence prediction）获取词语和句子级别的特征。\n",
    "\n",
    "<div align=center><img src=\"./assets/bert_pretrain_finetune.jpg\" alt=\"bert-pretrain\" width=\"1000\"></div>\n",
    "\n",
    "> <font size=2>图片来源：Devlin, J.; Chang, M. W.; Lee, K.; Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef77d903",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Masked Language Model (Masked LM)\n",
    "\n",
    "BERT模型通过Masked LM捕捉**词语层面**的信息。\n",
    "\n",
    "我们随机将每个句子中15%的词语进行遮盖，替换成掩码\\<mask\\>。在训练过程中，模型会对句子进行“完形填空”，预测这些被遮盖的词语是什么，通过减小被mask词语的损失值来对模型进行优化。\n",
    "\n",
    "<div align=center><img src=\"./assets/masked_lm.png\" alt=\"masked-lm\" width=\"1000\"></div>\n",
    "\n",
    "> <font size=2>图片来源: [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa96afc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "由于\\<mask\\>仅在预训练中出现，为了让预训练和微调中的数据处理尽可能接近，我们在随机mask的时候进行如下操作：\n",
    "- 80%的概率替换为\\<mask\\>\n",
    "- 10%的概率替换为文本中的随机词\n",
    "- 10%的概率不进行替换，保持原有的词元\n",
    "\n",
    "<div align=center><img src=\"./assets/masked_lm_2.png\" alt=\"masked-lm-2\" width=\"1000\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4469a1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "我们通过`BERTPredictionHeadTranform`实现单层感知机，对被遮盖的词元进行预测。在前向网络中，我们需要输入BERT模型的编码结果`hidden_states`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d4e932",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "activation_map = {\n",
    "    'relu': nn.ReLU(),\n",
    "    'gelu': nn.GELU(False),\n",
    "    'gelu_approximate': nn.GELU(),\n",
    "    'swish':nn.SiLU()\n",
    "}\n",
    "\n",
    "class BertPredictionHeadTransform(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Dense(config.hidden_size, config.hidden_size, weight_init=TruncatedNormal(config.initializer_range))\n",
    "        self.transform_act_fn = activation_map.get(config.hidden_act, nn.GELU(False))\n",
    "        self.layer_norm = nn.LayerNorm((config.hidden_size,), epsilon=config.layer_norm_eps)\n",
    "    \n",
    "    def construct(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c825d61f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "根据被遮盖的词元位置`masked_lm_positions`，获得这些词元的预测输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0cc3f8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import mindspore.ops as ops\n",
    "import mindspore.numpy as mnp\n",
    "from mindspore import Parameter, Tensor\n",
    "\n",
    "class BertLMPredictionHead(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(BertLMPredictionHead, self).__init__()\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        self.decoder = nn.Dense(config.hidden_size, config.vocab_size, has_bias=False, weight_init=TruncatedNormal(config.initializer_range))\n",
    "\n",
    "        self.bias = Parameter(initializer('zeros', config.vocab_size), 'bias')\n",
    "\n",
    "    def construct(self, hidden_states, masked_lm_positions):\n",
    "        # hidden_states: [batch_size, seq_len, hidden_size]\n",
    "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
    "        if masked_lm_positions is not None:\n",
    "            # flat_offfsets: [batch_size, ]\n",
    "            flat_offsets = mnp.arange(batch_size) * seq_len\n",
    "            # flat_position: [batch_size * mask_len, ]\n",
    "            flat_position = (masked_lm_positions + flat_offsets.reshape(-1, 1)).reshape(-1)\n",
    "            # flat_sequence_tensor: [batch_size * seq_len, hidden_size]\n",
    "            flat_sequence_tensor = hidden_states.reshape(-1, hidden_size)\n",
    "            # hidden_states: [batch_size * mask_len, hidden_size]\n",
    "            hidden_states = ops.gather(flat_sequence_tensor, flat_position, 0)\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states) + self.bias\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c0e09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Next Sentence Prediction (NSP)\n",
    "\n",
    "BERT通过NSP捕捉**句子级别**的信息，使其可以理解句子与句子之间的联系，从而能够应用于问答或者推理任务。\n",
    "\n",
    "NSP本质上是一个**二分类任务**，通过输入一个句子对，判断两句话是否为连续句子。输入的两个句子A和B中，B有50%的概率是A的下一句。\n",
    "\n",
    "<div align=center><img src=\"./assets/nsp.png\" alt=\"nsp\" width=\"500\"></div>\n",
    "\n",
    "> <font size=2>图片来源: [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert)</font>\n",
    "\n",
    "另外，输入的内容最好是document-level的语料，而非sentence-level的语料，这样训练出的模型可以具备抓取长序列特征的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209b829",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "在这里，我们使用一个单隐藏层的多层感知机`BERTPooler`进行二分类预测。因为特殊占位符\\<cls\\>在预训练中对应了句子级别的特征信息，所以多层感知机分类器只需要输出\\<cls\\>对应的隐藏层输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1784651",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class BertPooler(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(BertPooler, self).__init__()\n",
    "        self.dense = nn.Dense(config.hidden_size, config.hidden_size, activation='tanh', weight_init=TruncatedNormal(config.initializer_range))\n",
    "    \n",
    "    def construct(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding.\n",
    "        # to the first token\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b75af11",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "最后，多层感知机分类器的输出通过一个线性层`self.seq_relationship`，输出对nsp的预测。\n",
    "\n",
    "在`BERTPreTrainingHeads`中，我们对以上提到的两种方式进行整合。最终输出Maked LM（`prediction scores`）和NSP（`seq_realtionship_score`）的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc1f2a8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class BertPreTrainingHeads(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(BertPreTrainingHeads, self).__init__()\n",
    "        self.predictions = BertLMPredictionHead(config)\n",
    "        self.seq_relationship = nn.Dense(config.hidden_size, 2, weight_init=TruncatedNormal(config.initializer_range))\n",
    "    \n",
    "    def construct(self, sequence_output, pooled_output, masked_lm_positions):\n",
    "        prediction_scores = self.predictions(sequence_output, masked_lm_positions)\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return prediction_scores, seq_relationship_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bd5d8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### BERT预训练代码整合\n",
    "\n",
    "我们将上述的类进行实例化，并借此回顾一下BERT预训练的整体流程。\n",
    "\n",
    "1. `BertModel`构建BERT模型；\n",
    "2. `BertPretrainingHeads`整合了Masked LM与NSP两个训练任务， 输出预测结果；\n",
    "    - `BertLMPredictionHead`：输入BERT编码与\\<mask\\>的位置，输出对应位置词元的预测；\n",
    "    - `BERTPooler`：输入BERT编码，输出对\\<cls\\>的隐藏状态，并在`BertPretrainingHeads`中通过线性层输出预测结果；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc5a310",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class BertForPretraining(nn.Cell):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertPreTrainingHeads(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.cls.predictions.decoder.weight = self.bert.embeddings.word_embeddings.embedding_table\n",
    "\n",
    "    def construct(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, masked_lm_positions=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask\n",
    "        )\n",
    "        # ic(outputs) # [shape(batch_size, 128, 256), shape(batch_size, 256)]\n",
    "\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output, masked_lm_positions)\n",
    "\n",
    "        outputs = (prediction_scores, seq_relationship_score,) + outputs[2:]\n",
    "        # ic(outputs) # [shape(batch_size, 128, 256), shape(batch_size, 256)]\n",
    "\n",
    "        return outputs"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
